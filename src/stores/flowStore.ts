import { create } from 'zustand'
import { persist } from 'zustand/middleware'
import {
  addEdge,
  applyEdgeChanges,
  applyNodeChanges,
  type Connection,
  type EdgeChange,
  type NodeChange,
} from 'reactflow'
import { GeminiAPIClient, MockGeminiAPI } from '../services/geminiAPI'
import { KlingAPIClient, MockKlingAPI } from '../services/klingAPI'
import { retryWithBackoff } from '../utils/retry'
import { getStorageInfo, prepareForStorage, getStorageWarning } from '../utils/storage'
import { createBackup } from '../utils/backup'
import { saveImage, getImage } from '../utils/indexedDB'
import type {
  GeminiVideoNodeData,
  GridNodeData,
  ImageImportNodeData,
  KlingVideoNodeData,
  MotionPromptNodeData,
  NanoImageNodeData,
  NodeData,
  NodeType,
  TextPromptNodeData,
  WorkflowEdge,
  WorkflowNode,
} from '../types/nodes'
import { createNodeData } from '../types/nodes'

type HistoryState = {
  nodes: WorkflowNode[]
  edges: WorkflowEdge[]
}

type FlowState = {
  nodes: WorkflowNode[]
  edges: WorkflowEdge[]
  selectedNodeId: string | null
  apiKey: string
  klingApiKey: string
  openaiApiKey: string  // OpenAI API Key
  abortControllers: Map<string, AbortController>
  history: HistoryState[]
  historyIndex: number
  imageModal: { isOpen: boolean; imageUrl: string | null }
  setSelectedNodeId: (id: string | null) => void
  setApiKey: (key: string) => void
  setKlingApiKey: (key: string) => void
  setOpenaiApiKey: (key: string) => void  // OpenAI API Key Setter
  openImageModal: (imageUrl: string) => void
  closeImageModal: () => void
  onNodesChange: (changes: NodeChange[]) => void
  onEdgesChange: (changes: EdgeChange[]) => void
  onConnect: (connection: Connection) => void
  addNode: (node: WorkflowNode) => void
  updateNodeData: (id: string, data: NodeData) => void
  saveWorkflow: () => boolean
  loadWorkflow: () => boolean
  importWorkflow: (nodes: WorkflowNode[], edges: WorkflowEdge[]) => boolean
  exportWorkflow: () => string
  undo: () => void
  redo: () => void
  runWorkflow: () => Promise<void>
  runGeminiNode: (id: string) => Promise<void>
  runNanoImageNode: (id: string) => Promise<void>
  runKlingNode: (id: string) => Promise<void>
  runLLMPromptNode: (id: string) => Promise<void>
  cancelNodeExecution: (id: string) => void
}

const getEdgeClass = (edge: WorkflowEdge, nodes: WorkflowNode[]) => {
  const sourceNode = nodes.find((node) => node.id === edge.source)
  switch (sourceNode?.type) {
    case 'textPrompt':
      return 'edge-text-prompt'
    case 'motionPrompt':
      return 'edge-motion-prompt'
    case 'imageImport':
      return 'edge-image-import'
    case 'nanoImage':
      return 'edge-nano-image'
    case 'geminiVideo':
      return 'edge-gemini-video'
    case 'klingVideo':
      return 'edge-kling-video'
    case 'gridNode':
      return 'edge-text-prompt' // Use violet color
    case 'cellRegenerator':
      return 'edge-motion-prompt' // Use purple color
    case 'gridComposer':
      return 'edge-kling-video' // Use emerald color
    case 'llmPrompt':
      return 'edge-motion-prompt' // Use pink color
    default:
      return 'edge-default'
  }
}

const normalizeEdges = (edges: WorkflowEdge[], nodes: WorkflowNode[]) => {
  const nodeIds = new Set(nodes.map(n => n.id))
  
  // Filter out edges that reference deleted nodes
  return edges
    .filter(edge => nodeIds.has(edge.source) && nodeIds.has(edge.target))
    .map((edge) => ({
      ...edge,
      type: 'bezier',
      className: getEdgeClass(edge, nodes),
    }))
}

const sanitizeEdgesForStorage = (edges: WorkflowEdge[]) =>
  edges.map(({ source, target, sourceHandle, targetHandle, id }) => ({
    id,
    source,
    target,
    sourceHandle,
    targetHandle,
  }))

const isConnectionAllowed = (sourceType: NodeType, targetType: NodeType) => {
  if (sourceType === 'imageImport' && targetType === 'motionPrompt') return true
  if (sourceType === 'imageImport' && targetType === 'nanoImage') return true
  if (sourceType === 'nanoImage' && targetType === 'nanoImage') return true
  if (sourceType === 'textPrompt' && targetType === 'imageImport') return true
  if (sourceType === 'textPrompt' && targetType === 'nanoImage') return true
  if (sourceType === 'textPrompt' && targetType === 'motionPrompt') return true
  if (sourceType === 'motionPrompt' && targetType === 'nanoImage') return true
  if (sourceType === 'motionPrompt' && targetType === 'geminiVideo') return true
  if (sourceType === 'nanoImage' && targetType === 'geminiVideo') return true
  if (sourceType === 'imageImport' && targetType === 'geminiVideo') return true
  // Kling connections
  if (sourceType === 'motionPrompt' && targetType === 'klingVideo') return true
  if (sourceType === 'textPrompt' && targetType === 'klingVideo') return true
  if (sourceType === 'nanoImage' && targetType === 'klingVideo') return true
  if (sourceType === 'imageImport' && targetType === 'klingVideo') return true
  // Grid Node connections
  if (sourceType === 'textPrompt' && targetType === 'gridNode') return true
  if (sourceType === 'motionPrompt' && targetType === 'gridNode') return true
  if (sourceType === 'gridNode' && targetType === 'nanoImage') return true
  // Cell Regenerator connections
  if (sourceType === 'gridNode' && targetType === 'cellRegenerator') return true
  if (sourceType === 'nanoImage' && targetType === 'cellRegenerator') return true
  if (sourceType === 'imageImport' && targetType === 'cellRegenerator') return true
  if (sourceType === 'cellRegenerator' && targetType === 'nanoImage') return true
  if (sourceType === 'cellRegenerator' && targetType === 'imageImport') return true
  // Grid Composer connections
  if (sourceType === 'gridNode' && targetType === 'gridComposer') return true
  if (sourceType === 'nanoImage' && targetType === 'gridComposer') return true
  if (sourceType === 'imageImport' && targetType === 'gridComposer') return true
  if (sourceType === 'cellRegenerator' && targetType === 'gridComposer') return true
  if (sourceType === 'gridComposer' && targetType === 'nanoImage') return true
  if (sourceType === 'gridComposer' && targetType === 'imageImport') return true
  if (sourceType === 'gridComposer' && targetType === 'geminiVideo') return true
  if (sourceType === 'gridComposer' && targetType === 'klingVideo') return true
  // LLM Prompt connections
  if (sourceType === 'textPrompt' && targetType === 'llmPrompt') return true
  if (sourceType === 'motionPrompt' && targetType === 'llmPrompt') return true
  if (sourceType === 'imageImport' && targetType === 'llmPrompt') return true
  if (sourceType === 'nanoImage' && targetType === 'llmPrompt') return true
  if (sourceType === 'gridComposer' && targetType === 'llmPrompt') return true
  if (sourceType === 'llmPrompt' && targetType === 'textPrompt') return true
  if (sourceType === 'llmPrompt' && targetType === 'nanoImage') return true
  if (sourceType === 'llmPrompt' && targetType === 'motionPrompt') return true
  if (sourceType === 'llmPrompt' && targetType === 'gridNode') return true
  if (sourceType === 'llmPrompt' && targetType === 'geminiVideo') return true
  if (sourceType === 'llmPrompt' && targetType === 'klingVideo') return true
  return false
}

const getExecutionOrder = (nodes: WorkflowNode[], edges: WorkflowEdge[]) => {
  const incoming = new Map<string, number>()
  const outgoing = new Map<string, string[]>()

  nodes.forEach((node) => {
    incoming.set(node.id, 0)
    outgoing.set(node.id, [])
  })

  edges.forEach((edge) => {
    incoming.set(edge.target, (incoming.get(edge.target) ?? 0) + 1)
    outgoing.get(edge.source)?.push(edge.target)
  })

  const queue = nodes.filter((node) => (incoming.get(node.id) ?? 0) === 0)
  const order: string[] = []

  while (queue.length) {
    const node = queue.shift()
    if (!node) break
    order.push(node.id)
    const neighbors = outgoing.get(node.id) ?? []
    neighbors.forEach((neighbor) => {
      incoming.set(neighbor, (incoming.get(neighbor) ?? 1) - 1)
      if ((incoming.get(neighbor) ?? 0) === 0) {
        const neighborNode = nodes.find((candidate) => candidate.id === neighbor)
        if (neighborNode) queue.push(neighborNode)
      }
    })
  }

  return order
}

const getIncomingNodes = (
  nodeId: string,
  edges: WorkflowEdge[],
  nodes: WorkflowNode[],
) =>
  edges
    .filter((edge) => edge.target === nodeId)
    .map((edge) => nodes.find((node) => node.id === edge.source))
    .filter(Boolean) as WorkflowNode[]

const getIncomingTextPrompt = (
  nodeId: string,
  edges: WorkflowEdge[],
  nodes: WorkflowNode[],
) => {
  const promptEdge = edges.find((edge) => edge.target === nodeId)
  if (!promptEdge) return undefined
  const promptNode = nodes.find((node) => node.id === promptEdge.source)
  if (!promptNode || promptNode.type !== 'textPrompt') return undefined
  return (promptNode.data as TextPromptNodeData).prompt
}

const STORAGE_KEY = 'nano-banana-workflow-v3'

// Helper function to make error messages more user-friendly
const formatErrorMessage = (error: unknown): string => {
  if (!(error instanceof Error)) return 'ÏòàÏÉÅÏπò Î™ªÌïú Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.'
  
  const message = error.message.toLowerCase()
  
  // API quota exceeded
  if (message.includes('quota') && message.includes('exceeded')) {
    return 'API Ìï†ÎãπÎüâÏù¥ Ï¥àÍ≥ºÎêòÏóàÏäµÎãàÎã§. Ïû†Ïãú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî.'
  }
  
  // Other common errors
  if (message.includes('network') || message.includes('fetch')) {
    return 'ÎÑ§Ìä∏ÏõåÌÅ¨ Ïó∞Í≤∞ÏùÑ ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî.'
  }
  
  if (message.includes('api key') || message.includes('unauthorized')) {
    return 'API ÌÇ§Î•º ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî.'
  }
  
  // Return original message if no match
  return error.message
}

const sanitizeNodesForStorage = (nodes: WorkflowNode[], forExport = false): WorkflowNode[] =>
  nodes.map((node) => {
    const data = { ...(node.data as Record<string, unknown>) }
    
    // Ïù¥Ï†ú localStorageÏóêÎèÑ Ïù¥ÎØ∏ÏßÄÎ•º Ï†ÄÏû•Ìï©ÎãàÎã§
    // base64 DataURLÏùÄ Ïú†ÏßÄ (ÏÉàÎ°úÍ≥†Ïπ® ÌõÑÏóêÎèÑ Î≥µÏõêÎê®)
    // Îã®, blob URLÏùÄ Ï†úÍ±∞ (ÌéòÏù¥ÏßÄ Ïû¨Î°úÎìú Ïãú Î¨¥Ìö®ÌôîÎê®)
    
    // Always remove blob URLs (they don't survive page reload)
    if (typeof data.imageUrl === 'string' && data.imageUrl.startsWith('blob:')) {
      delete data.imageUrl
      delete data.width
      delete data.height
    }
    if (typeof data.outputImageUrl === 'string' && data.outputImageUrl.startsWith('blob:')) {
      delete data.outputImageUrl
    }
    if (typeof data.inputImageUrl === 'string' && data.inputImageUrl.startsWith('blob:')) {
      delete data.inputImageUrl
    }
    if (typeof data.composedImageUrl === 'string' && data.composedImageUrl.startsWith('blob:')) {
      delete data.composedImageUrl
    }
    
    // regeneratedImages Í∞ùÏ≤¥ ÎÇ¥Î∂ÄÏùò blob URLÎèÑ Ï†úÍ±∞
    if (data.regeneratedImages && typeof data.regeneratedImages === 'object') {
      const cleanedImages: Record<string, string> = {}
      for (const [key, value] of Object.entries(data.regeneratedImages)) {
        if (typeof value === 'string' && !value.startsWith('blob:')) {
          cleanedImages[key] = value
        }
      }
      data.regeneratedImages = cleanedImages
    }
    
    // inputImages Í∞ùÏ≤¥ ÎÇ¥Î∂ÄÏùò blob URLÎèÑ Ï†úÍ±∞
    if (data.inputImages && typeof data.inputImages === 'object') {
      const cleanedImages: Record<string, string> = {}
      for (const [key, value] of Object.entries(data.inputImages)) {
        if (typeof value === 'string' && !value.startsWith('blob:')) {
          cleanedImages[key] = value
        }
      }
      data.inputImages = cleanedImages
    }
    if (typeof data.outputVideoUrl === 'string' && data.outputVideoUrl.startsWith('blob:')) {
      delete data.outputVideoUrl
    }
    
    // Reset status for all generation nodes
    if (node.type === 'nanoImage') {
      data.status = 'idle'
      delete data.error
      delete data.lastExecutionTime
    }
    if (node.type === 'geminiVideo') {
      data.status = 'idle'
      data.progress = 0
      delete data.error
      delete data.lastExecutionTime
    }
    if (node.type === 'klingVideo') {
      data.status = 'idle'
      data.progress = 0
      delete data.error
      delete data.taskId
      delete data.lastExecutionTime
    }
    return { ...node, data: data as NodeData }
  })

const MAX_HISTORY_SIZE = 20

const saveToHistory = (get: () => FlowState, set: (state: Partial<FlowState>) => void) => {
  const { nodes, edges, history, historyIndex } = get()
  
  // Remove any history after current index (when undoing then making new changes)
  const newHistory = history.slice(0, historyIndex + 1)
  
  // Add current state
  newHistory.push({ nodes: JSON.parse(JSON.stringify(nodes)), edges: JSON.parse(JSON.stringify(edges)) })
  
  // Keep only last MAX_HISTORY_SIZE states
  if (newHistory.length > MAX_HISTORY_SIZE) {
    newHistory.shift()
  }
  
  set({ 
    history: newHistory, 
    historyIndex: newHistory.length - 1 
  })
}

// ‚ö° Throttled localStorage for better performance
const createThrottledStorage = () => {
  let saveTimeout: NodeJS.Timeout | null = null
  const SAVE_DELAY = 1000 // 1Ï¥à throttle

  return {
    getItem: (name: string) => {
      const value = localStorage.getItem(name)
      return value ? JSON.parse(value) : null
    },
    setItem: (name: string, value: any) => {
      // Throttle: 1Ï¥à ÎèôÏïà Ïó¨Îü¨ Î≤à Ìò∏Ï∂úÎêòÎ©¥ ÎßàÏßÄÎßâ Í≤ÉÎßå Ï†ÄÏû•
      if (saveTimeout) {
        clearTimeout(saveTimeout)
      }
      
      saveTimeout = setTimeout(() => {
        try {
          const serialized = JSON.stringify(value)
          localStorage.setItem(name, serialized)
          console.log('üíæ Throttled save completed')
        } catch (error) {
          console.error('‚ùå Save failed:', error)
        }
      }, SAVE_DELAY)
    },
    removeItem: (name: string) => {
      localStorage.removeItem(name)
    },
  }
}

export const useFlowStore = create<FlowState>()(
  persist(
    (set, get) => ({
      nodes: [],
      edges: [],
      selectedNodeId: null,
      // .env ÌååÏùºÏóêÏÑú API ÌÇ§ ÏûêÎèô Î°úÎìú
      apiKey: import.meta.env.VITE_GEMINI_API_KEY || '',
      klingApiKey: import.meta.env.VITE_KLING_API_KEY || '',
      openaiApiKey: import.meta.env.VITE_OPENAI_API_KEY || '',  // OpenAI API Key
      abortControllers: new Map(),
      history: [],
      historyIndex: -1,
      imageModal: { isOpen: false, imageUrl: null },
      setSelectedNodeId: (id) => set({ selectedNodeId: id }),
      setApiKey: (key) => set({ apiKey: key }),
      setKlingApiKey: (key) => set({ klingApiKey: key }),
      setOpenaiApiKey: (key) => set({ openaiApiKey: key }),  // OpenAI API Key Setter
      openImageModal: (imageUrl) => set({ imageModal: { isOpen: true, imageUrl } }),
      closeImageModal: () => set({ imageModal: { isOpen: false, imageUrl: null } }),
  onNodesChange: (changes) => {
    try {
      // üéØ ÏÑ±Îä• ÏµúÏ†ÅÌôî: position Î≥ÄÍ≤ΩÎßå ÏûàÏúºÎ©¥ Î°úÍ∑∏ ÏÉùÎûµ
      const hasNonPositionChange = changes.some(
        change => change.type !== 'position' && change.type !== 'dimensions'
      )
      
      if (hasNonPositionChange) {
        console.log('üîÑ onNodesChange:', changes)
      }
      
      // Clean up abort controllers for removed nodes
      const removedNodeIds = changes
        .filter(change => change.type === 'remove')
        .map(change => (change as any).id)
      
      if (removedNodeIds.length > 0) {
        console.log('üóëÔ∏è ÎÖ∏Îìú ÏÇ≠Ï†ú ÏãúÎèÑ:', removedNodeIds)
        const { abortControllers } = get()
        removedNodeIds.forEach(id => {
          const controller = abortControllers.get(id)
          if (controller) {
            console.log('üßπ Cleaning up abort controller for deleted node:', id)
            controller.abort()
            abortControllers.delete(id)
          }
        })
        if (removedNodeIds.length > 0) {
          set({ abortControllers: new Map(abortControllers) })
        }
      }
      
      const currentNodes = get().nodes
      const currentEdges = get().edges
      
      const newNodes = applyNodeChanges(changes, currentNodes) as WorkflowNode[]
      const newEdges = normalizeEdges(currentEdges, newNodes)
      
      set({ 
        nodes: newNodes,
        edges: newEdges
      })
      
      // ‚ö° ÏÑ±Îä• ÏµúÏ†ÅÌôî: add/removeÎßå history Ï†ÄÏû• (position/selectÎäî Ï†úÏô∏)
      const shouldSaveHistory = changes.some(change => 
        change.type === 'add' || change.type === 'remove'
      )
      if (shouldSaveHistory) {
        saveToHistory(get, set)
      }
    } catch (error) {
      console.error('‚ùå Error in onNodesChange:', error)
      // ÏóêÎü¨ Î∞úÏÉùÌï¥ÎèÑ Ïï±Ïù¥ Î©àÏ∂îÏßÄ ÏïäÎèÑÎ°ù
    }
  },
  onEdgesChange: (changes) => {
    try {
      const currentEdges = get().edges
      const currentNodes = get().nodes
      const newEdges = applyEdgeChanges(changes, currentEdges)
      
      set({
        edges: normalizeEdges(newEdges, currentNodes),
      })
      
      // Save to history for add/remove changes
      const shouldSaveHistory = changes.some(change => 
        change.type === 'add' || change.type === 'remove'
      )
      if (shouldSaveHistory) {
        saveToHistory(get, set)
      }
    } catch (error) {
      console.error('Error in onEdgesChange:', error)
      // Don't crash the app, just log the error
    }
  },
  onConnect: (connection) => {
    const { source, target } = connection
    if (!source || !target) return
    const sourceNode = get().nodes.find((node) => node.id === source)
    const targetNode = get().nodes.find((node) => node.id === target)
    if (!sourceNode || !targetNode || !sourceNode.type || !targetNode.type) return
    if (!isConnectionAllowed(sourceNode.type, targetNode.type)) return

    set({
      edges: normalizeEdges(
        addEdge(
          {
            ...connection,
            type: 'bezier',
          },
          get().edges,
        ),
        get().nodes,
      ),
    })
    saveToHistory(get, set)
  },
  addNode: (node) => {
    set({ nodes: [...get().nodes, node] })
    saveToHistory(get, set)
  },
  updateNodeData: (id, data) => {
    set({
      nodes: get().nodes.map((node) => {
        if (node.id === id) {
          // Support partial updates by spreading existing data
          const newData = typeof data === 'function' ? data(node.data) : data
          return { ...node, data: { ...node.data, ...newData } }
        }
        return node
      }),
    })
    // Note: persist middleware will automatically save to localStorage
  },
  saveWorkflow: () => {
    try {
      // üìä persist ÎØ∏Îì§Ïõ®Ïñ¥Í∞Ä ÏûêÎèôÏúºÎ°ú Ï†ÄÏû•ÌïòÎØÄÎ°ú, Ïó¨Í∏∞ÏÑúÎäî Î∞±ÏóÖÎßå ÏÉùÏÑ±
      console.log('üíæ Î∞±ÏóÖ ÏÉùÏÑ± Ï§ë...')
      
      // Ï†ÄÏû•Í≥µÍ∞Ñ Ï≤¥ÌÅ¨
      const storageInfo = getStorageInfo()
      console.log(`üìä Storage: ${storageInfo.usedMB} MB / ${storageInfo.limitMB} MB (${storageInfo.percentage.toFixed(1)}%)`)
      
      const warning = getStorageWarning(storageInfo)
      if (warning) {
        console.warn(warning)
      }
      
      // persistÍ∞Ä Ï†ÄÏû•Ìïú Îç∞Ïù¥ÌÑ∞ Í∞ÄÏ†∏Ïò§Í∏∞
      const persistedData = localStorage.getItem('nano-banana-workflow-v3')
      if (persistedData) {
        // üîí ÏûêÎèô Î∞±ÏóÖ ÏÉùÏÑ± (5Î∂ÑÎßàÎã§ Ìïú Î≤àÏî©Îßå)
        const lastBackupKey = 'last-backup-time'
        const lastBackup = parseInt(localStorage.getItem(lastBackupKey) || '0')
        const now = Date.now()
        const fiveMinutes = 5 * 60 * 1000
        
        if (now - lastBackup > fiveMinutes) {
          // persist ÌòïÏãù Í∑∏ÎåÄÎ°ú Î∞±ÏóÖ
          createBackup(persistedData)
          localStorage.setItem(lastBackupKey, now.toString())
          console.log('‚úÖ Î∞±ÏóÖ ÏÉùÏÑ± ÏôÑÎ£å')
        } else {
          console.log('‚è≠Ô∏è Î∞±ÏóÖ ÏÉùÏÑ± Í±¥ÎÑàÎúÄ (5Î∂Ñ Ïù¥ÎÇ¥)')
        }
      }
      
      // persistÍ∞Ä ÏûêÎèôÏúºÎ°ú Ï†ÄÏû•ÌïòÎØÄÎ°ú Ìï≠ÏÉÅ ÏÑ±Í≥µ Î∞òÌôò
      return true
    } catch (error) {
      console.error('‚ùå Î∞±ÏóÖ ÏÉùÏÑ± Ïã§Ìå®:', error)
      return false
    }
  },
  loadWorkflow: () => {
    try {
      console.log('üîÑ loadWorkflow Ìò∏Ï∂úÎê®')
      const raw = localStorage.getItem(STORAGE_KEY)
      if (!raw) {
        console.log('‚ÑπÔ∏è localStorageÏóê Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå')
        return false
      }
      
      const parsed = JSON.parse(raw)
      console.log('üì¶ localStorage Îç∞Ïù¥ÌÑ∞ ÌååÏã± ÏÑ±Í≥µ:', parsed)
      
      // persist ÌòïÏãù ÌôïÏù∏: { state: {...}, version: 0 }
      let nodes: WorkflowNode[] = []
      let edges: WorkflowEdge[] = []
      
      if (parsed.state) {
        // persist ÎØ∏Îì§Ïõ®Ïñ¥ ÌòïÏãù
        console.log('‚úÖ persist ÌòïÏãù Í∞êÏßÄ')
        nodes = Array.isArray(parsed.state.nodes) ? parsed.state.nodes : []
        edges = Array.isArray(parsed.state.edges) ? parsed.state.edges : []
      } else if (parsed.nodes) {
        // Íµ¨Î≤ÑÏ†Ñ ÎòêÎäî export ÌòïÏãù
        console.log('‚ÑπÔ∏è Íµ¨Î≤ÑÏ†Ñ ÌòïÏãù Í∞êÏßÄ')
        nodes = Array.isArray(parsed.nodes) ? parsed.nodes : []
        edges = Array.isArray(parsed.edges) ? parsed.edges : []
      }
      
      console.log('üìä Î°úÎìúÎêú Îç∞Ïù¥ÌÑ∞:', { nodeCount: nodes.length, edgeCount: edges.length })
      
      if (nodes.length === 0) {
        console.log('‚ö†Ô∏è ÎÖ∏ÎìúÍ∞Ä ÏóÜÏùå')
        return false
      }
      
      set({
        nodes,
        edges: normalizeEdges(edges, nodes),
        selectedNodeId: null,
      })
      
      console.log('‚úÖ ÏõåÌÅ¨ÌîåÎ°úÏö∞ Î≥µÏõê ÏôÑÎ£å')
      return true
    } catch (error) {
      console.error('‚ùå loadWorkflow Ïã§Ìå®:', error)
      return false
    }
  },
  importWorkflow: (nodes, edges) => {
    try {
      const currentState = get()
      const existingNodes = currentState.nodes
      const existingEdges = currentState.edges
      
      // Create ID mapping to avoid conflicts
      const idMap = new Map<string, string>()
      const existingIds = new Set(existingNodes.map(n => n.id))
      
      // Generate new IDs for imported nodes if they conflict
      const newNodes = nodes.map(node => {
        let newId = node.id
        
        // If ID already exists, generate a new one
        if (existingIds.has(newId)) {
          newId = `${node.type}-${crypto.randomUUID?.() ?? Date.now()}`
        }
        
        idMap.set(node.id, newId)
        
        // Calculate offset: place imported nodes to the right of existing nodes
        const maxX = existingNodes.length > 0 
          ? Math.max(...existingNodes.map(n => n.position.x + 250))
          : 0
        
        return {
          ...node,
          id: newId,
          position: {
            x: node.position.x + maxX + 50,
            y: node.position.y
          },
          selected: false,
        }
      })
      
      // Update edge IDs to match new node IDs
      const newEdges = edges.map(edge => {
        const newSource = idMap.get(edge.source) ?? edge.source
        const newTarget = idMap.get(edge.target) ?? edge.target
        
        return {
          ...edge,
          id: `${newSource}-${edge.sourceHandle ?? 'output'}-${newTarget}-${edge.targetHandle ?? 'input'}`,
          source: newSource,
          target: newTarget,
        }
      })
      
      // Merge with existing nodes and edges
      const mergedNodes = [...existingNodes, ...newNodes]
      const mergedEdges = normalizeEdges([...existingEdges, ...newEdges], mergedNodes)
      
      set({
        nodes: mergedNodes,
        edges: mergedEdges,
        selectedNodeId: null,
      })
      
      // Save to history
      saveToHistory(get, set)
      
      return true
    } catch (error) {
      console.error('Import workflow failed:', error)
      return false
    }
  },
  exportWorkflow: () => {
    const { nodes, edges } = get()
    return JSON.stringify({
      version: '1.0',
      timestamp: new Date().toISOString(),
      nodes: sanitizeNodesForStorage(nodes, true), // Keep base64 for export
      edges: sanitizeEdgesForStorage(edges),
    }, null, 2)
  },
  undo: () => {
    const { history, historyIndex } = get()
    if (historyIndex > 0) {
      const previousState = history[historyIndex - 1]
      set({
        nodes: JSON.parse(JSON.stringify(previousState.nodes)),
        edges: normalizeEdges(JSON.parse(JSON.stringify(previousState.edges)), previousState.nodes),
        historyIndex: historyIndex - 1,
        selectedNodeId: null,
      })
    }
  },
  redo: () => {
    const { history, historyIndex } = get()
    if (historyIndex < history.length - 1) {
      const nextState = history[historyIndex + 1]
      set({
        nodes: JSON.parse(JSON.stringify(nextState.nodes)),
        edges: normalizeEdges(JSON.parse(JSON.stringify(nextState.edges)), nextState.nodes),
        historyIndex: historyIndex + 1,
        selectedNodeId: null,
      })
    }
  },
  runNanoImageNode: async (id) => {
    const { nodes, edges, abortControllers } = get()
    const current = nodes.find((node) => node.id === id)
    if (!current || current.type !== 'nanoImage') return

    // ‚úÖ Prevent duplicate execution
    const currentData = current.data as NanoImageNodeData
    if (currentData.status === 'processing') {
      console.warn('‚ö†Ô∏è Node is already processing, skipping duplicate execution')
      return
    }

    // ‚úÖ Rate limiting: Check last execution time
    const now = Date.now()
    const lastExecution = (currentData as any).lastExecutionTime || 0
    const minInterval = 3000 // 3 seconds minimum between executions
    
    if (now - lastExecution < minInterval) {
      const waitTime = Math.ceil((minInterval - (now - lastExecution)) / 1000)
      set({
        nodes: nodes.map((node) =>
          node.id === id
            ? {
                ...node,
                data: {
                  ...node.data,
                  status: 'error',
                  error: `ÎÑàÎ¨¥ Îπ†Î•¥Í≤å Ïã§ÌñâÌñàÏäµÎãàÎã§. ${waitTime}Ï¥à ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî.`,
                },
              }
            : node,
        ),
      })
      return
    }

    // Create abort controller for this execution
    const abortController = new AbortController()
    abortControllers.set(id, abortController)
    set({ abortControllers: new Map(abortControllers) })

    const updateNode = (updater: (data: NodeData) => NodeData) => {
      set({
        nodes: get().nodes.map((node) =>
          node.id === id ? { ...node, data: updater(node.data) } : node,
        ),
      })
    }

    const incoming = getIncomingNodes(id, edges, get().nodes)
    
    // Get prompt from various sources
    let prompt = ''
    
    // Check for gridNode
    const gridNodeEdge = edges.find((e) => e.target === id && 
      get().nodes.find(n => n.id === e.source)?.type === 'gridNode')
    
    if (gridNodeEdge) {
      const gridNode = get().nodes.find((n) => n.id === gridNodeEdge.source)
      if (gridNode?.type === 'gridNode') {
        const data = gridNode.data as GridNodeData
        const slotId = gridNodeEdge.sourceHandle
        prompt = slotId ? data.generatedPrompts?.[slotId] || '' : ''
      }
    } else {
      // Check for prompt handle connection
      const promptEdge = edges.find((e) => e.target === id && e.targetHandle === 'prompt')
      if (promptEdge) {
        const promptNode = get().nodes.find((n) => n.id === promptEdge.source)
        prompt =
          promptNode?.type === 'textPrompt'
            ? (promptNode.data as TextPromptNodeData).prompt
            : promptNode?.type === 'motionPrompt'
              ? (promptNode.data as MotionPromptNodeData).combinedPrompt
              : promptNode?.type === 'llmPrompt'
                ? (promptNode.data as any).outputPrompt || ''
                : ''
      } else {
        // Fallback: Original prompt logic (any connection)
        const promptNode = incoming.find(
          (node) => node.type === 'textPrompt' || node.type === 'motionPrompt' || node.type === 'llmPrompt',
        )
        prompt =
          promptNode?.type === 'textPrompt'
            ? (promptNode.data as TextPromptNodeData).prompt
            : promptNode?.type === 'motionPrompt'
              ? (promptNode.data as MotionPromptNodeData).combinedPrompt
              : promptNode?.type === 'llmPrompt'
                ? (promptNode.data as any).outputPrompt || ''
                : ''
      }
    }
    
    // Collect multiple reference images
    const referenceImages: string[] = []
    const referencePrompts: string[] = []
    const data = current.data as NanoImageNodeData
    const maxRefs = data.maxReferences || 3
    
    // Check each ref-N handle
    for (let i = 1; i <= maxRefs; i++) {
      const refEdge = edges.find((e) => e.target === id && e.targetHandle === `ref-${i}`)
      if (refEdge) {
        const refNode = get().nodes.find((n) => n.id === refEdge.source)
        if (refNode) {
          let imageDataUrl: string | undefined
          let refPrompt: string | undefined
          
          if (refNode.type === 'imageImport') {
            const imgData = refNode.data as ImageImportNodeData
            imageDataUrl = imgData.imageDataUrl
            refPrompt = getIncomingTextPrompt(refNode.id, edges, get().nodes) ?? imgData.referencePrompt
          } else if (refNode.type === 'nanoImage') {
            const imgData = refNode.data as NanoImageNodeData
            imageDataUrl = imgData.outputImageDataUrl
          } else if (refNode.type === 'gridComposer') {
            const imgData = refNode.data as any
            imageDataUrl = imgData.composedImageDataUrl || imgData.composedImageUrl
          } else if (refNode.type === 'cellRegenerator') {
            const imgData = refNode.data as any
            // For cell regenerator, try to get image from sourceHandle (e.g., S1, S2, etc.)
            const cellId = refEdge.sourceHandle
            if (cellId && imgData.regeneratedImages?.[cellId]) {
              imageDataUrl = imgData.regeneratedImages[cellId]
            }
          }
          
          if (imageDataUrl) {
            // üî• Convert idb: or s3: reference to actual DataURL
            if (imageDataUrl.startsWith('idb:') || imageDataUrl.startsWith('s3:')) {
              try {
                const actualDataUrl = await getImage(imageDataUrl)
                if (actualDataUrl) {
                  referenceImages.push(actualDataUrl)
                } else {
                  console.warn(`‚ö†Ô∏è Failed to load reference image: ${imageDataUrl}`)
                }
              } catch (error) {
                console.error(`‚ùå Error loading reference image: ${imageDataUrl}`, error)
              }
            } else {
              referenceImages.push(imageDataUrl)
            }
            
            if (refPrompt) {
              referencePrompts.push(`Reference ${i}: ${refPrompt}`)
            }
          }
        }
      }
    }
    
    // Fallback: Check for old-style connection (no handle specified)
    if (referenceImages.length === 0) {
      const imageNode =
        incoming.find((node) => node.type === 'imageImport') ??
        incoming.find((node) => node.type === 'nanoImage') ??
        incoming.find((node) => node.type === 'gridComposer') ??
        incoming.find((node) => node.type === 'cellRegenerator')

      let inputImageDataUrl: string | undefined
      
      if (imageNode?.type === 'imageImport') {
        inputImageDataUrl = (imageNode.data as ImageImportNodeData).imageDataUrl
      } else if (imageNode?.type === 'nanoImage') {
        inputImageDataUrl = (imageNode.data as NanoImageNodeData).outputImageDataUrl
      } else if (imageNode?.type === 'gridComposer') {
        const imgData = imageNode.data as any
        inputImageDataUrl = imgData.composedImageDataUrl || imgData.composedImageUrl
      } else if (imageNode?.type === 'cellRegenerator') {
        // For cell regenerator without specific handle, we can't determine which cell to use
        // User should use specific cell output handles (S1, S2, etc.)
        inputImageDataUrl = undefined
      }
      
      const referencePrompt =
        imageNode?.type === 'imageImport'
          ? getIncomingTextPrompt(imageNode.id, edges, get().nodes) ??
            (imageNode.data as ImageImportNodeData).referencePrompt
          : undefined
      
      if (inputImageDataUrl) {
        // üî• Convert idb: or s3: reference to actual DataURL
        if (inputImageDataUrl.startsWith('idb:') || inputImageDataUrl.startsWith('s3:')) {
          try {
            const actualDataUrl = await getImage(inputImageDataUrl)
            if (actualDataUrl) {
              referenceImages.push(actualDataUrl)
            } else {
              console.warn(`‚ö†Ô∏è Failed to load fallback reference image: ${inputImageDataUrl}`)
            }
          } catch (error) {
            console.error(`‚ùå Error loading fallback reference image: ${inputImageDataUrl}`, error)
          }
        } else {
          referenceImages.push(inputImageDataUrl)
        }
        
        if (referencePrompt) {
          referencePrompts.push(`Reference 1: ${referencePrompt}`)
        }
      }
    }

    if (!prompt.trim()) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'ÌÖçÏä§Ìä∏ ÌîÑÎ°¨ÌîÑÌä∏ ÎÖ∏ÎìúÎ•º Ïó∞Í≤∞Ìï¥ Ï£ºÏÑ∏Ïöî.',
      }))
      return
    }

    const apiKey = get().apiKey || import.meta.env.VITE_GEMINI_API_KEY || ''
    
    if (!apiKey) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'Gemini API KeyÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§. ÏÉÅÎã® "API Key" Î≤ÑÌäºÏùÑ ÎàåÎü¨ÏÑú ÏÑ§Ï†ïÌïòÏÑ∏Ïöî.',
      }))
      return
    }

    updateNode((prev) => ({
      ...prev,
      status: 'processing',
      error: undefined,
      lastExecutionTime: now,
    }))

    const client = new GeminiAPIClient(apiKey)

    try {
      // Check if aborted before starting
      if (abortController.signal.aborted) {
        throw new Error('ÏûëÏóÖÏù¥ Ï∑®ÏÜåÎêòÏóàÏäµÎãàÎã§.')
      }

      // Check if Grid Composer + LLM Prompt are connected
      const gridComposerEdge = edges.find(e => e.target === id && get().nodes.find(n => n.id === e.source)?.type === 'gridComposer')
      const gridComposerNode = gridComposerEdge ? get().nodes.find(n => n.id === gridComposerEdge.source) : null
      const hasGridComposerRef = referenceImages.length > 0 && !!gridComposerEdge
      
      const llmPromptEdge = edges.find(e => 
        e.target === id && 
        e.targetHandle === 'prompt' && 
        get().nodes.find(n => n.id === e.source)?.type === 'llmPrompt'
      )
      const llmPromptNode = llmPromptEdge ? get().nodes.find(n => n.id === llmPromptEdge.source) : null
      const hasLLMPrompt = !!llmPromptNode
      
      // Get reference mode from LLM Prompt Helper
      const referenceMode = (llmPromptNode?.data as any)?.referenceMode || 'exact'
      
      // Extract Grid Composer label info (for Nano Banana)
      let gridLabelInfoForNano = ''
      if (gridComposerNode && gridComposerNode.type === 'gridComposer') {
        const gridData = gridComposerNode.data as any
        if (gridData.inputImages && gridData.slots) {
          const layout = gridData.gridLayout || '1x3'
          const slots = gridData.slots as Array<{ id: string; label: string; metadata?: string }>
          
          const slotDescriptions = slots
            .filter(slot => gridData.inputImages[slot.id])
            .map((slot, index) => {
              const position = ['Ï≤´ Î≤àÏß∏', 'Îëê Î≤àÏß∏', 'ÏÑ∏ Î≤àÏß∏', 'ÎÑ§ Î≤àÏß∏', 'Îã§ÏÑØ Î≤àÏß∏', 'Ïó¨ÏÑØ Î≤àÏß∏'][index] || `${index + 1}Î≤àÏß∏`
              let description = `- ${position} Ï∞∏Í≥† ÏöîÏÜå (${slot.id}): ${slot.label}`
              if (slot.metadata && slot.metadata.trim()) {
                description += ` - ${slot.metadata}`
              }
              return description
            })
            .join('\n')
          
          if (slotDescriptions) {
            gridLabelInfoForNano = `\n\nüìã Ï∞∏Í≥† Ïù¥ÎØ∏ÏßÄ Íµ¨ÏÑ± (${layout} Í∑∏Î¶¨Îìú):\n${slotDescriptions}\n\n`
          }
        }
      }
      
      // Check if Motion Prompt is connected (for camera transformation)
      const motionPromptEdge = edges.find(e => 
        e.target === id && 
        e.targetHandle === 'prompt' && 
        get().nodes.find(n => n.id === e.source)?.type === 'motionPrompt'
      )
      const motionPromptNode = motionPromptEdge ? get().nodes.find(n => n.id === motionPromptEdge.source) : null
      const hasMotionPrompt = !!motionPromptNode
      
      // Add reference prompts to main prompt if available
      let enhancedPrompt = referencePrompts.length > 0
        ? `${prompt}\n\n${referencePrompts.join('\n')}`
        : prompt
      
      // üé• Motion Prompt + Reference Image: Force camera transformation
      if (hasMotionPrompt && referenceImages.length > 0) {
        const motionData = motionPromptNode?.data as MotionPromptNodeData
        const hasCameraMovement = 
          (motionData.cameraRotation && motionData.cameraRotation !== 0) ||
          (motionData.cameraTilt && motionData.cameraTilt !== 0) ||
          (motionData.cameraDistance && motionData.cameraDistance !== 1.0)
        
        if (hasCameraMovement) {
          // Check if 90-degree rotation is specified
          const has90DegreeRotation = Math.abs(motionData.cameraRotation || 0) === 90
          const rotationDirection = (motionData.cameraRotation || 0) > 0 ? 'right' : 'left'
          const visibleSide = (motionData.cameraRotation || 0) > 0 ? 'LEFT' : 'RIGHT'
          
          let specialRotationNote = ''
          if (has90DegreeRotation) {
            specialRotationNote = `

üö® CRITICAL 90-DEGREE SIDE VIEW INSTRUCTION:
The prompt specifies "rotate ${rotationDirection} 90¬∞" - this is a PERPENDICULAR side view!

MANDATORY REQUIREMENTS for 90¬∞ rotation:
‚úÖ Camera positioned at COMPLETE 90-degree angle (perpendicular to subject)
‚úÖ Subject facing PERPENDICULAR to camera (left-to-right across frame)
‚úÖ ONLY ${visibleSide} side profile visible (complete side view)
‚úÖ NO frontal face visible - pure lateral perspective
‚úÖ Subject appears in profile, oriented across the frame horizontally
‚úÖ This is NOT a frontal or three-quarter view - it's a FULL SIDE VIEW

üö´ ABSOLUTELY FORBIDDEN at 90¬∞:
‚ùå Showing any frontal face
‚ùå Any three-quarter or angled view
‚ùå Subject facing toward camera
‚ùå Any frontal perspective elements

90¬∞ means PERPENDICULAR - imagine looking at subject from directly their ${rotationDirection} side.`
          }
          
          enhancedPrompt = `üé¨ CRITICAL: CAMERA TRANSFORMATION WITH CHARACTER CONSISTENCY üé¨${specialRotationNote}

‚ö†Ô∏è MANDATORY INSTRUCTIONS (PRIORITY ORDER):

1Ô∏è‚É£ HIGHEST PRIORITY - CHARACTER/STYLE CONSISTENCY (from reference image):
   ‚úÖ MUST PRESERVE EXACTLY:
   - Character facial features, hair, eye color, skin tone
   - Character clothing, outfit design, colors, materials
   - Character body proportions, build, posture style
   - Lighting quality, color palette, tone, mood
   - Visual style, textures, rendering quality
   - Background architectural elements, props, environment
   
   üö´ ABSOLUTELY FORBIDDEN:
   - Changing character appearance (face, hair, body)
   - Changing outfit design, colors, or materials
   - Changing color palette or visual tone
   - Altering character identity or features

2Ô∏è‚É£ SECOND PRIORITY - CAMERA TRANSFORMATION (from prompt):
   üì∑ REQUIRED CAMERA CHANGES:
   ${prompt}
   
   ‚úÖ YOU MUST:
   - Apply the specified camera angle/rotation
   - Apply the specified camera tilt (high/low angle)
   - Apply the specified camera distance/zoom
   - Change viewpoint perspective from reference
   
   üö® SPECIAL: If "rotate right/left 90¬∞" is specified:
   - This means PERPENDICULAR side view (NOT frontal!)
   - Camera positioned at 90-degree angle to subject
   - Subject faces PERPENDICULAR to camera (left-to-right across frame)
   - ONLY one side profile visible, NO frontal face
   - Complete lateral/side perspective
   
   üö´ DO NOT:
   - Keep the same camera angle as reference
   - Ignore camera transformation instructions
   - Show frontal face when 90¬∞ is specified

üéØ EXECUTION STRATEGY:
Step 1: Extract visual identity from reference (character, style, colors)
Step 2: Apply camera transformation to that exact character/scene
Step 3: Verify character consistency is maintained

üí° ANALOGY: You're photographing the SAME character from a DIFFERENT angle.
- The character stays IDENTICAL
- Only the camera moves to a new position

‚ú® FINAL CHECK:
- Does the character look EXACTLY like the reference? ‚úì
- Is the camera angle DIFFERENT from reference? ‚úì
- Both must be TRUE for success!

Generate the image maintaining PERFECT character consistency while applying the EXACT camera transformation specified above.`
        }
      }
      
      // üéØ Grid Composer + LLM: Ï∞∏Ï°∞ Ï†ïÌôïÎèÑÏóê Îî∞Îùº ÏßÄÏãú Ï∂îÍ∞Ä
      else if (hasGridComposerRef && hasLLMPrompt) {
        if (referenceMode === 'exact') {
          // Ï†ïÌôïÏÑ± Î™®Îìú: Ï∞∏Ï°∞ Ïù¥ÎØ∏ÏßÄ PIXEL-LEVEL Î≥µÏ†ú
          enhancedPrompt = `‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è CRITICAL: EXACT REFERENCE IMAGE REPLICATION REQUIRED ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è
${gridLabelInfoForNano}
STRICT MODE: Reference image is ABSOLUTE PRIMARY source.
Text prompt = ONLY for understanding story/actions. Your task = PIXEL-PERFECT VISUAL COPY.

üìå TEXT PROMPT = STORY/ACTIONS (PRESERVE 100%):
- If text says "holding helmet" ‚Üí Generate "holding helmet" (NOT "wearing helmet"!)
- If text says "walking" ‚Üí Generate "walking" (keep action!)
- If text says "one person" ‚Üí Generate "one person" (NOT "two"!)
- Preserve ALL actions, character counts, story elements from text prompt

üé® REFERENCE IMAGE = VISUAL DESIGN (REPLICATE 100%):
- S1 Background: Copy EXACT colors, lighting, structure, materials
- S2 Character: Copy EXACT appearance, outfit, hair, facial features
- S3 Object/Robot: Copy EXACT colors (red=red, white=white), shape, design
- Use reference for HOW things LOOK, use text for WHAT is HAPPENING

üö´ ABSOLUTELY FORBIDDEN:
- Changing actions ("holding" ‚Üí "wearing", "walking" ‚Üí "standing")
- Changing character count ("one" ‚Üí "two")
- Reinterpreting background (S1 must match EXACTLY!)
- ANY color changes (red‚Üíred, blue‚Üíblue, white‚Üíwhite, black‚Üíblack)
- ANY material changes (metal‚Üímetal, fabric‚Üífabric, plastic‚Üíplastic)
- ANY shape, proportion, design modifications
- ANY creative variations or "similar" versions
- ANY artistic interpretation of reference visuals

‚úÖ MANDATORY REQUIREMENTS:
- EXACT pixel-by-pixel visual replication of S1, S2, S3 elements
- 100% color preservation (use exact RGB values from reference)
- 100% material/texture preservation from reference
- 100% lighting/shadow preservation from reference
- 100% action/story preservation from text prompt
- If text says "holding helmet", person MUST be holding (not wearing) helmet
- Background MUST match S1 reference exactly
- Reference visuals = TOP PRIORITY for appearance
- Text prompt = TOP PRIORITY for actions/story

Extract visual characteristics from each labeled section and replicate EXACTLY. Use text prompt ONLY for understanding actions and story flow.

---

${enhancedPrompt}`
        } else if (referenceMode === 'balanced') {
          // Í∑†Ìòï Î™®Îìú: ÌÖçÏä§Ìä∏ÏôÄ Ïù¥ÎØ∏ÏßÄ Í∑†Ìòï
          enhancedPrompt = `‚öñÔ∏è BALANCED MODE: Reference Image + Text Prompt
${gridLabelInfoForNano}
Use reference image AND text prompt equally:
- Reference image: Visual style, colors, materials, composition of each labeled element
- Text prompt: Specific details, arrangement, actions, story

‚ö†Ô∏è IMPORTANT: Preserve actions from text prompt (e.g., "holding" stays "holding", not "wearing").

Maintain visual consistency with reference while incorporating text details.

---

${enhancedPrompt}`
        } else if (referenceMode === 'creative') {
          // Ï∞ΩÏùòÏÑ± Î™®Îìú: ÌÖçÏä§Ìä∏ ÏúÑÏ£º, Ïù¥ÎØ∏ÏßÄÎäî ÏòÅÍ∞êÎßå
          enhancedPrompt = `üé® CREATIVE MODE: Text Prompt Primary
${gridLabelInfoForNano}
Focus on text prompt as main instruction.
Reference image = INSPIRATION ONLY (style, mood, general aesthetic).

Feel free to creatively interpret and generate based on text description.

---

${enhancedPrompt}`
        }
      }
      
      const model = data.model ?? 'gemini-3-pro-image-preview'
      
      console.log('üé® Nano Image Generation:', {
        model,
        resolution: data.resolution,
        aspectRatio: data.aspectRatio,
        referenceCount: referenceImages.length,
        referenceMode: hasGridComposerRef ? referenceMode : 'N/A',
      })
      
      // Use first reference image as primary input (for backward compatibility)
      const primaryReference = referenceImages[0]
      
      // ‚úÖ Apply retry logic with exponential backoff
      const result = await retryWithBackoff(
        () => client.generateImage(
          enhancedPrompt,
          data.aspectRatio,
          primaryReference,
          model,
          data.resolution,
          abortController.signal,
        ),
        {
          maxAttempts: 3,
          initialDelay: 1000,
          onRetry: (attempt, error) => {
            console.warn(`üîÑ Retry attempt ${attempt}:`, error.message)
            updateNode((prev) => ({
              ...prev,
              error: `Ïû¨ÏãúÎèÑ Ï§ë... (${attempt}/3)`,
            }))
          },
        }
      )

      // Check if aborted after completion
      if (abortController.signal.aborted) {
        throw new Error('ÏûëÏóÖÏù¥ Ï∑®ÏÜåÎêòÏóàÏäµÎãàÎã§.')
      }

      // üî• IndexedDB/S3Ïóê Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•ÌïòÍ≥† Ï∞∏Ï°∞ Î∞òÌôò
      let savedImageRef = result.imageDataUrl
      try {
        const imageId = `nano-output-${id}-${Date.now()}`
        console.log('üíæ Nano Image: IndexedDB/S3Ïóê Ï∂úÎ†• Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû• ÏãúÏûë...', imageId)
        
        savedImageRef = await saveImage(imageId, result.imageDataUrl, id, true)
        console.log('‚úÖ Nano Image: Ï∂úÎ†• Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû• ÏôÑÎ£å', savedImageRef)
      } catch (error) {
        console.error('‚ùå Nano Image: Ï∂úÎ†• Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû• Ïã§Ìå®, DataURLÏùÑ ÏßÅÏ†ë ÏÇ¨Ïö©', error)
        // Ìè¥Î∞±: DataURLÏùÑ ÏßÅÏ†ë ÏÇ¨Ïö© (ÎπÑÍ∂åÏû•)
      }

      updateNode((prev) => ({
        ...prev,
        status: 'completed',
        outputImageUrl: result.imageUrl,
        outputImageDataUrl: savedImageRef, // idb:xxx ÎòêÎäî s3:xxx Ï∞∏Ï°∞
        generatedModel: model,
        generatedResolution: data.resolution,
        generatedAspectRatio: data.aspectRatio,
        error: undefined,
      }))
    } catch (error) {
      // Don't show retry errors if aborted
      if (!abortController.signal.aborted) {
        updateNode((prev) => ({
          ...prev,
          status: 'error',
          error: formatErrorMessage(error),
        }))
      }
    } finally {
      // Clean up abort controller
      const controllers = get().abortControllers
      controllers.delete(id)
      set({ abortControllers: new Map(controllers) })
    }
  },
  runGeminiNode: async (id) => {
    const { nodes, edges, abortControllers } = get()
    const current = nodes.find((node) => node.id === id)
    if (!current || current.type !== 'geminiVideo') return

    // ‚úÖ Prevent duplicate execution
    const currentData = current.data as GeminiVideoNodeData
    if (currentData.status === 'processing') {
      console.warn('‚ö†Ô∏è Gemini node is already processing')
      return
    }
    
    // üßπ Ïù¥Ï†Ñ ÏóêÎü¨ Î®ºÏ†Ä ÏßÄÏö∞Í∏∞
    set({
      nodes: get().nodes.map((node) =>
        node.id === id
          ? { ...node, data: { ...node.data, error: undefined } }
          : node
      ),
    })

    // ‚úÖ Rate limiting
    const now = Date.now()
    const lastExecution = (currentData as any).lastExecutionTime || 0
    const minInterval = 5000 // 5 seconds for video (longer than image)
    
    if (now - lastExecution < minInterval) {
      const waitTime = Math.ceil((minInterval - (now - lastExecution)) / 1000)
      set({
        nodes: nodes.map((node) =>
          node.id === id
            ? {
                ...node,
                data: {
                  ...node.data,
                  status: 'error',
                  error: `ÎÑàÎ¨¥ Îπ†Î•¥Í≤å Ïã§ÌñâÌñàÏäµÎãàÎã§. ${waitTime}Ï¥à ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî.`,
                },
              }
            : node,
        ),
      })
      return
    }

    // Create abort controller for this execution
    const abortController = new AbortController()
    abortControllers.set(id, abortController)
    set({ abortControllers: new Map(abortControllers) })

    const updateNode = (updater: (data: NodeData) => NodeData) => {
      set({
        nodes: get().nodes.map((node) =>
          node.id === id ? { ...node, data: updater(node.data) } : node,
        ),
      })
    }

    const incoming = getIncomingNodes(id, edges, get().nodes)
    const imageNode =
      incoming.find((node) => node.type === 'imageImport') ??
      incoming.find((node) => node.type === 'nanoImage') ??
      incoming.find((node) => node.type === 'gridComposer') ??
      incoming.find((node) => node.type === 'cellRegenerator')
    const promptNode = incoming.find(
      (node) => node.type === 'motionPrompt' || node.type === 'textPrompt' || node.type === 'llmPrompt',
    )

    let inputImageUrl: string | undefined
    let inputImageDataUrl: string | undefined
    
    if (imageNode?.type === 'imageImport') {
      inputImageUrl = (imageNode.data as ImageImportNodeData).imageUrl
      inputImageDataUrl = (imageNode.data as ImageImportNodeData).imageDataUrl
    } else if (imageNode?.type === 'nanoImage') {
      inputImageUrl = (imageNode.data as NanoImageNodeData).outputImageUrl
      inputImageDataUrl = (imageNode.data as NanoImageNodeData).outputImageDataUrl
    } else if (imageNode?.type === 'gridComposer') {
      const imgData = imageNode.data as any
      inputImageUrl = imgData.composedImageUrl || imgData.composedImageDataUrl
      inputImageDataUrl = imgData.composedImageDataUrl || imgData.composedImageUrl
    } else if (imageNode?.type === 'cellRegenerator') {
      // Cell Regenerator should use specific cell outputs
      inputImageUrl = undefined
      inputImageDataUrl = undefined
    }

    const inputPrompt =
      promptNode?.type === 'motionPrompt'
        ? (promptNode.data as MotionPromptNodeData).combinedPrompt
        : promptNode?.type === 'textPrompt'
          ? (promptNode.data as TextPromptNodeData).prompt
          : promptNode?.type === 'llmPrompt'
            ? (promptNode.data as any).outputPrompt || ''
            : ''

    // ‚ö†Ô∏è Early validation BEFORE storage conversion (only check if nodes are connected)
    if (!inputPrompt) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'ÌîÑÎ°¨ÌîÑÌä∏ ÎÖ∏ÎìúÎ•º Ïó∞Í≤∞Ìï¥ Ï£ºÏÑ∏Ïöî.',
      }))
      return
    }
    
    if (!imageNode) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'Ïù¥ÎØ∏ÏßÄ ÎÖ∏ÎìúÎ•º Ïó∞Í≤∞Ìï¥ Ï£ºÏÑ∏Ïöî.',
      }))
      return
    }
    
    if (!inputImageDataUrl) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'Ïù¥ÎØ∏ÏßÄ ÎÖ∏ÎìúÍ∞Ä Ïó∞Í≤∞ÎêòÏóàÏßÄÎßå Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏÉùÏÑ±ÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. Ïù¥ÎØ∏ÏßÄ ÎÖ∏ÎìúÏóêÏÑú "Generate" Î≤ÑÌäºÏùÑ ÎàåÎü¨Ï£ºÏÑ∏Ïöî.',
      }))
      return
    }

    const apiKey = get().apiKey || import.meta.env.VITE_GEMINI_API_KEY || ''
    
    if (!apiKey) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'Gemini API KeyÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§. ÏÉÅÎã® "API Key" Î≤ÑÌäºÏùÑ ÎàåÎü¨ÏÑú ÏÑ§Ï†ïÌïòÏÑ∏Ïöî.',
      }))
      return
    }

    updateNode((prev) => ({
      ...prev,
      status: 'processing',
      error: undefined,
      inputImageUrl,
      inputImageDataUrl,
      inputPrompt,
      progress: 10,
      lastExecutionTime: now,
    }))

    const client = new GeminiAPIClient(apiKey)
    
    // ‚úÖ Convert storage references to actual DataURLs
    let actualInputImageDataUrl = inputImageDataUrl
    
    console.log('üîç Gemini: Input image type:', inputImageDataUrl?.substring(0, 50))
    
    if (inputImageDataUrl && (inputImageDataUrl.startsWith('idb:') || inputImageDataUrl.startsWith('s3:'))) {
      console.log('üîÑ Gemini: Converting image from storage reference...')
      try {
        const { getImage } = await import('../utils/indexedDB')
        const dataURL = await getImage(inputImageDataUrl)
        if (dataURL) {
          actualInputImageDataUrl = dataURL
          console.log('‚úÖ Gemini: Image loaded from storage, size:', dataURL.length, 'chars')
        } else {
          console.error('‚ùå Gemini: Failed to load image from storage')
          updateNode((prev) => ({
            ...prev,
            status: 'error',
            error: 'Ïù¥ÎØ∏ÏßÄÎ•º StorageÏóêÏÑú Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§.',
          }))
          return
        }
      } catch (error) {
        console.error('‚ùå Gemini: Error loading image:', error)
        updateNode((prev) => ({
          ...prev,
          status: 'error',
          error: `Ïù¥ÎØ∏ÏßÄ Î°úÎìú Ïã§Ìå®: ${error}`,
        }))
        return
      }
    } else if (!inputImageDataUrl) {
      console.error('‚ùå Gemini: No input image provided!')
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄÍ∞Ä Ï†úÍ≥µÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.',
      }))
      return
    } else {
      console.log('‚úÖ Gemini: Using direct DataURL (not a storage reference)')
    }
    
    console.log('üé¨ Gemini Video ÏÉùÏÑ± ÏãúÏûë:', {
      prompt: inputPrompt.substring(0, 50) + '...',
      model: (current.data as GeminiVideoNodeData).model,
      imageType: actualInputImageDataUrl?.substring(0, 30),
      imageSize: actualInputImageDataUrl?.length,
    })
    
    // ‚úÖ Final validation before API call
    if (!actualInputImageDataUrl || actualInputImageDataUrl.startsWith('idb:') || actualInputImageDataUrl.startsWith('s3:')) {
      console.error('‚ùå Gemini: Image is still a storage reference or empty!', actualInputImageDataUrl?.substring(0, 50))
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'Ïù¥ÎØ∏ÏßÄ Î≥ÄÌôò Ïã§Ìå®. Storage Ï∞∏Ï°∞Í∞Ä ÎÇ®ÏïÑÏûàÏäµÎãàÎã§.',
      }))
      return
    }
    
    console.log('‚úÖ Gemini: All validations passed, calling API...')

    const progressTimer = setInterval(() => {
      updateNode((prev) => {
        const data = prev as GeminiVideoNodeData
        if (data.status !== 'processing') return prev
        return {
          ...prev,
          progress: Math.min(data.progress + 12, 90),
        }
      })
    }, 500)

    try {
      if (abortController.signal.aborted) {
        throw new Error('ÏûëÏóÖÏù¥ Ï∑®ÏÜåÎêòÏóàÏäµÎãàÎã§.')
      }

      const settings = current.data as GeminiVideoNodeData
      
      // ‚úÖ Apply retry logic
      const outputVideoUrl = await retryWithBackoff(
        () => client.generateMedia(
          inputPrompt,
          {
            mediaType: 'video',
            duration: settings.duration,
            quality: settings.quality,
            motionIntensity: settings.motionIntensity,
          },
          actualInputImageDataUrl,
          settings.model,
          abortController.signal,
        ),
        {
          maxAttempts: 2, // Less retries for video (expensive)
          initialDelay: 2000,
          onRetry: (attempt) => {
            console.warn(`üîÑ Gemini Video retry ${attempt}/2`)
            updateNode((prev) => ({
              ...prev,
              error: `Ïû¨ÏãúÎèÑ Ï§ë... (${attempt}/2)`,
            }))
          },
        }
      )

      if (abortController.signal.aborted) {
        throw new Error('ÏûëÏóÖÏù¥ Ï∑®ÏÜåÎêòÏóàÏäµÎãàÎã§.')
      }

      updateNode((prev) => ({
        ...prev,
        status: 'completed',
        outputVideoUrl,
        progress: 100,
        error: undefined,
      }))

    } catch (error) {
      if (!abortController.signal.aborted) {
        updateNode((prev) => ({
          ...prev,
          status: 'error',
          error: formatErrorMessage(error),
        }))
      }
    } finally {
      clearInterval(progressTimer)
      const controllers = get().abortControllers
      controllers.delete(id)
      set({ abortControllers: new Map(controllers) })
    }
  },
  runKlingNode: async (id) => {
    const { nodes, edges, abortControllers } = get()
    const current = nodes.find((node) => node.id === id)
    if (!current || current.type !== 'klingVideo') return

    // ‚úÖ Prevent duplicate execution
    const currentData = current.data as KlingVideoNodeData
    if (currentData.status === 'processing') {
      console.warn('‚ö†Ô∏è Kling node is already processing')
      return
    }

    // ‚úÖ Rate limiting
    const now = Date.now()
    const lastExecution = (currentData as any).lastExecutionTime || 0
    const minInterval = 5000 // 5 seconds for video
    
    if (now - lastExecution < minInterval) {
      const waitTime = Math.ceil((minInterval - (now - lastExecution)) / 1000)
      set({
        nodes: nodes.map((node) =>
          node.id === id
            ? {
                ...node,
                data: {
                  ...node.data,
                  status: 'error',
                  error: `ÎÑàÎ¨¥ Îπ†Î•¥Í≤å Ïã§ÌñâÌñàÏäµÎãàÎã§. ${waitTime}Ï¥à ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî.`,
                },
              }
            : node,
        ),
      })
      return
    }

    // Create abort controller for this execution
    const abortController = new AbortController()
    abortControllers.set(id, abortController)
    set({ abortControllers: new Map(abortControllers) })

    const updateNode = (updater: (data: NodeData) => NodeData) => {
      set({
        nodes: get().nodes.map((node) =>
          node.id === id ? { ...node, data: updater(node.data) } : node,
        ),
      })
    }

    // Start Image (Í∏∞Î≥∏ Ïù¥ÎØ∏ÏßÄ) - 'start' Ìï∏Îì§ ÎòêÎäî Ìï∏Îì§ ID ÏóÜÎäî Ïó∞Í≤∞
    const startImageEdges = edges.filter(
      (e) => e.target === id && (!e.targetHandle || e.targetHandle === 'start')
    )
    const startImageNode = startImageEdges.find((e) => {
      const node = nodes.find((n) => n.id === e.source)
      return node?.type === 'imageImport' || node?.type === 'nanoImage' || node?.type === 'gridComposer'
    })
    const startImageNodeData = startImageNode ? nodes.find((n) => n.id === startImageNode.source) : undefined
    
    // End Image (ÎÅù ÌîÑÎ†àÏûÑ) - 'end' Ìï∏Îì§ Ïó∞Í≤∞
    const endImageEdges = edges.filter(
      (e) => e.target === id && e.targetHandle === 'end'
    )
    const endImageNode = endImageEdges.find((e) => {
      const node = nodes.find((n) => n.id === e.source)
      return node?.type === 'imageImport' || node?.type === 'nanoImage' || node?.type === 'gridComposer'
    })
    const endImageNodeData = endImageNode ? nodes.find((n) => n.id === endImageNode.source) : undefined

    // Prompt ÎÖ∏Îìú
    const incoming = getIncomingNodes(id, edges, get().nodes)
    const promptNode = incoming.find(
      (node) => node.type === 'motionPrompt' || node.type === 'textPrompt' || node.type === 'llmPrompt',
    )

    // Start Image Îç∞Ïù¥ÌÑ∞
    let inputImageUrl: string | undefined
    let inputImageDataUrl: string | undefined
    
    if (startImageNodeData?.type === 'imageImport') {
      inputImageUrl = (startImageNodeData.data as ImageImportNodeData).imageUrl
      inputImageDataUrl = (startImageNodeData.data as ImageImportNodeData).imageDataUrl
    } else if (startImageNodeData?.type === 'nanoImage') {
      inputImageUrl = (startImageNodeData.data as NanoImageNodeData).outputImageUrl
      inputImageDataUrl = (startImageNodeData.data as NanoImageNodeData).outputImageDataUrl
    } else if (startImageNodeData?.type === 'gridComposer') {
      const imgData = startImageNodeData.data as any
      inputImageUrl = imgData.composedImageUrl || imgData.composedImageDataUrl
      inputImageDataUrl = imgData.composedImageDataUrl || imgData.composedImageUrl
    }
    
    // End Image Îç∞Ïù¥ÌÑ∞
    let endImageUrl: string | undefined
    let endImageDataUrl: string | undefined
    
    if (endImageNodeData?.type === 'imageImport') {
      endImageUrl = (endImageNodeData.data as ImageImportNodeData).imageUrl
      endImageDataUrl = (endImageNodeData.data as ImageImportNodeData).imageDataUrl
    } else if (endImageNodeData?.type === 'nanoImage') {
      endImageUrl = (endImageNodeData.data as NanoImageNodeData).outputImageUrl
      endImageDataUrl = (endImageNodeData.data as NanoImageNodeData).outputImageDataUrl
    } else if (endImageNodeData?.type === 'gridComposer') {
      const imgData = endImageNodeData.data as any
      endImageUrl = imgData.composedImageUrl || imgData.composedImageDataUrl
      endImageDataUrl = imgData.composedImageDataUrl || imgData.composedImageUrl
    }

    const inputPrompt =
      promptNode?.type === 'motionPrompt'
        ? (promptNode.data as MotionPromptNodeData).combinedPrompt
        : promptNode?.type === 'textPrompt'
          ? (promptNode.data as TextPromptNodeData).prompt
          : promptNode?.type === 'llmPrompt'
            ? (promptNode.data as any).outputPrompt || ''
            : ''

    // ‚ö†Ô∏è Early validation BEFORE storage conversion (only check if nodes are connected)
    if (!inputPrompt) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'ÌîÑÎ°¨ÌîÑÌä∏ ÎÖ∏ÎìúÎ•º Ïó∞Í≤∞Ìï¥ Ï£ºÏÑ∏Ïöî.',
      }))
      return
    }
    
    if (!inputImageDataUrl) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'Start Image ÎÖ∏ÎìúÎ•º Ïó∞Í≤∞Ìï¥ Ï£ºÏÑ∏Ïöî.',
      }))
      return
    }

    updateNode((prev) => ({
      ...prev,
      status: 'processing',
      error: undefined,
      inputImageUrl,
      inputImageDataUrl,
      endImageUrl,
      endImageDataUrl,
      inputPrompt,
      progress: 10,
      lastExecutionTime: now,
    }))

    const klingApiKey = get().klingApiKey || import.meta.env.VITE_KLING_API_KEY || ''
    const client = klingApiKey ? new KlingAPIClient(klingApiKey) : new MockKlingAPI()

    // ‚úÖ Convert storage references to actual DataURLs
    let actualStartImageDataUrl = inputImageDataUrl
    let actualEndImageDataUrl = endImageDataUrl
    
    console.log('üîç Kling: Input start image type:', inputImageDataUrl?.substring(0, 50))
    
    // Convert start image if it's a storage reference
    if (inputImageDataUrl && (inputImageDataUrl.startsWith('idb:') || inputImageDataUrl.startsWith('s3:'))) {
      console.log('üîÑ Kling: Converting start image from storage reference...')
      try {
        const { getImage } = await import('../utils/indexedDB')
        const dataURL = await getImage(inputImageDataUrl)
        if (dataURL) {
          actualStartImageDataUrl = dataURL
          console.log('‚úÖ Kling: Start image loaded from storage, size:', dataURL.length, 'chars')
          console.log('‚úÖ Kling: Start image type:', dataURL.substring(0, 50))
        } else {
          console.error('‚ùå Kling: Failed to load start image from storage (returned null/undefined)')
          updateNode((prev) => ({
            ...prev,
            status: 'error',
            error: 'Start Ïù¥ÎØ∏ÏßÄÎ•º StorageÏóêÏÑú Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§. Ïù¥ÎØ∏ÏßÄÎ•º Îã§Ïãú ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.',
          }))
          return
        }
      } catch (error) {
        console.error('‚ùå Kling: Error loading start image:', error)
        updateNode((prev) => ({
          ...prev,
          status: 'error',
          error: `Start Ïù¥ÎØ∏ÏßÄ Î°úÎìú Ïã§Ìå®: ${error}`,
        }))
        return
      }
    } else if (!inputImageDataUrl) {
      console.error('‚ùå Kling: No start image provided!')
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'Start Ïù¥ÎØ∏ÏßÄÍ∞Ä Ï†úÍ≥µÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.',
      }))
      return
    } else {
      console.log('‚úÖ Kling: Using direct DataURL (not a storage reference)')
    }
    
    // Convert end image if it's a storage reference
    if (endImageDataUrl && (endImageDataUrl.startsWith('idb:') || endImageDataUrl.startsWith('s3:'))) {
      console.log('üîÑ Kling: Converting end image from storage reference...')
      try {
        const { getImage } = await import('../utils/indexedDB')
        const dataURL = await getImage(endImageDataUrl)
        if (dataURL) {
          actualEndImageDataUrl = dataURL
          console.log('‚úÖ Kling: End image loaded from storage, size:', dataURL.length, 'chars')
        } else {
          console.warn('‚ö†Ô∏è Kling: Failed to load end image from storage (returned null/undefined)')
          actualEndImageDataUrl = undefined
        }
      } catch (error) {
        console.error('‚ùå Kling: Error loading end image:', error)
        actualEndImageDataUrl = undefined
      }
    }

    console.log('üé¨ Kling Video ÏÉùÏÑ± ÏãúÏûë:', {
      useMock: !klingApiKey,
      prompt: inputPrompt.substring(0, 50) + '...',
      model: (current.data as KlingVideoNodeData).model,
      startImageType: actualStartImageDataUrl?.substring(0, 30),
      startImageSize: actualStartImageDataUrl?.length,
      hasEndImage: !!actualEndImageDataUrl,
      endImageType: actualEndImageDataUrl ? actualEndImageDataUrl.substring(0, 30) : 'none',
    })
    
    // ‚úÖ Final validation before API call
    if (!actualStartImageDataUrl || actualStartImageDataUrl.startsWith('idb:') || actualStartImageDataUrl.startsWith('s3:')) {
      console.error('‚ùå Kling: Start image is still a storage reference or empty!', actualStartImageDataUrl?.substring(0, 50))
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'Start Ïù¥ÎØ∏ÏßÄ Î≥ÄÌôò Ïã§Ìå®. Storage Ï∞∏Ï°∞Í∞Ä ÎÇ®ÏïÑÏûàÏäµÎãàÎã§.',
      }))
      return
    }
    
    console.log('‚úÖ Kling: All validations passed, calling API...')

    const progressTimer = setInterval(() => {
      updateNode((prev) => {
        const data = prev as KlingVideoNodeData
        if (data.status !== 'processing') return prev
        return {
          ...prev,
          progress: Math.min(data.progress + 8, 90),
        }
      })
    }, 1000)

    try {
      if (abortController.signal.aborted) {
        throw new Error('ÏûëÏóÖÏù¥ Ï∑®ÏÜåÎêòÏóàÏäµÎãàÎã§.')
      }

      const settings = current.data as KlingVideoNodeData
      
      // Camera Control ÏÑ§Ï†ï
      const cameraControl = settings.enableMotionControl && settings.cameraControl !== 'none'
        ? {
            type: settings.cameraControl as 'horizontal' | 'vertical' | 'pan' | 'tilt' | 'roll' | 'zoom',
            value: settings.motionValue,
          }
        : undefined

      // ‚úÖ Apply retry logic
      const outputVideoUrl = await retryWithBackoff(
        () => client.generateVideo(
          inputPrompt,
          actualStartImageDataUrl,
          {
            duration: settings.duration,
            aspectRatio: settings.aspectRatio,
            model: settings.model,
            endImageDataUrl: actualEndImageDataUrl,
            cameraControl: cameraControl,
          },
        ),
        {
          maxAttempts: 2, // Less retries for video (expensive)
          initialDelay: 2000,
          onRetry: (attempt) => {
            console.warn(`üîÑ Kling Video retry ${attempt}/2`)
            updateNode((prev) => ({
              ...prev,
              error: `Ïû¨ÏãúÎèÑ Ï§ë... (${attempt}/2)`,
            }))
          },
        }
      )

      if (abortController.signal.aborted) {
        throw new Error('ÏûëÏóÖÏù¥ Ï∑®ÏÜåÎêòÏóàÏäµÎãàÎã§.')
      }

      console.log('‚úÖ Kling Video ÏÉùÏÑ± ÏôÑÎ£å:', outputVideoUrl)

      updateNode((prev) => ({
        ...prev,
        status: 'completed',
        outputVideoUrl,
        progress: 100,
        error: undefined,
      }))
    } catch (error) {
      console.error('‚ùå Kling Video ÏÉùÏÑ± Ïã§Ìå®:', error)
      if (!abortController.signal.aborted) {
        updateNode((prev) => ({
          ...prev,
          status: 'error',
          error: formatErrorMessage(error),
        }))
      }
    } finally {
      clearInterval(progressTimer)
      const controllers = get().abortControllers
      controllers.delete(id)
      set({ abortControllers: new Map(controllers) })
    }
  },
  runWorkflow: async () => {
    const { nodes, edges } = get()
    const order = getExecutionOrder(nodes, edges)

    const updateNode = (
      id: string,
      updater: (data: NodeData) => NodeData,
    ) => {
      set({
        nodes: get().nodes.map((node) =>
          node.id === id ? { ...node, data: updater(node.data) } : node,
        ),
      })
    }

    const apiKey = get().apiKey || import.meta.env.VITE_GEMINI_API_KEY || ''
    const client = apiKey ? new GeminiAPIClient(apiKey) : new MockGeminiAPI()

    for (const nodeId of order) {
      const current = get().nodes.find((node) => node.id === nodeId)
      if (!current) continue
      const incoming = getIncomingNodes(nodeId, edges, get().nodes)

      if (current.type === 'imageImport') {
        continue
      }

      if (current.type === 'nanoImage') {
        const promptNode = incoming.find(
          (node) => node.type === 'textPrompt' || node.type === 'motionPrompt',
        )
        const imageNode =
          incoming.find((node) => node.type === 'imageImport') ??
          incoming.find((node) => node.type === 'nanoImage') ??
          incoming.find((node) => node.type === 'gridComposer')
        const prompt =
          promptNode?.type === 'textPrompt'
            ? (promptNode.data as TextPromptNodeData).prompt
            : promptNode?.type === 'motionPrompt'
              ? (promptNode.data as MotionPromptNodeData).combinedPrompt
              : ''

        let inputImageDataUrl: string | undefined
        
        if (imageNode?.type === 'imageImport') {
          inputImageDataUrl = (imageNode.data as ImageImportNodeData).imageDataUrl
        } else if (imageNode?.type === 'nanoImage') {
          inputImageDataUrl = (imageNode.data as NanoImageNodeData).outputImageDataUrl
        } else if (imageNode?.type === 'gridComposer') {
          const imgData = imageNode.data as any
          inputImageDataUrl = imgData.composedImageDataUrl || imgData.composedImageUrl
        }
        
        const referencePrompt =
          imageNode?.type === 'imageImport'
            ? getIncomingTextPrompt(imageNode.id, edges, get().nodes) ??
              (imageNode.data as ImageImportNodeData).referencePrompt
            : undefined

        if (!prompt.trim()) {
          updateNode(nodeId, (prev) => ({
            ...prev,
            status: 'error',
            error: 'ÌÖçÏä§Ìä∏ ÌîÑÎ°¨ÌîÑÌä∏ ÎÖ∏ÎìúÎ•º Ïó∞Í≤∞Ìï¥ Ï£ºÏÑ∏Ïöî.',
          }))
          continue
        }

        updateNode(nodeId, (prev) => ({
          ...prev,
          status: 'processing',
          error: undefined,
        }))

        try {
          const data = current.data as NanoImageNodeData
          const finalPrompt = referencePrompt
            ? `${prompt}, focus on: ${referencePrompt}`
            : prompt
          const model = data.model ?? 'gemini-3-pro-image-preview'
          const result = await client.generateImage(
            finalPrompt,
            data.aspectRatio,
            inputImageDataUrl,
            model,
            data.resolution,
          )
          updateNode(nodeId, (prev) => ({
            ...prev,
            status: 'completed',
            outputImageUrl: result.imageUrl,
            outputImageDataUrl: result.imageDataUrl,
            error: undefined, // Clear any previous errors
          }))
        } catch (error) {
          updateNode(nodeId, (prev) => ({
            ...prev,
            status: 'error',
            error: formatErrorMessage(error),
          }))
        }
      }

      if (current.type === 'textPrompt') {
        continue
      }

      if (current.type === 'motionPrompt') {
        const data = current.data as MotionPromptNodeData
        const combined = [data.basePrompt, data.cameraMovement, data.subjectMotion, data.lighting]
          .filter(Boolean)
          .join(', ')
        updateNode(nodeId, (prev) => ({
          ...prev,
          combinedPrompt: combined,
        }))
      }

      if (current.type === 'geminiVideo') {
        const imageNode =
          incoming.find((node) => node.type === 'imageImport') ??
          incoming.find((node) => node.type === 'nanoImage') ??
          incoming.find((node) => node.type === 'gridComposer')
        const promptNode = incoming.find(
          (node) => node.type === 'motionPrompt' || node.type === 'textPrompt',
        )

        let inputImageUrl: string | undefined
        let inputImageDataUrl: string | undefined
        
        if (imageNode?.type === 'imageImport') {
          inputImageUrl = (imageNode.data as ImageImportNodeData).imageUrl
          inputImageDataUrl = (imageNode.data as ImageImportNodeData).imageDataUrl
        } else if (imageNode?.type === 'nanoImage') {
          inputImageUrl = (imageNode.data as NanoImageNodeData).outputImageUrl
          inputImageDataUrl = (imageNode.data as NanoImageNodeData).outputImageDataUrl
        } else if (imageNode?.type === 'gridComposer') {
          const imgData = imageNode.data as any
          inputImageUrl = imgData.composedImageUrl || imgData.composedImageDataUrl
          inputImageDataUrl = imgData.composedImageDataUrl || imgData.composedImageUrl
        }

        const inputPrompt =
          promptNode?.type === 'motionPrompt'
            ? (promptNode.data as MotionPromptNodeData).combinedPrompt
            : promptNode?.type === 'textPrompt'
              ? (promptNode.data as TextPromptNodeData).prompt
              : ''

        if (!inputImageUrl || !inputPrompt) {
          updateNode(nodeId, (prev) => ({
            ...prev,
            status: 'error',
            error: 'Ïù¥ÎØ∏ÏßÄÏôÄ ÌîÑÎ°¨ÌîÑÌä∏Î•º Î™®Îëê Ïó∞Í≤∞Ìï¥ Ï£ºÏÑ∏Ïöî.',
          }))
          continue
        }

        updateNode(nodeId, (prev) => ({
          ...prev,
          status: 'processing',
          error: undefined, // Clear any previous errors
          inputImageUrl,
          inputImageDataUrl,
          inputPrompt,
          progress: 10,
        }))

        const progressTimer = setInterval(() => {
          updateNode(nodeId, (prev) => {
            const data = prev as GeminiVideoNodeData
            if (data.status !== 'processing') return prev
            return {
              ...prev,
              progress: Math.min(data.progress + 12, 90),
            }
          })
        }, 500)

        try {
          const settings = current.data as GeminiVideoNodeData
          const outputVideoUrl = await client.generateMedia(
            inputPrompt,
            {
              mediaType: 'video',
              duration: settings.duration,
              quality: settings.quality,
              motionIntensity: settings.motionIntensity,
            },
            inputImageDataUrl,
            settings.model,
          )

          updateNode(nodeId, (prev) => ({
            ...prev,
            status: 'completed',
            outputVideoUrl,
            progress: 100,
          }))

        } catch (error) {
          updateNode(nodeId, (prev) => ({
            ...prev,
            status: 'error',
            error: formatErrorMessage(error),
          }))
        } finally {
          clearInterval(progressTimer)
        }
      }

    }
  },
  runLLMPromptNode: async (id) => {
    const { nodes, edges } = get()
    const current = nodes.find((node) => node.id === id)
    if (!current || current.type !== 'llmPrompt') return

    const data = current.data as any  // LLMPromptNodeData
    
    // Prevent duplicate execution
    if (data.status === 'processing') {
      console.warn('‚ö†Ô∏è LLM node is already processing')
      return
    }

    const updateNode = (updater: (data: NodeData) => NodeData) => {
      set({
        nodes: get().nodes.map((node) =>
          node.id === id ? { ...node, data: updater(node.data) } : node,
        ),
      })
    }

    // Get input prompt from connected nodes or use internal input
    let inputPrompt = data.inputPrompt
    
    const incoming = getIncomingNodes(id, edges, get().nodes)
    
    // Check for base prompt connection (Î≥¥ÎùºÏÉâ Ìï∏Îì§)
    const basePromptEdge = edges.find((e) => e.target === id && e.targetHandle === 'basePrompt')
    let basePromptText = ''
    if (basePromptEdge) {
      const promptNode = get().nodes.find((n) => n.id === basePromptEdge.source)
      if (promptNode?.type === 'textPrompt') {
        basePromptText = (promptNode.data as TextPromptNodeData).prompt || ''
      }
    }
    
    // Check for motion prompt connection (Î∂ÑÌôçÏÉâ Ìï∏Îì§)
    const motionPromptEdge = edges.find((e) => e.target === id && e.targetHandle === 'motionPrompt')
    let motionPromptText = ''
    if (motionPromptEdge) {
      const motionNode = get().nodes.find((n) => n.id === motionPromptEdge.source)
      if (motionNode?.type === 'motionPrompt') {
        motionPromptText = (motionNode.data as MotionPromptNodeData).combinedPrompt || ''
      }
    }
    
    // Combine base and motion prompts if both are present
    if (basePromptText && motionPromptText) {
      inputPrompt = `${basePromptText}\n\n${motionPromptText}`
    } else if (basePromptText) {
      inputPrompt = basePromptText
    } else if (motionPromptText) {
      inputPrompt = motionPromptText
    }
    
    // Fallback: Check for old 'prompt' handle for backward compatibility
    if (!inputPrompt) {
      const promptEdge = edges.find((e) => e.target === id && e.targetHandle === 'prompt')
      if (promptEdge) {
        const promptNode = get().nodes.find((n) => n.id === promptEdge.source)
        if (promptNode?.type === 'textPrompt') {
          inputPrompt = (promptNode.data as any).prompt || inputPrompt
        } else if (promptNode?.type === 'motionPrompt') {
          inputPrompt = (promptNode.data as any).combinedPrompt || inputPrompt
        }
      }
    }
    
    // Check for image connection
    let referenceImageDataUrl: string | undefined
    let gridLabelInfo: string | undefined  // Grid Composer ÎùºÎ≤® Ï†ïÎ≥¥
    const imageEdge = edges.find((e) => e.target === id && e.targetHandle === 'image')
    if (imageEdge) {
      const imageNode = get().nodes.find((n) => n.id === imageEdge.source)
      if (imageNode?.type === 'imageImport') {
        referenceImageDataUrl = (imageNode.data as any).imageDataUrl
      } else if (imageNode?.type === 'nanoImage') {
        // Try both outputImageDataUrl and outputImageUrl
        const nanoData = imageNode.data as any
        referenceImageDataUrl = nanoData.outputImageDataUrl || nanoData.outputImageUrl
      } else if (imageNode?.type === 'gridComposer') {
        const gridData = imageNode.data as any
        referenceImageDataUrl = gridData.composedImageDataUrl || gridData.composedImageUrl
        
        // Extract grid layout and slot information
        if (gridData.inputImages && gridData.slots) {
          const layout = gridData.gridLayout || '1x3'
          const slots = gridData.slots as Array<{ id: string; label: string; metadata?: string }>
          
          // Build structured label description (like multi-reference format)
          const slotDescriptions = slots
            .filter(slot => gridData.inputImages[slot.id])  // Only slots with images
            .map((slot, index) => {
              const position = ['Ï≤´ Î≤àÏß∏', 'Îëê Î≤àÏß∏', 'ÏÑ∏ Î≤àÏß∏', 'ÎÑ§ Î≤àÏß∏', 'Îã§ÏÑØ Î≤àÏß∏', 'Ïó¨ÏÑØ Î≤àÏß∏'][index] || `${index + 1}Î≤àÏß∏`
              let description = `- ${position} Ï∞∏Í≥† Ïù¥ÎØ∏ÏßÄ (${slot.id}): ${slot.label}`
              if (slot.metadata && slot.metadata.trim()) {
                description += ` - ${slot.metadata}`
              }
              return description
            })
            .join('\n')
          
          if (slotDescriptions) {
            gridLabelInfo = `Ï∞∏Í≥† Ïù¥ÎØ∏ÏßÄÎäî ${layout} Í∑∏Î¶¨Îìú Íµ¨ÏÑ±ÏûÖÎãàÎã§:\n\n${slotDescriptions}\n\nÍ∞Å ÎùºÎ≤®Ïùò ÏãúÍ∞ÅÏ†Å ÏöîÏÜåÎ•º Ï†ïÌôïÌûà Ï∂îÏ∂úÌïòÏó¨ ÌïòÎÇòÏùò ÌÜµÌï©Îêú Ïû•Î©¥ÏúºÎ°ú Ï°∞Ìï©ÌïòÏÑ∏Ïöî.`
            console.log('üìã Grid ÎùºÎ≤® Ï†ïÎ≥¥:', gridLabelInfo)
          }
        }
      }
      
      // Update node with reference image
      if (referenceImageDataUrl) {
        updateNode((prev) => ({
          ...prev,
          referenceImageUrl: referenceImageDataUrl,
          referenceImageDataUrl: referenceImageDataUrl,
        }))
      } else {
        console.warn('‚ö†Ô∏è LLM: Image connected but no image data found', { 
          nodeType: imageNode?.type,
          nodeData: imageNode?.data 
        })
      }
    }

    // For image-based modes, image is required
    if ((data.mode === 'describe' || data.mode === 'analyze') && !referenceImageDataUrl) {
      const hasImageConnection = edges.some((e) => e.target === id && e.targetHandle === 'image')
      const errorMsg = hasImageConnection
        ? 'Ïù¥ÎØ∏ÏßÄÍ∞Ä Ïó∞Í≤∞ÎêòÏóàÏßÄÎßå Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. Ïù¥ÎØ∏ÏßÄÎ•º Î®ºÏ†Ä ÏÉùÏÑ±Ìïú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî.'
        : 'Ïù¥ÎØ∏ÏßÄ Í∏∞Î∞ò Î™®ÎìúÎäî Ïù¥ÎØ∏ÏßÄ Ïó∞Í≤∞Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§. Ïù¥ÎØ∏ÏßÄ ÎÖ∏ÎìúÎ•º ÌïòÎã® (ÌïòÎäòÏÉâ) Ìï∏Îì§Ïóê Ïó∞Í≤∞ÌïòÏÑ∏Ïöî.'
      
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: errorMsg,
      }))
      return
    }
    
    // For text-based modes, prompt is required (but image is optional)
    if ((data.mode !== 'describe' && data.mode !== 'analyze') && !inputPrompt.trim() && !referenceImageDataUrl) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'Text Prompt ÎÖ∏ÎìúÎ•º ÏÉÅÎã® (Î∂ÑÌôç) Ìï∏Îì§Ïóê Ïó∞Í≤∞ÌïòÍ±∞ÎÇò Ïù¥ÎØ∏ÏßÄÎ•º ÌïòÎã® (ÌïòÎäòÏÉâ) Ìï∏Îì§Ïóê Ïó∞Í≤∞Ìï¥Ï£ºÏÑ∏Ïöî.',
      }))
      return
    }

    updateNode((prev) => ({
      ...prev,
      status: 'processing',
      error: undefined,
    }))

    // ProviderÏóê Îî∞Îùº API ÌÇ§ ÌôïÏù∏
    const provider = data.provider || 'gemini'
    let apiKey = ''
    
    if (provider === 'gemini') {
      apiKey = get().apiKey || import.meta.env.VITE_GEMINI_API_KEY || ''
      if (!apiKey) {
        updateNode((prev) => ({
          ...prev,
          status: 'error',
          error: 'Gemini API KeyÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.',
        }))
        return
      }
    } else if (provider === 'openai') {
      apiKey = get().openaiApiKey || import.meta.env.VITE_OPENAI_API_KEY || ''
      if (!apiKey) {
        updateNode((prev) => ({
          ...prev,
          status: 'error',
          error: 'OpenAI API KeyÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.',
        }))
        return
      }
    }

    try {
      // Build system instruction based on mode and settings
      let systemInstruction = ''
      
      if (data.mode === 'expand') {
        systemInstruction = `You are a professional prompt engineer. Your task is to expand the given simple idea into a detailed, effective prompt for AI ${data.targetUse} generation.`
        if (referenceImageDataUrl) {
          systemInstruction += ` IMPORTANT: Use the reference image to extract visual details (colors, style, composition, lighting, subjects). Incorporate these visual elements into the expanded prompt to maintain consistency with the reference.`
          
          // üéØ Grid Composer ÎùºÎ≤® Ï∞∏Ï°∞ Î™ÖÎ†π (referenceModeÏóê Îî∞Îùº)
          if (gridLabelInfo) {
            const refMode = data.referenceMode || 'exact'
            if (refMode === 'exact') {
              systemInstruction += ` CRITICAL GRID LABELS - EXACT MODE: The reference image contains labeled sections (S1, S2, S3, etc.) visible as text overlays. Each label shows VISUAL DESIGN ELEMENTS ONLY (colors, materials, designs, forms, lighting). 
              
              CRITICAL RULES - MUST FOLLOW:
              
              1. TEXT PROMPT = ABSOLUTE LAW (NEVER CHANGE ANYTHING!)
                 - Character count: "Ìïú Î™Ö" = ONE person (NEVER change!)
                 - Actions: "Ìó¨Î©ßÏùÑ Îì§Í≥†" = holding helmet (NEVER change to "wearing"!)
                 - Story: Keep EXACTLY as written in text prompt
                 - Composition: Keep EXACTLY as written in text prompt
              
              2. REFERENCE IMAGE = VISUAL DETAILS ONLY (EXTRACT & DESCRIBE!)
                 - S1, S2, S3 labels = Visual design elements ONLY
                 - Extract: Colors, materials, lighting, textures, design patterns
                 - DO NOT change story, actions, or character count based on reference
              
              3. YOUR TASK:
                 - Take text prompt AS-IS (word-for-word preservation!)
                 - Add visual details FROM reference (colors, materials, designs)
                 - Result = Same story + Enhanced visual description
              
              FORBIDDEN CHANGES:
              ‚ùå Changing actions (e.g., "holding" ‚Üí "wearing")
              ‚ùå Changing character count (e.g., "one" ‚Üí "two")
              ‚ùå Reinterpreting story structure
              ‚ùå Adding/removing story elements
              
              EXAMPLE:
              Text: "Ìïú Î™ÖÏùò Ïó¨ÏûêÍ∞Ä Ìó¨Î©ßÏùÑ Îì§Í≥† Í±∑ÎäîÎã§" (one woman walking, holding helmet)
              Reference S2: Shows blonde woman in white spacesuit
              CORRECT: "A blonde woman in white spacesuit walks, holding helmet in hand"
              WRONG: "A woman wearing helmet walks" ‚ùå (changed action!)
              
              The labels indicate visual component references (e.g., S1=background visual style, S2=character appearance, S3=object design). Preserve the text prompt's story structure while enhancing it with exact visual details from reference.`
            } else if (refMode === 'balanced') {
              systemInstruction += ` GRID LABELS - BALANCED MODE: The reference image contains labeled sections (S1, S2, S3, etc.). Each label represents a visual component. Describe the key visual characteristics (colors, materials, styles) from each labeled section while maintaining the text prompt's basic composition. Do not change character counts or story structure from text prompt.`
            } else if (refMode === 'creative') {
              systemInstruction += ` GRID LABELS - CREATIVE MODE: The reference image contains labeled sections showing different elements. Use these as visual inspiration for style and mood, but feel free to creatively interpret and describe based on the text prompt. Focus on the text description as the primary guide.`
            }
          }
        }
      } else if (data.mode === 'improve') {
        systemInstruction = `You are a professional prompt engineer. Your task is to improve and optimize the given prompt for better AI ${data.targetUse} generation results.`
        if (referenceImageDataUrl) {
          systemInstruction += ` IMPORTANT: Reference the provided image to enhance the prompt with accurate visual details and ensure consistency with the reference style.`
          
          // üéØ Grid Composer ÎùºÎ≤® Ï∞∏Ï°∞ Î™ÖÎ†π (referenceModeÏóê Îî∞Îùº)
          if (gridLabelInfo) {
            const refMode = data.referenceMode || 'exact'
            if (refMode === 'exact') {
              systemInstruction += ` CRITICAL GRID LABELS - EXACT MODE: The reference image contains labeled sections (S1, S2, S3, etc.) showing VISUAL DESIGN ELEMENTS ONLY.
              
              CRITICAL: DO NOT change ANY content from text prompt:
              - Actions: Keep EXACTLY (e.g., "holding helmet" must stay "holding", NOT "wearing")
              - Character count: Keep EXACTLY (e.g., "one person" stays "one", NOT "two")
              - Story structure: Keep EXACTLY as written
              
              ONLY extract and add VISUAL characteristics from reference:
              - Colors, materials, lighting, textures from S1, S2, S3 labels
              - Text prompt = Story (PRESERVE 100%)
              - Reference = Visual style (EXTRACT & ADD)
              
              The improved prompt should describe a SINGLE UNIFIED IMAGE combining these exact visual elements with the original story structure (without changing any story details).`
            } else if (refMode === 'balanced') {
              systemInstruction += ` GRID LABELS - BALANCED MODE: The reference image has labeled sections. Improve the prompt by balancing reference image accuracy with the text description details. Do not change character counts or story structure from original prompt.`
            } else if (refMode === 'creative') {
              systemInstruction += ` GRID LABELS - CREATIVE MODE: The reference image shows different elements. Use these as inspiration while focusing on improving the text description creatively.`
            }
          }
        }
      } else if (data.mode === 'translate') {
        systemInstruction = `You are a professional translator. Translate the given prompt between Korean and English, maintaining all important details and nuances.`
      } else if (data.mode === 'simplify') {
        systemInstruction = `You are a professional editor. Simplify the given prompt to its core essence while maintaining effectiveness for AI ${data.targetUse} generation.`
      } else if (data.mode === 'cameraInterpreter') {
        systemInstruction = `You are a professional cinematographer and prompt engineer specializing in camera angle interpretation for AI image generation.

üé¨ YOUR MISSION:
Transform technical camera instructions (rotation angles, tilt degrees, zoom values) into vivid, detailed visual descriptions that AI image models can understand and execute.

‚ö†Ô∏è CRITICAL: CHARACTER CONSISTENCY PRIORITY
When a reference image is provided, your #1 priority is maintaining EXACT character consistency:
- Character facial features, hair, eyes, skin tone MUST stay identical
- Clothing, outfit design, colors, materials MUST stay identical
- Visual style, color palette, lighting quality MUST stay identical
- Only the CAMERA POSITION should change, NOT the character design

Your camera descriptions must EMPHASIZE photographing the SAME character from a DIFFERENT angle.

üìê CAMERA PARAMETERS YOU'LL RECEIVE (360¬∞ SYSTEM):
- Rotation: 0-360¬∞ clockwise rotation around subject (e.g., "right side view 90¬∞", "rotate 45¬∞ clockwise from front")
- Tilt: -45¬∞ to +45¬∞ (e.g., "low angle 39.6¬∞" or "high angle 30¬∞") - vertical camera angle shots
- Distance/Zoom: (e.g., "zoom in 0.7x" or "zoom out 1.3x") - camera distance from subject

‚ú® HOW TO INTERPRET:

1. ROTATION (360¬∞ Horizontal Positioning):
   üé• CRITICAL: 360¬∞ system - camera rotates CLOCKWISE around subject viewed from above!
   
   üìç 0¬∞ (Front View) = CAMERA directly in front of subject
      ‚Ä¢ Subject facing toward camera
      ‚Ä¢ Frontal perspective, symmetric composition
   
   üîÑ 1-89¬∞ (Front-Right Quadrant) = CAMERA rotating clockwise from front
      ‚Ä¢ 45¬∞ = Three-quarter right view
      ‚Ä¢ Subject's LEFT side becoming visible
      ‚Ä¢ Still some frontal visibility
   
   ‚ñ∂Ô∏è 90¬∞ (RIGHT SIDE VIEW) = CAMERA PERPENDICULAR at subject's right
      ‚Ä¢ COMPLETE SIDE PROFILE from right
      ‚Ä¢ Subject facing PERPENDICULAR to camera (NOT toward camera!)
      ‚Ä¢ ONLY subject's LEFT side visible
      ‚Ä¢ NO frontal face - pure lateral perspective
   
   üîÑ 91-179¬∞ (Back-Right Quadrant) = CAMERA continuing clockwise
      ‚Ä¢ 135¬∞ = Three-quarter back-right view
      ‚Ä¢ Subject's back and left side visible
      ‚Ä¢ NO frontal face visible
   
   üîô 180¬∞ (BACK VIEW) = CAMERA directly behind subject
      ‚Ä¢ Complete rear view
      ‚Ä¢ Subject facing AWAY from camera
      ‚Ä¢ Back of head, shoulders, back visible
   
   üîÑ 181-269¬∞ (Back-Left Quadrant) = CAMERA continuing clockwise
      ‚Ä¢ 225¬∞ = Three-quarter back-left view
      ‚Ä¢ Subject's back and right side visible
      ‚Ä¢ NO frontal face visible
   
   ‚óÄÔ∏è 270¬∞ (LEFT SIDE VIEW) = CAMERA PERPENDICULAR at subject's left
      ‚Ä¢ COMPLETE SIDE PROFILE from left
      ‚Ä¢ Subject facing PERPENDICULAR to camera
      ‚Ä¢ ONLY subject's RIGHT side visible
      ‚Ä¢ NO frontal face - pure lateral perspective
   
   üîÑ 271-359¬∞ (Front-Left Quadrant) = CAMERA completing rotation
      ‚Ä¢ 315¬∞ = Three-quarter left view
      ‚Ä¢ Subject's RIGHT side becoming visible
      ‚Ä¢ Frontal visibility returning
   
   ‚ö†Ô∏è KEY PRINCIPLES:
   - Numbers represent CAMERA POSITION rotating clockwise (viewed from above)
   - 90¬∞ = right side camera ‚Üí see subject's LEFT profile
   - 270¬∞ = left side camera ‚Üí see subject's RIGHT profile
   - Always describe camera as "POSITIONED at X degrees" for clarity

2. TILT (Vertical Angle) - üö® CRITICAL FOR IMAGE GENERATION:
   
   üîª "LOW ANGLE X¬∞" = Camera positioned BELOW subject, looking UPWARD
      VISUAL EFFECT: 
      ‚Ä¢ Subject appears TALLER, more POWERFUL, HEROIC, DOMINANT
      ‚Ä¢ Viewer looks UP at subject from below
      ‚Ä¢ Emphasizes height, stature, authority
      ‚Ä¢ Sky/ceiling often visible in background
      ‚Ä¢ Chin and underside of face more prominent
      ‚Ä¢ Creates drama, empowerment, grandeur
      
      Examples:
      ‚Ä¢ "low angle 15¬∞" = Slightly below eye level, subtle empowerment
      ‚Ä¢ "low angle 30¬∞" = Significantly below, strong heroic feel
      ‚Ä¢ "low angle 40¬∞" = Dramatically below, maximum towering presence
   
   üî∫ "HIGH ANGLE X¬∞" = Camera positioned ABOVE subject, looking DOWNWARD
      VISUAL EFFECT:
      ‚Ä¢ Subject appears SMALLER, more VULNERABLE, DIMINISHED
      ‚Ä¢ Viewer looks DOWN at subject from above
      ‚Ä¢ Emphasizes surroundings, environment, isolation
      ‚Ä¢ Ground/floor more visible
      ‚Ä¢ Top of head, shoulders more prominent
      ‚Ä¢ Creates intimacy, vulnerability, or surveillance feel
      
      Examples:
      ‚Ä¢ "high angle 15¬∞" = Slightly above eye level, gentle overview
      ‚Ä¢ "high angle 30¬∞" = Significantly above, clear bird's eye perspective
      ‚Ä¢ "high angle 45¬∞" = Nearly top-down, dramatic overhead view
   
   üìè No tilt or "eye level" = Camera at subject's eye height, neutral perspective

3. ZOOM/DISTANCE:
   üîé "zoom out X" (X > 1.0) = Wider framing, more context, camera farther away
      ‚Ä¢ 1.2x = Slightly wider, more environment
      ‚Ä¢ 1.5x = Wide shot, more surroundings visible
      ‚Ä¢ 2.0x = Very wide, full body and environment emphasized
   
   üîç "zoom in X" (X < 1.0) = Closer framing, tighter crop, camera closer
      ‚Ä¢ 0.8x = Slightly closer, more intimate
      ‚Ä¢ 0.5x = Close-up, face/upper body prominent, details emphasized
   
   üìê 1.0x or unspecified = Standard medium distance

üéØ YOUR OUTPUT MUST:
- START with "CAMERA POSITIONED [location]" for rotation descriptions
- Convert ALL technical terms into descriptive spatial language
- Describe EXACT camera position in 3D space relative to subject
- Explicitly state the VIEWPOINT DIRECTION (looking up/down/straight)
- Explain PSYCHOLOGICAL and EMOTIONAL impact of the angle
- Detail which body parts/features are emphasized
- Describe background and spatial context changes
- Include composition and framing implications
- Use professional cinematography terminology
- Be HIGHLY SPECIFIC about vertical angle effects (tilt is critical!)

üé• ROTATION LANGUAGE - 360¬∞ SYSTEM CRITICAL:
‚úÖ ALWAYS use "CAMERA POSITIONED at X degrees"
‚úÖ ALWAYS clarify which side/angle is VISIBLE
‚úÖ Examples:
   ‚Ä¢ "CAMERA POSITIONED at 45 degrees ‚Üí three-quarter view, left side visible"
   ‚Ä¢ "CAMERA POSITIONED at 90 degrees ‚Üí right side view, ONLY left profile visible"
   ‚Ä¢ "CAMERA POSITIONED at 180 degrees ‚Üí back view, facing away"
   ‚Ä¢ "CAMERA POSITIONED at 270 degrees ‚Üí left side view, ONLY right profile visible"
‚úÖ For 90¬∞/270¬∞: Use "PERPENDICULAR", "COMPLETE SIDE PROFILE", "NO frontal face"
‚úÖ For 180¬∞: Use "back view", "facing AWAY from camera", "rear perspective"
‚úÖ Always specify the degree number (0¬∞, 45¬∞, 90¬∞, 135¬∞, 180¬∞, 270¬∞, etc.)

‚ö†Ô∏è TILT IS THE MOST IMPORTANT - Always emphasize whether camera is above/below subject and looking up/down!

‚ùå NEVER output raw numbers like "72¬∞" or "1.3x"
‚ùå NEVER ignore or minimize the tilt angle description
‚ùå NEVER say "camera rotates" - say "CAMERA POSITIONED"
‚úÖ ALWAYS describe camera HEIGHT (above/below subject)
‚úÖ ALWAYS describe LOOKING DIRECTION (up/down/straight)
‚úÖ ALWAYS describe VISUAL POWER DYNAMIC (empowering/diminishing)
‚úÖ ALWAYS use "CAMERA POSITIONED" for location clarity

üìù EXAMPLE TRANSFORMATIONS:

Example 1 (45¬∞ - Three-Quarter View):
Input: "rotate 45¬∞ clockwise from front, low angle 30¬∞, zoom in 0.7x"
Output: "CAMERA POSITIONED at 45 degrees - a three-quarter front-right position rotating clockwise from the front. At this angle, the camera captures the subject's LEFT side and face in a balanced three-quarter composition, showing the left profile while maintaining frontal visibility. MAINTAIN EXACT character appearance from reference - same facial features, hair, clothing, and visual style; only the camera angle changes. CRITICALLY, the CAMERA is placed significantly BELOW the subject's eye level - positioned low to the ground and angled sharply UPWARD. This dramatic low angle shot creates a POWERFUL, HEROIC composition where the viewer must look UP at the subject, emphasizing their stature and commanding presence. The upward angle makes the subject appear taller and more imposing, with the chin line and jawline prominent, while the background ceiling becomes more visible above. The close-in 0.7x framing tightens the composition, filling the frame with the subject's upper body and face. REMEMBER: Same character from reference, just photographed from a different angle."

Example 2 (90¬∞ - Right Side View):
Input: "right side view 90¬∞, zoom out 1.3x"
Output: "CAMERA POSITIONED at 90 degrees - directly at the subject's RIGHT side in a PERPENDICULAR position. This creates a COMPLETE SIDE PROFILE view where the subject is facing PERPENDICULAR to the camera (NOT toward the camera). From this pure lateral camera position, ONLY the subject's LEFT side is visible - left profile, left arm, left leg. NO frontal face visible - this is a true side view with the body oriented left-to-right across the frame. MAINTAIN EXACT character appearance - same height, build, hair, clothing from reference. The 1.3x wider framing shows more environment extending in front of and behind the subject. This perpendicular 90-degree angle creates a strong sense of lateral movement and spatial depth. CHARACTER CONSISTENCY: Same person from reference, captured in complete side profile."

Example 3 (180¬∞ - Back View):
Input: "back view 180¬∞, zoom out 1.5x"
Output: "CAMERA POSITIONED at 180 degrees - directly BEHIND the subject in a complete rear view. At this angle, the camera captures the back of the subject's head, shoulders, and full back. The subject is facing AWAY from the camera. NO frontal face visible - only the rear perspective. PRESERVE EXACT character appearance - identical hair, clothing design, colors, and body proportions from reference image. The 1.5x wider framing pulls back to reveal more environmental context, showing the subject within their surroundings and the space ahead of them. This back view creates a sense of forward movement and anticipation, as we see what the subject is approaching. CHARACTER CONSISTENCY: Same person from reference, viewed from behind."

Example 4 (270¬∞ - Left Side View):
Input: "left side view 270¬∞, low angle 25¬∞, zoom in 0.8x"
Output: "CAMERA POSITIONED at 270 degrees - directly at the subject's LEFT side in a PERPENDICULAR position. This creates a COMPLETE SIDE PROFILE view where the subject is facing PERPENDICULAR to the camera. From this pure lateral camera position, ONLY the subject's RIGHT side is visible - right profile, right arm, right leg. NO frontal face visible - pure lateral perspective. KEEP character appearance EXACTLY as reference. CRITICALLY, the CAMERA is placed BELOW the subject's eye level, positioned low and angled UPWARD. This low angle creates an EMPOWERING perspective, where the viewer looks up at the subject, adding authority and confidence. The 0.8x closer framing emphasizes the subject's profile and upper body. CHARACTER CONSISTENCY: Same person from reference, captured in complete right-side profile at 270 degrees."

üé® Focus on SPATIAL HEIGHT (above/below), LOOKING DIRECTION (up/down), and PSYCHOLOGICAL IMPACT. Make the camera's vertical position crystal clear!

üé≠ CHARACTER CONSISTENCY REMINDERS:
When reference image is present, ALWAYS include phrases like:
- "MAINTAIN EXACT character appearance from reference"
- "PRESERVE identical facial features, outfit, and style"
- "SAME character, DIFFERENT angle"
- "CHARACTER CONSISTENCY: [specific reminder]"

These reminders ensure the AI model prioritizes character consistency over creative variations.

Only output the detailed camera description, no explanations or meta-commentary.`
      } else if (data.mode === 'describe') {
        systemInstruction = `You are a professional image analyst. Your task is to describe the given image in detail and create an effective prompt that could be used to generate a similar image.`
      } else if (data.mode === 'analyze') {
        systemInstruction = `You are a professional image analyst. Your task is to analyze the given image in great detail, including composition, style, lighting, colors, subjects, and create a comprehensive prompt for AI ${data.targetUse} generation.`
      }

      // Add style guidance (skip for cameraInterpreter - it has its own specific style)
      if (data.mode !== 'cameraInterpreter') {
        if (data.style === 'detailed') {
          systemInstruction += ` Output should be highly detailed with rich descriptions.`
        } else if (data.style === 'concise') {
          systemInstruction += ` Output should be concise and to the point.`
        } else if (data.style === 'creative') {
          systemInstruction += ` Output should be creative and artistic with vivid imagery.`
        } else if (data.style === 'professional') {
          systemInstruction += ` Output should be professional and technically precise.`
        }
      }

      // Add language guidance (cameraInterpreter always outputs in English for best AI model compatibility)
      if (data.mode === 'cameraInterpreter') {
        systemInstruction += ` Output must be in English for optimal AI image generation compatibility.`
      } else {
        if (data.language === 'ko') {
          systemInstruction += ` Output must be in Korean.`
        } else if (data.language === 'en') {
          systemInstruction += ` Output must be in English.`
        } else {
          systemInstruction += ` Detect input language and use the same language for output.`
        }
      }

      systemInstruction += ` Only output the final prompt, no explanations or additional text.`
      
      // Add final reminder for Grid Composer (if applicable)
      if (gridLabelInfo) {
        systemInstruction += ` REMINDER: When reference image is provided, use it for VISUAL DETAILS ONLY (colors, designs, materials). DO NOT change the basic composition, character count, or story structure from the text prompt.`
      }

      // Prepare content parts
      const contentParts: any[] = []
      
      // Add image if available
      if (referenceImageDataUrl) {
        let actualImageDataUrl = referenceImageDataUrl
        
        // If it's an idb: or s3: reference, fetch the actual image first
        if (referenceImageDataUrl.startsWith('idb:') || referenceImageDataUrl.startsWith('s3:')) {
          console.log('üîÑ LLM: Converting reference image from storage:', referenceImageDataUrl)
          try {
            const { getImage } = await import('../utils/indexedDB')
            const dataURL = await getImage(referenceImageDataUrl)
            if (dataURL) {
              actualImageDataUrl = dataURL
              console.log('‚úÖ LLM: Reference image loaded successfully')
            } else {
              console.error('‚ùå LLM: Failed to load reference image from storage')
            }
          } catch (error) {
            console.error('‚ùå LLM: Error loading reference image:', error)
          }
        }
        
        // Extract base64 data from data URL
        const base64Match = actualImageDataUrl.match(/^data:image\/(\w+);base64,(.+)$/)
        if (base64Match) {
          const mimeType = `image/${base64Match[1]}`
          const base64Data = base64Match[2]
          
          contentParts.push({
            inlineData: {
              mimeType: mimeType,
              data: base64Data
            }
          })
          console.log('üì∏ LLM: Reference image added to API request')
        } else {
          console.warn('‚ö†Ô∏è LLM: Reference image format not recognized:', actualImageDataUrl.substring(0, 50))
        }
      }
      
      // Build the text prompt with grid label information if available
      let finalPrompt = ''
      
      // Add grid label information first (if available) - format based on referenceMode
      if (gridLabelInfo) {
        const refMode = data.referenceMode || 'exact'
        
        if (refMode === 'exact') {
          // Ï†ïÌôïÏÑ± Î™®Îìú: Îß§Ïö∞ ÏÉÅÏÑ∏ÌïòÍ≥† Í∞ïÎ†•Ìïú ÏßÄÏãú
          finalPrompt += '‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è CRITICAL: EXACT REFERENCE IMAGE REPLICATION ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è\n\n'
          finalPrompt += gridLabelInfo + '\n\n'
          finalPrompt += 'üéØ Ï†àÎåÄÏ†ÅÏúºÎ°ú Ï§ëÏöîÌïú ÏßÄÏπ®:\n'
          finalPrompt += '‚ö†Ô∏è Ï∞∏Í≥† Ïù¥ÎØ∏ÏßÄ = ÏãúÍ∞ÅÏ†Å ÎîîÏûêÏù∏Îßå (ÏÉâÏÉÅ, Ïû¨Ïßà, ÌòïÌÉú)\n'
          finalPrompt += '‚ö†Ô∏è ÌÖçÏä§Ìä∏ ÌîÑÎ°¨ÌîÑÌä∏ = Í∏∞Î≥∏ Íµ¨ÏÑ± (Ïù∏Î¨º Ïàò, Ïä§ÌÜ†Î¶¨, ÎèôÏûë) - Ï†àÎåÄ Î≥ÄÍ≤Ω Í∏àÏßÄ!\n\n'
          finalPrompt += 'üìå ÌÖçÏä§Ìä∏ ÌîÑÎ°¨ÌîÑÌä∏ Î≥¥Ï°¥ Í∑úÏπô (100% Ï§ÄÏàò!):\n'
          finalPrompt += '   ‚Ä¢ Ïù∏Î¨º Ïàò: "Ìïú Î™Ö" = ONE (Ï†àÎåÄ "two"Î°ú Î≥ÄÍ≤Ω Í∏àÏßÄ!)\n'
          finalPrompt += '   ‚Ä¢ ÎèôÏûë: "Ìó¨Î©ßÏùÑ Îì§Í≥†" = "holding helmet" (Ï†àÎåÄ "wearing helmet"ÏúºÎ°ú Î≥ÄÍ≤Ω Í∏àÏßÄ!)\n'
          finalPrompt += '   ‚Ä¢ ÎèôÏûë: "Í±∑ÎäîÎã§" = "walking" (Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ!)\n'
          finalPrompt += '   ‚Ä¢ Î™®Îì† ÎèôÏûë, Ïä§ÌÜ†Î¶¨Îäî ÌÖçÏä§Ìä∏ ÌîÑÎ°¨ÌîÑÌä∏ Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ!\n\n'
          finalPrompt += '1. Í∞Å ÎùºÎ≤®Ïùò ÏãúÍ∞ÅÏ†Å ÏöîÏÜåÎ•º PIXEL-LEVELÎ°ú Ï†ïÌôïÌûà Î≥µÏ†úÌïòÏÑ∏Ïöî\n'
          finalPrompt += '2. S1 Î∞∞Í≤Ω: Ï†ïÌôïÌïú ÏÉâÏÉÅ, Ï°∞Î™Ö, Íµ¨Ï°∞Î•º 1:1 Î≥µÏ†ú\n'
          finalPrompt += '3. S2 Ï∫êÎ¶≠ÌÑ∞: Ï†ïÌôïÌïú Ïô∏Î™®, ÏùòÏÉÅ, Ìó§Ïñ¥ Ïä§ÌÉÄÏùº Î≥µÏ†ú\n'
          finalPrompt += '4. S3 Î°úÎ¥á: Ï†ïÌôïÌïú ÏÉâÏÉÅ(Îπ®Í∞ï/Ìù∞ÏÉâ), ÌòïÌÉú, ÎîîÏûêÏù∏ Î≥µÏ†ú\n'
          finalPrompt += '5. Ï∂úÎ†•ÏùÄ Îã®Ïùº ÌÜµÌï© Ïù¥ÎØ∏ÏßÄÏó¨Ïïº Ìï©ÎãàÎã§ (Í∑∏Î¶¨Îìú Í∏àÏßÄ)\n\n'
          finalPrompt += 'üö´ Ï†àÎåÄ Í∏àÏßÄÏÇ¨Ìï≠:\n'
          finalPrompt += '   ‚ùå ÎèôÏûë Î≥ÄÍ≤Ω ("Îì§Í≥†" ‚Üí "Ïì∞Í≥†", "holding" ‚Üí "wearing")\n'
          finalPrompt += '   ‚ùå Ïù∏Î¨º Ïàò Î≥ÄÍ≤Ω ("Ìïú Î™Ö" ‚Üí "Îëê Î™Ö")\n'
          finalPrompt += '   ‚ùå Î∞∞Í≤Ω ÎîîÏûêÏù∏ Î≥ÄÍ≤Ω (S1Í≥º Îã§Î•∏ Î∞∞Í≤Ω)\n'
          finalPrompt += '   ‚ùå ÏÉâÏÉÅ Î≥ÄÍ≤Ω (Îπ®Í∞ï ‚Üí ÌïòÏñë, ÌååÎûë ‚Üí Ï¥àÎ°ù)\n'
          finalPrompt += '   ‚ùå Ïä§ÌÜ†Î¶¨ Ïû¨Ìï¥ÏÑù\n\n'
          finalPrompt += 'REFERENCE = VISUAL ONLY. TEXT = COMPOSITION (NEVER CHANGE!)\n\n'
          finalPrompt += '---\n\n'
        } else if (refMode === 'balanced') {
          // Í∑†Ìòï Î™®Îìú: Ï†ÅÎãπÌïú ÏßÄÏãú
          finalPrompt += '‚öñÔ∏è BALANCED MODE: Reference Image + Text Description\n\n'
          finalPrompt += gridLabelInfo + '\n\n'
          finalPrompt += 'üí° ÏßÄÏπ®: Í∞Å ÎùºÎ≤®Ïùò Ï£ºÏöî ÏãúÍ∞ÅÏ†Å ÏöîÏÜå(ÏÉâÏÉÅ, Ïä§ÌÉÄÏùº, Íµ¨Ï°∞)Î•º Ïú†ÏßÄÌïòÎ©¥ÏÑú ÌÖçÏä§Ìä∏ ÏÑ§Î™ÖÏùò ÎîîÌÖåÏùºÏùÑ Î∞òÏòÅÌïòÏÑ∏Ïöî.\n'
          finalPrompt += '‚ö†Ô∏è Ï§ëÏöî: ÌÖçÏä§Ìä∏ ÌîÑÎ°¨ÌîÑÌä∏Ïùò Ïù∏Î¨º Ïàò, Í∏∞Î≥∏ Íµ¨ÏÑ±ÏùÄ Ïú†ÏßÄÌïòÏÑ∏Ïöî.\n\n'
          finalPrompt += '---\n\n'
        } else if (refMode === 'creative') {
          // Ï∞ΩÏùòÏÑ± Î™®Îìú: Í∞ÑÎã®Ìïú Ï∞∏Í≥†Îßå
          finalPrompt += 'üé® CREATIVE MODE: Reference for Inspiration\n\n'
          finalPrompt += gridLabelInfo + '\n\n'
          finalPrompt += 'üí° Ï∞∏Í≥†: ÏúÑ Ïù¥ÎØ∏ÏßÄÎäî Ïä§ÌÉÄÏùºÍ≥º Î∂ÑÏúÑÍ∏∞ Ï∞∏Í≥†Ïö©ÏûÖÎãàÎã§. ÌÖçÏä§Ìä∏ ÏÑ§Î™ÖÏùÑ Í∏∞Î∞òÏúºÎ°ú Ï∞ΩÏùòÏ†ÅÏúºÎ°ú ÏÉùÏÑ±ÌïòÏÑ∏Ïöî.\n\n'
          finalPrompt += '---\n\n'
        }
      }
      
      // Add text prompt
      if (inputPrompt.trim()) {
        finalPrompt += inputPrompt
      } else if (data.mode === 'describe') {
        finalPrompt += 'Describe this image in detail and create a prompt for generating a similar image.'
      } else if (data.mode === 'analyze') {
        finalPrompt += 'Analyze this image comprehensively (composition, style, lighting, colors, subjects, mood) and create a detailed prompt for AI image generation.'
      } else if (referenceImageDataUrl) {
        // If only image is provided without text, use a default instruction
        finalPrompt += 'Describe this image and create an effective prompt for AI image generation.'
      }
      
      if (finalPrompt.trim()) {
        contentParts.push({
          text: finalPrompt.trim()
        })
      }
      
      // Validate we have content to send
      if (contentParts.length === 0) {
        throw new Error('ÌîÑÎ°¨ÌîÑÌä∏ ÎòêÎäî Ïù¥ÎØ∏ÏßÄÎ•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.')
      }

      // Call LLM API based on provider
      let outputPrompt = ''
      
      if (provider === 'gemini') {
        // üîµ Gemini API Ìò∏Ï∂ú
        const url = `https://generativelanguage.googleapis.com/v1beta/models/${data.model}:generateContent?key=${apiKey}`
        
        const abortController = new AbortController()
        const timeoutId = setTimeout(() => abortController.abort(), 60000)
        
        try {
          const response = await fetch(url, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
            },
            body: JSON.stringify({
              contents: [{
                parts: contentParts
              }],
              systemInstruction: {
                parts: [{
                  text: systemInstruction
                }]
              },
              generationConfig: {
                temperature: 0.7,
                maxOutputTokens: 4096,
              }
            }),
            signal: abortController.signal
          })
          
          clearTimeout(timeoutId)

          if (!response.ok) {
            const errorData = await response.json().catch(() => ({}))
            throw new Error(errorData.error?.message || `API Error: ${response.status}`)
          }

          const result = await response.json()
          outputPrompt = result.candidates?.[0]?.content?.parts?.[0]?.text || ''
        } catch (fetchError: any) {
          clearTimeout(timeoutId)
          if (fetchError.name === 'AbortError') {
            throw new Error('LLM ÏÉùÏÑ± ÏãúÍ∞ÑÏù¥ Ï¥àÍ≥ºÎêòÏóàÏäµÎãàÎã§ (60Ï¥à).')
          }
          throw fetchError
        }
      } else if (provider === 'openai') {
        // üü¢ OpenAI API Ìò∏Ï∂ú
        const url = 'https://api.openai.com/v1/chat/completions'
        
        // OpenAI Î©îÏãúÏßÄ ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò
        const messages: any[] = [
          { role: 'system', content: systemInstruction }
        ]
        
        // User Î©îÏãúÏßÄ Íµ¨ÏÑ± (ÌÖçÏä§Ìä∏ + Ïù¥ÎØ∏ÏßÄ)
        const userContent: any[] = []
        
        // ÌÖçÏä§Ìä∏ Ï∂îÍ∞Ä
        if (finalPrompt.trim()) {
          userContent.push({ type: 'text', text: finalPrompt.trim() })
        }
        
        // Ïù¥ÎØ∏ÏßÄ Ï∂îÍ∞Ä (GPT-4o, GPT-4o-miniÎäî Vision ÏßÄÏõê)
        if (referenceImageDataUrl && actualImageDataUrl.startsWith('data:image')) {
          userContent.push({
            type: 'image_url',
            image_url: { url: actualImageDataUrl }
          })
        }
        
        if (userContent.length > 0) {
          messages.push({ role: 'user', content: userContent })
        }
        
        const abortController = new AbortController()
        const timeoutId = setTimeout(() => abortController.abort(), 60000)
        
        try {
          const response = await fetch(url, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'Authorization': `Bearer ${apiKey}`
            },
            body: JSON.stringify({
              model: data.model,
              messages: messages,
              temperature: 0.7,
              max_tokens: 4096,
            }),
            signal: abortController.signal
          })
          
          clearTimeout(timeoutId)

          if (!response.ok) {
            const errorData = await response.json().catch(() => ({}))
            throw new Error(errorData.error?.message || `API Error: ${response.status}`)
          }

          const result = await response.json()
          outputPrompt = result.choices?.[0]?.message?.content || ''
        } catch (fetchError: any) {
          clearTimeout(timeoutId)
          if (fetchError.name === 'AbortError') {
            throw new Error('LLM ÏÉùÏÑ± ÏãúÍ∞ÑÏù¥ Ï¥àÍ≥ºÎêòÏóàÏäµÎãàÎã§ (60Ï¥à).')
          }
          throw fetchError
        }
      }

      if (!outputPrompt) {
        throw new Error('LLMÏù¥ ÏùëÎãµÏùÑ ÏÉùÏÑ±ÌïòÏßÄ Î™ªÌñàÏäµÎãàÎã§.')
      }

      console.log(`‚úÖ LLM ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ± ÏôÑÎ£å (${provider}):`, outputPrompt.length, 'Ïûê')

      updateNode((prev) => ({
        ...prev,
        status: 'completed',
        outputPrompt: outputPrompt.trim(),
        error: undefined,
      }))
    } catch (error: any) {
      console.error('‚ùå LLM ÏÉùÏÑ± Ïã§Ìå®:', error)
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: formatErrorMessage(error),
      }))
    }
  },
  cancelNodeExecution: (id) => {
    console.log('üõë Cancelling node execution:', id)
    const { abortControllers } = get()
    const controller = abortControllers.get(id)
    if (controller) {
      try {
        controller.abort()
        abortControllers.delete(id)
        set({ abortControllers: new Map(abortControllers) })
        
        console.log('‚úÖ Abort controller cancelled successfully')
        
        // Update node status to idle immediately
        set({
          nodes: get().nodes.map((node) =>
            node.id === id
              ? {
                  ...node,
                  data: {
                    ...node.data,
                    status: 'idle',
                    error: 'ÏûëÏóÖÏù¥ Ï∑®ÏÜåÎêòÏóàÏäµÎãàÎã§.',
                    progress: 0,
                  },
                }
              : node,
          ),
        })
      } catch (error) {
        console.error('‚ùå Error cancelling node:', error)
        // Still update status even if abort fails
        set({
          nodes: get().nodes.map((node) =>
            node.id === id
              ? {
                  ...node,
                  data: {
                    ...node.data,
                    status: 'idle',
                    error: 'ÏûëÏóÖ Ï∑®ÏÜå Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.',
                    progress: 0,
                  },
                }
              : node,
          ),
        })
      }
    } else {
      console.warn('‚ö†Ô∏è No abort controller found for node:', id)
    }
  },
    }),
    {
      name: 'nano-banana-workflow-v3',
      storage: createThrottledStorage(), // ‚ö° Throttled storage
      partialize: (state) => {
        // üî• Ï†ÄÏû• Ï†Ñ ÏûêÎèô Ïö©Îüâ Í¥ÄÎ¶¨
        const storageInfo = getStorageInfo()
        console.log(`üíæ Persist: ${storageInfo.usedMB} MB / ${storageInfo.limitMB} MB (${storageInfo.percentage.toFixed(1)}%)`)
        
        // 90% Ïù¥ÏÉÅÏù¥Î©¥ Í∏¥Í∏â Ï†ïÎ¶¨
        const shouldCleanup = storageInfo.percentage > 90
        const nodesToSave = shouldCleanup 
          ? prepareForStorage(state.nodes, true) // Í∏¥Í∏â Ï†ïÎ¶¨
          : prepareForStorage(state.nodes, false) // ÏùºÎ∞ò Ï†ïÎ¶¨
        
        if (shouldCleanup) {
          console.warn('‚ö†Ô∏è localStorage 90% Ï¥àÍ≥º! Í∏¥Í∏â Ï†ïÎ¶¨ Ïã§Ìñâ')
        }
        
        return {
          nodes: sanitizeNodesForStorage(nodesToSave),
          edges: sanitizeEdgesForStorage(state.edges),
          apiKey: state.apiKey,
          klingApiKey: state.klingApiKey,
          openaiApiKey: state.openaiApiKey,  // OpenAI API Key Ï†ÄÏû•
        }
      },
      onRehydrateStorage: () => {
        console.log('üîÑ Zustand persist: Î≥µÏõê ÏãúÏûë...')
        return (state) => {
          if (state) {
            // API ÌÇ§Í∞Ä Ï†ÄÏû•ÎêòÏñ¥ ÏûàÏßÄ ÏïäÏúºÎ©¥ .envÏóêÏÑú ÏûêÎèô Î°úÎìú
            if (!state.apiKey) {
              state.apiKey = import.meta.env.VITE_GEMINI_API_KEY || ''
              if (state.apiKey) {
                console.log('üîë Gemini API ÌÇ§ ÏûêÎèô Î°úÎìúÎê® (.env)')
              }
            }
            if (!state.klingApiKey) {
              state.klingApiKey = import.meta.env.VITE_KLING_API_KEY || ''
              if (state.klingApiKey) {
                console.log('üîë Kling API ÌÇ§ ÏûêÎèô Î°úÎìúÎê® (.env)')
              }
            }
            if (!state.openaiApiKey) {
              state.openaiApiKey = import.meta.env.VITE_OPENAI_API_KEY || ''
              if (state.openaiApiKey) {
                console.log('üîë OpenAI API ÌÇ§ ÏûêÎèô Î°úÎìúÎê® (.env)')
              }
            }
            
            console.log('‚úÖ Zustand persist: ÏÉÅÌÉú Î≥µÏõêÎê®', {
              nodeCount: state.nodes?.length ?? 0,
              edgeCount: state.edges?.length ?? 0,
              hasApiKey: !!state.apiKey,
              hasKlingApiKey: !!state.klingApiKey,
              hasOpenaiApiKey: !!state.openaiApiKey,
            })
            try {
              state.edges = normalizeEdges(state.edges, state.nodes)
              console.log('‚úÖ Edges Ï†ïÍ∑úÌôî ÏôÑÎ£å')
            } catch (error) {
              console.error('‚ùå Error normalizing edges on rehydrate:', error)
              // Reset to safe state if normalization fails
              state.edges = []
            }
          } else {
            console.log('‚ÑπÔ∏è Zustand persist: Î≥µÏõêÌï† ÏÉÅÌÉú ÏóÜÏùå (ÏÉà ÏãúÏûë)')
          }
        }
      },
    }
  )
)

export const createWorkflowNode = (type: NodeType, position: { x: number; y: number }): WorkflowNode => ({
  id: `${type}-${crypto.randomUUID?.() ?? Date.now()}`,
  type,
  position,
  data: createNodeData(type),
})
