import { create } from 'zustand'
import { persist } from 'zustand/middleware'
import {
  addEdge,
  applyEdgeChanges,
  applyNodeChanges,
  type Connection,
  type EdgeChange,
  type NodeChange,
} from 'reactflow'
import { GeminiAPIClient, MockGeminiAPI } from '../services/geminiAPI'
import { KlingAPIClient, MockKlingAPI } from '../services/klingAPI'
import { SoraAPIClient, MockSoraAPI } from '../services/soraAPI'
import { retryWithBackoff } from '../utils/retry'
import { getStorageInfo, prepareForStorage, getStorageWarning } from '../utils/storage'
import { createBackup } from '../utils/backup'
import { saveImage, getImage } from '../utils/indexedDB'
import type {
  GeminiVideoNodeData,
  GridNodeData,
  ImageImportNodeData,
  KlingVideoNodeData,
  MotionPromptNodeData,
  NanoImageNodeData,
  NodeData,
  NodeType,
  SoraVideoNodeData,
  TextPromptNodeData,
  WorkflowEdge,
  WorkflowNode,
} from '../types/nodes'
import { createNodeData } from '../types/nodes'

type HistoryState = {
  nodes: WorkflowNode[]
  edges: WorkflowEdge[]
}

type FlowState = {
  nodes: WorkflowNode[]
  edges: WorkflowEdge[]
  selectedNodeId: string | null
  apiKey: string
  klingApiKey: string
  openaiApiKey: string  // OpenAI API Key
  abortControllers: Map<string, AbortController>
  history: HistoryState[]
  historyIndex: number
  imageModal: { isOpen: boolean; imageUrl: string | null }
  setSelectedNodeId: (id: string | null) => void
  setApiKey: (key: string) => void
  setKlingApiKey: (key: string) => void
  setOpenaiApiKey: (key: string) => void  // OpenAI API Key Setter
  openImageModal: (imageUrl: string) => void
  closeImageModal: () => void
  onNodesChange: (changes: NodeChange[]) => void
  onEdgesChange: (changes: EdgeChange[]) => void
  onConnect: (connection: Connection) => void
  addNode: (node: WorkflowNode) => void
  updateNodeData: (id: string, data: NodeData) => void
  saveWorkflow: () => boolean
  loadWorkflow: () => boolean
  importWorkflow: (nodes: WorkflowNode[], edges: WorkflowEdge[]) => boolean
  exportWorkflow: () => string
  undo: () => void
  redo: () => void
  runWorkflow: () => Promise<void>
  runGeminiNode: (id: string) => Promise<void>
  runNanoImageNode: (id: string) => Promise<void>
  runKlingNode: (id: string) => Promise<void>
  runSoraNode: (id: string) => Promise<void>
  runLLMPromptNode: (id: string) => Promise<void>
  runCellRegeneratorNode: (id: string) => Promise<void>
  cancelNodeExecution: (id: string) => void
}

const getEdgeClass = (edge: WorkflowEdge, nodes: WorkflowNode[]) => {
  const sourceNode = nodes.find((node) => node.id === edge.source)
  switch (sourceNode?.type) {
    case 'textPrompt':
      return 'edge-text-prompt'
    case 'motionPrompt':
      return 'edge-motion-prompt'
    case 'imageImport':
      return 'edge-image-import'
    case 'nanoImage':
      return 'edge-nano-image'
    case 'geminiVideo':
      return 'edge-gemini-video'
    case 'klingVideo':
      return 'edge-kling-video'
    case 'soraVideo':
      return 'edge-sora-video'
    case 'gridNode':
      return 'edge-text-prompt' // Use violet color
    case 'cellRegenerator':
      return 'edge-motion-prompt' // Use purple color
    case 'gridComposer':
      return 'edge-kling-video' // Use emerald color
    case 'llmPrompt':
      return 'edge-motion-prompt' // Use pink color
    default:
      return 'edge-default'
  }
}

const normalizeEdges = (edges: WorkflowEdge[], nodes: WorkflowNode[]) => {
  const nodeIds = new Set(nodes.map(n => n.id))
  
  // Filter out edges that reference deleted nodes
  return edges
    .filter(edge => nodeIds.has(edge.source) && nodeIds.has(edge.target))
    .map((edge) => ({
      ...edge,
      type: 'bezier',
      className: getEdgeClass(edge, nodes),
    }))
}

const sanitizeEdgesForStorage = (edges: WorkflowEdge[]) =>
  edges.map(({ source, target, sourceHandle, targetHandle, id }) => ({
    id,
    source,
    target,
    sourceHandle,
    targetHandle,
  }))

const isConnectionAllowed = (sourceType: NodeType, targetType: NodeType) => {
  if (sourceType === 'imageImport' && targetType === 'motionPrompt') return true
  if (sourceType === 'imageImport' && targetType === 'nanoImage') return true
  if (sourceType === 'nanoImage' && targetType === 'nanoImage') return true
  if (sourceType === 'textPrompt' && targetType === 'imageImport') return true
  if (sourceType === 'textPrompt' && targetType === 'nanoImage') return true
  if (sourceType === 'textPrompt' && targetType === 'motionPrompt') return true
  if (sourceType === 'motionPrompt' && targetType === 'nanoImage') return true
  if (sourceType === 'motionPrompt' && targetType === 'geminiVideo') return true
  if (sourceType === 'nanoImage' && targetType === 'geminiVideo') return true
  if (sourceType === 'imageImport' && targetType === 'geminiVideo') return true
  // Kling connections
  if (sourceType === 'motionPrompt' && targetType === 'klingVideo') return true
  if (sourceType === 'textPrompt' && targetType === 'klingVideo') return true
  if (sourceType === 'nanoImage' && targetType === 'klingVideo') return true
  if (sourceType === 'imageImport' && targetType === 'klingVideo') return true
  // Grid Node connections
  if (sourceType === 'textPrompt' && targetType === 'gridNode') return true
  if (sourceType === 'motionPrompt' && targetType === 'gridNode') return true
  if (sourceType === 'gridNode' && targetType === 'nanoImage') return true
  // Cell Regenerator connections
  if (sourceType === 'gridNode' && targetType === 'cellRegenerator') return true
  if (sourceType === 'nanoImage' && targetType === 'cellRegenerator') return true
  if (sourceType === 'imageImport' && targetType === 'cellRegenerator') return true
  if (sourceType === 'cellRegenerator' && targetType === 'nanoImage') return true
  if (sourceType === 'cellRegenerator' && targetType === 'imageImport') return true
  // Grid Composer connections
  if (sourceType === 'gridNode' && targetType === 'gridComposer') return true
  if (sourceType === 'nanoImage' && targetType === 'gridComposer') return true
  if (sourceType === 'imageImport' && targetType === 'gridComposer') return true
  if (sourceType === 'cellRegenerator' && targetType === 'gridComposer') return true
  if (sourceType === 'gridComposer' && targetType === 'nanoImage') return true
  if (sourceType === 'gridComposer' && targetType === 'imageImport') return true
  if (sourceType === 'gridComposer' && targetType === 'geminiVideo') return true
  if (sourceType === 'gridComposer' && targetType === 'klingVideo') return true
  // Sora Video connections
  if (sourceType === 'motionPrompt' && targetType === 'soraVideo') return true
  if (sourceType === 'textPrompt' && targetType === 'soraVideo') return true
  if (sourceType === 'nanoImage' && targetType === 'soraVideo') return true
  if (sourceType === 'imageImport' && targetType === 'soraVideo') return true
  if (sourceType === 'gridComposer' && targetType === 'soraVideo') return true
  if (sourceType === 'llmPrompt' && targetType === 'soraVideo') return true
  // LLM Prompt connections
  if (sourceType === 'textPrompt' && targetType === 'llmPrompt') return true
  if (sourceType === 'motionPrompt' && targetType === 'llmPrompt') return true
  if (sourceType === 'imageImport' && targetType === 'llmPrompt') return true
  if (sourceType === 'nanoImage' && targetType === 'llmPrompt') return true
  if (sourceType === 'gridComposer' && targetType === 'llmPrompt') return true
  if (sourceType === 'llmPrompt' && targetType === 'textPrompt') return true
  if (sourceType === 'llmPrompt' && targetType === 'nanoImage') return true
  if (sourceType === 'llmPrompt' && targetType === 'motionPrompt') return true
  if (sourceType === 'llmPrompt' && targetType === 'gridNode') return true
  if (sourceType === 'llmPrompt' && targetType === 'geminiVideo') return true
  if (sourceType === 'llmPrompt' && targetType === 'klingVideo') return true
  return false
}

const getExecutionOrder = (nodes: WorkflowNode[], edges: WorkflowEdge[]) => {
  const incoming = new Map<string, number>()
  const outgoing = new Map<string, string[]>()

  nodes.forEach((node) => {
    incoming.set(node.id, 0)
    outgoing.set(node.id, [])
  })

  edges.forEach((edge) => {
    incoming.set(edge.target, (incoming.get(edge.target) ?? 0) + 1)
    outgoing.get(edge.source)?.push(edge.target)
  })

  const queue = nodes.filter((node) => (incoming.get(node.id) ?? 0) === 0)
  const order: string[] = []

  while (queue.length) {
    const node = queue.shift()
    if (!node) break
    order.push(node.id)
    const neighbors = outgoing.get(node.id) ?? []
    neighbors.forEach((neighbor) => {
      incoming.set(neighbor, (incoming.get(neighbor) ?? 1) - 1)
      if ((incoming.get(neighbor) ?? 0) === 0) {
        const neighborNode = nodes.find((candidate) => candidate.id === neighbor)
        if (neighborNode) queue.push(neighborNode)
      }
    })
  }

  return order
}

const getIncomingNodes = (
  nodeId: string,
  edges: WorkflowEdge[],
  nodes: WorkflowNode[],
) =>
  edges
    .filter((edge) => edge.target === nodeId)
    .map((edge) => nodes.find((node) => node.id === edge.source))
    .filter(Boolean) as WorkflowNode[]

const getIncomingTextPrompt = (
  nodeId: string,
  edges: WorkflowEdge[],
  nodes: WorkflowNode[],
) => {
  const promptEdge = edges.find((edge) => edge.target === nodeId)
  if (!promptEdge) return undefined
  const promptNode = nodes.find((node) => node.id === promptEdge.source)
  if (!promptNode || promptNode.type !== 'textPrompt') return undefined
  return (promptNode.data as TextPromptNodeData).prompt
}

const STORAGE_KEY = 'nano-banana-workflow-v3'

// Helper function to make error messages more user-friendly
const formatErrorMessage = (error: unknown): string => {
  if (!(error instanceof Error)) return 'ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'
  
  const message = error.message.toLowerCase()
  
  // API quota exceeded
  if (message.includes('quota') && message.includes('exceeded')) {
    return 'API í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.'
  }
  
  // Other common errors
  if (message.includes('network') || message.includes('fetch')) {
    return 'ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.'
  }
  
  if (message.includes('api key') || message.includes('unauthorized')) {
    return 'API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.'
  }
  
  // Return original message if no match
  return error.message
}

const sanitizeNodesForStorage = (nodes: WorkflowNode[], forExport = false): WorkflowNode[] =>
  nodes.map((node) => {
    const data = { ...(node.data as Record<string, unknown>) }
    
    // ì´ì œ localStorageì—ë„ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•©ë‹ˆë‹¤
    // base64 DataURLì€ ìœ ì§€ (ìƒˆë¡œê³ ì¹¨ í›„ì—ë„ ë³µì›ë¨)
    // ë‹¨, blob URLì€ ì œê±° (í˜ì´ì§€ ì¬ë¡œë“œ ì‹œ ë¬´íš¨í™”ë¨)
    
    // Always remove blob URLs (they don't survive page reload)
    if (typeof data.imageUrl === 'string' && data.imageUrl.startsWith('blob:')) {
      delete data.imageUrl
      delete data.width
      delete data.height
    }
    if (typeof data.outputImageUrl === 'string' && data.outputImageUrl.startsWith('blob:')) {
      delete data.outputImageUrl
    }
    if (typeof data.inputImageUrl === 'string' && data.inputImageUrl.startsWith('blob:')) {
      delete data.inputImageUrl
    }
    if (typeof data.composedImageUrl === 'string' && data.composedImageUrl.startsWith('blob:')) {
      delete data.composedImageUrl
    }
    
    // regeneratedImages ê°ì²´ ë‚´ë¶€ì˜ blob URLë„ ì œê±°
    if (data.regeneratedImages && typeof data.regeneratedImages === 'object') {
      const cleanedImages: Record<string, string> = {}
      for (const [key, value] of Object.entries(data.regeneratedImages)) {
        if (typeof value === 'string' && !value.startsWith('blob:')) {
          cleanedImages[key] = value
        }
      }
      data.regeneratedImages = cleanedImages
    }
    
    // inputImages ê°ì²´ ë‚´ë¶€ì˜ blob URLë„ ì œê±°
    if (data.inputImages && typeof data.inputImages === 'object') {
      const cleanedImages: Record<string, string> = {}
      for (const [key, value] of Object.entries(data.inputImages)) {
        if (typeof value === 'string' && !value.startsWith('blob:')) {
          cleanedImages[key] = value
        }
      }
      data.inputImages = cleanedImages
    }
    if (typeof data.outputVideoUrl === 'string' && data.outputVideoUrl.startsWith('blob:')) {
      delete data.outputVideoUrl
    }
    
    // Reset status for all generation nodes
    if (node.type === 'nanoImage') {
      data.status = 'idle'
      delete data.error
      delete data.lastExecutionTime
    }
    if (node.type === 'geminiVideo') {
      data.status = 'idle'
      data.progress = 0
      delete data.error
      delete data.lastExecutionTime
    }
    if (node.type === 'klingVideo') {
      data.status = 'idle'
      data.progress = 0
      delete data.error
      delete data.taskId
      delete data.lastExecutionTime
    }
    return { ...node, data: data as NodeData }
  })

const MAX_HISTORY_SIZE = 20

const saveToHistory = (get: () => FlowState, set: (state: Partial<FlowState>) => void) => {
  const { nodes, edges, history, historyIndex } = get()
  
  // Remove any history after current index (when undoing then making new changes)
  const newHistory = history.slice(0, historyIndex + 1)
  
  // Add current state
  newHistory.push({ nodes: JSON.parse(JSON.stringify(nodes)), edges: JSON.parse(JSON.stringify(edges)) })
  
  // Keep only last MAX_HISTORY_SIZE states
  if (newHistory.length > MAX_HISTORY_SIZE) {
    newHistory.shift()
  }
  
  set({ 
    history: newHistory, 
    historyIndex: newHistory.length - 1 
  })
}

// âš¡ Throttled localStorage for better performance
const createThrottledStorage = () => {
  let saveTimeout: NodeJS.Timeout | null = null
  const SAVE_DELAY = 1000 // 1ì´ˆ throttle

  return {
    getItem: (name: string) => {
      const value = localStorage.getItem(name)
      return value ? JSON.parse(value) : null
    },
    setItem: (name: string, value: any) => {
      // Throttle: 1ì´ˆ ë™ì•ˆ ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œë˜ë©´ ë§ˆì§€ë§‰ ê²ƒë§Œ ì €ì¥
      if (saveTimeout) {
        clearTimeout(saveTimeout)
      }
      
      saveTimeout = setTimeout(() => {
        try {
          const serialized = JSON.stringify(value)
          localStorage.setItem(name, serialized)
          console.log('ğŸ’¾ Throttled save completed')
        } catch (error) {
          console.error('âŒ Save failed:', error)
        }
      }, SAVE_DELAY)
    },
    removeItem: (name: string) => {
      localStorage.removeItem(name)
    },
  }
}

export const useFlowStore = create<FlowState>()(
  persist(
    (set, get) => ({
      nodes: [],
      edges: [],
      selectedNodeId: null,
      // .env íŒŒì¼ì—ì„œ API í‚¤ ìë™ ë¡œë“œ
      apiKey: import.meta.env.VITE_GEMINI_API_KEY || '',
      klingApiKey: import.meta.env.VITE_KLING_API_KEY || '',
      openaiApiKey: import.meta.env.VITE_OPENAI_API_KEY || '',  // OpenAI API Key
      abortControllers: new Map(),
      history: [],
      historyIndex: -1,
      imageModal: { isOpen: false, imageUrl: null },
      setSelectedNodeId: (id) => set({ selectedNodeId: id }),
      setApiKey: (key) => set({ apiKey: key }),
      setKlingApiKey: (key) => set({ klingApiKey: key }),
      setOpenaiApiKey: (key) => set({ openaiApiKey: key }),  // OpenAI API Key Setter
      openImageModal: (imageUrl) => set({ imageModal: { isOpen: true, imageUrl } }),
      closeImageModal: () => set({ imageModal: { isOpen: false, imageUrl: null } }),
  onNodesChange: (changes) => {
    try {
      // ğŸ¯ ì„±ëŠ¥ ìµœì í™”: position ë³€ê²½ë§Œ ìˆìœ¼ë©´ ë¡œê·¸ ìƒëµ
      const hasNonPositionChange = changes.some(
        change => change.type !== 'position' && change.type !== 'dimensions'
      )
      
      if (hasNonPositionChange) {
        console.log('ğŸ”„ onNodesChange:', changes)
      }
      
      // Clean up abort controllers for removed nodes
      const removedNodeIds = changes
        .filter(change => change.type === 'remove')
        .map(change => (change as any).id)
      
      if (removedNodeIds.length > 0) {
        console.log('ğŸ—‘ï¸ ë…¸ë“œ ì‚­ì œ ì‹œë„:', removedNodeIds)
        const { abortControllers } = get()
        removedNodeIds.forEach(id => {
          const controller = abortControllers.get(id)
          if (controller) {
            console.log('ğŸ§¹ Cleaning up abort controller for deleted node:', id)
            controller.abort()
            abortControllers.delete(id)
          }
        })
        if (removedNodeIds.length > 0) {
          set({ abortControllers: new Map(abortControllers) })
        }
      }
      
      const currentNodes = get().nodes
      const currentEdges = get().edges
      
      const newNodes = applyNodeChanges(changes, currentNodes) as WorkflowNode[]
      const newEdges = normalizeEdges(currentEdges, newNodes)
      
      set({ 
        nodes: newNodes,
        edges: newEdges
      })
      
      // âš¡ ì„±ëŠ¥ ìµœì í™”: add/removeë§Œ history ì €ì¥ (position/selectëŠ” ì œì™¸)
      const shouldSaveHistory = changes.some(change => 
        change.type === 'add' || change.type === 'remove'
      )
      if (shouldSaveHistory) {
        saveToHistory(get, set)
      }
    } catch (error) {
      console.error('âŒ Error in onNodesChange:', error)
      // ì—ëŸ¬ ë°œìƒí•´ë„ ì•±ì´ ë©ˆì¶”ì§€ ì•Šë„ë¡
    }
  },
  onEdgesChange: (changes) => {
    try {
      const currentEdges = get().edges
      const currentNodes = get().nodes
      const newEdges = applyEdgeChanges(changes, currentEdges)
      
      set({
        edges: normalizeEdges(newEdges, currentNodes),
      })
      
      // Save to history for add/remove changes
      const shouldSaveHistory = changes.some(change => 
        change.type === 'add' || change.type === 'remove'
      )
      if (shouldSaveHistory) {
        saveToHistory(get, set)
      }
    } catch (error) {
      console.error('Error in onEdgesChange:', error)
      // Don't crash the app, just log the error
    }
  },
  onConnect: (connection) => {
    const { source, target } = connection
    if (!source || !target) return
    const sourceNode = get().nodes.find((node) => node.id === source)
    const targetNode = get().nodes.find((node) => node.id === target)
    if (!sourceNode || !targetNode || !sourceNode.type || !targetNode.type) return
    if (!isConnectionAllowed(sourceNode.type, targetNode.type)) return

    set({
      edges: normalizeEdges(
        addEdge(
          {
            ...connection,
            type: 'bezier',
          },
          get().edges,
        ),
        get().nodes,
      ),
    })
    saveToHistory(get, set)
  },
  addNode: (node) => {
    set({ nodes: [...get().nodes, node] })
    saveToHistory(get, set)
  },
  updateNodeData: (id, data) => {
    set({
      nodes: get().nodes.map((node) => {
        if (node.id === id) {
          // Support partial updates by spreading existing data
          const newData = typeof data === 'function' ? data(node.data) : data
          return { ...node, data: { ...node.data, ...newData } }
        }
        return node
      }),
    })
    // Note: persist middleware will automatically save to localStorage
  },
  saveWorkflow: () => {
    try {
      // ğŸ“Š persist ë¯¸ë“¤ì›¨ì–´ê°€ ìë™ìœ¼ë¡œ ì €ì¥í•˜ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” ë°±ì—…ë§Œ ìƒì„±
      console.log('ğŸ’¾ ë°±ì—… ìƒì„± ì¤‘...')
      
      // ì €ì¥ê³µê°„ ì²´í¬
      const storageInfo = getStorageInfo()
      console.log(`ğŸ“Š Storage: ${storageInfo.usedMB} MB / ${storageInfo.limitMB} MB (${storageInfo.percentage.toFixed(1)}%)`)
      
      const warning = getStorageWarning(storageInfo)
      if (warning) {
        console.warn(warning)
      }
      
      // persistê°€ ì €ì¥í•œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
      const persistedData = localStorage.getItem('nano-banana-workflow-v3')
      if (persistedData) {
        // ğŸ”’ ìë™ ë°±ì—… ìƒì„± (5ë¶„ë§ˆë‹¤ í•œ ë²ˆì”©ë§Œ)
        const lastBackupKey = 'last-backup-time'
        const lastBackup = parseInt(localStorage.getItem(lastBackupKey) || '0')
        const now = Date.now()
        const fiveMinutes = 5 * 60 * 1000
        
        if (now - lastBackup > fiveMinutes) {
          // persist í˜•ì‹ ê·¸ëŒ€ë¡œ ë°±ì—…
          createBackup(persistedData)
          localStorage.setItem(lastBackupKey, now.toString())
          console.log('âœ… ë°±ì—… ìƒì„± ì™„ë£Œ')
        } else {
          console.log('â­ï¸ ë°±ì—… ìƒì„± ê±´ë„ˆëœ€ (5ë¶„ ì´ë‚´)')
        }
      }
      
      // persistê°€ ìë™ìœ¼ë¡œ ì €ì¥í•˜ë¯€ë¡œ í•­ìƒ ì„±ê³µ ë°˜í™˜
      return true
    } catch (error) {
      console.error('âŒ ë°±ì—… ìƒì„± ì‹¤íŒ¨:', error)
      return false
    }
  },
  loadWorkflow: () => {
    try {
      console.log('ğŸ”„ loadWorkflow í˜¸ì¶œë¨')
      const raw = localStorage.getItem(STORAGE_KEY)
      if (!raw) {
        console.log('â„¹ï¸ localStorageì— ë°ì´í„° ì—†ìŒ')
        return false
      }
      
      const parsed = JSON.parse(raw)
      console.log('ğŸ“¦ localStorage ë°ì´í„° íŒŒì‹± ì„±ê³µ:', parsed)
      
      // persist í˜•ì‹ í™•ì¸: { state: {...}, version: 0 }
      let nodes: WorkflowNode[] = []
      let edges: WorkflowEdge[] = []
      
      if (parsed.state) {
        // persist ë¯¸ë“¤ì›¨ì–´ í˜•ì‹
        console.log('âœ… persist í˜•ì‹ ê°ì§€')
        nodes = Array.isArray(parsed.state.nodes) ? parsed.state.nodes : []
        edges = Array.isArray(parsed.state.edges) ? parsed.state.edges : []
      } else if (parsed.nodes) {
        // êµ¬ë²„ì „ ë˜ëŠ” export í˜•ì‹
        console.log('â„¹ï¸ êµ¬ë²„ì „ í˜•ì‹ ê°ì§€')
        nodes = Array.isArray(parsed.nodes) ? parsed.nodes : []
        edges = Array.isArray(parsed.edges) ? parsed.edges : []
      }
      
      console.log('ğŸ“Š ë¡œë“œëœ ë°ì´í„°:', { nodeCount: nodes.length, edgeCount: edges.length })
      
      if (nodes.length === 0) {
        console.log('âš ï¸ ë…¸ë“œê°€ ì—†ìŒ')
        return false
      }
      
      set({
        nodes,
        edges: normalizeEdges(edges, nodes),
        selectedNodeId: null,
      })
      
      console.log('âœ… ì›Œí¬í”Œë¡œìš° ë³µì› ì™„ë£Œ')
      return true
    } catch (error) {
      console.error('âŒ loadWorkflow ì‹¤íŒ¨:', error)
      return false
    }
  },
  importWorkflow: (nodes, edges) => {
    try {
      const currentState = get()
      const existingNodes = currentState.nodes
      const existingEdges = currentState.edges
      
      // Create ID mapping to avoid conflicts
      const idMap = new Map<string, string>()
      const existingIds = new Set(existingNodes.map(n => n.id))
      
      // Generate new IDs for imported nodes if they conflict
      const newNodes = nodes.map(node => {
        let newId = node.id
        
        // If ID already exists, generate a new one
        if (existingIds.has(newId)) {
          newId = `${node.type}-${crypto.randomUUID?.() ?? Date.now()}`
        }
        
        idMap.set(node.id, newId)
        
        // Calculate offset: place imported nodes to the right of existing nodes
        const maxX = existingNodes.length > 0 
          ? Math.max(...existingNodes.map(n => n.position.x + 250))
          : 0
        
        return {
          ...node,
          id: newId,
          position: {
            x: node.position.x + maxX + 50,
            y: node.position.y
          },
          selected: false,
        }
      })
      
      // Update edge IDs to match new node IDs
      const newEdges = edges.map(edge => {
        const newSource = idMap.get(edge.source) ?? edge.source
        const newTarget = idMap.get(edge.target) ?? edge.target
        
        return {
          ...edge,
          id: `${newSource}-${edge.sourceHandle ?? 'output'}-${newTarget}-${edge.targetHandle ?? 'input'}`,
          source: newSource,
          target: newTarget,
        }
      })
      
      // Merge with existing nodes and edges
      const mergedNodes = [...existingNodes, ...newNodes]
      const mergedEdges = normalizeEdges([...existingEdges, ...newEdges], mergedNodes)
      
      set({
        nodes: mergedNodes,
        edges: mergedEdges,
        selectedNodeId: null,
      })
      
      // Save to history
      saveToHistory(get, set)
      
      return true
    } catch (error) {
      console.error('Import workflow failed:', error)
      return false
    }
  },
  exportWorkflow: () => {
    const { nodes, edges } = get()
    return JSON.stringify({
      version: '1.0',
      timestamp: new Date().toISOString(),
      nodes: sanitizeNodesForStorage(nodes, true), // Keep base64 for export
      edges: sanitizeEdgesForStorage(edges),
    }, null, 2)
  },
  undo: () => {
    const { history, historyIndex } = get()
    if (historyIndex > 0) {
      const previousState = history[historyIndex - 1]
      set({
        nodes: JSON.parse(JSON.stringify(previousState.nodes)),
        edges: normalizeEdges(JSON.parse(JSON.stringify(previousState.edges)), previousState.nodes),
        historyIndex: historyIndex - 1,
        selectedNodeId: null,
      })
    }
  },
  redo: () => {
    const { history, historyIndex } = get()
    if (historyIndex < history.length - 1) {
      const nextState = history[historyIndex + 1]
      set({
        nodes: JSON.parse(JSON.stringify(nextState.nodes)),
        edges: normalizeEdges(JSON.parse(JSON.stringify(nextState.edges)), nextState.nodes),
        historyIndex: historyIndex + 1,
        selectedNodeId: null,
      })
    }
  },
  runNanoImageNode: async (id) => {
    const { nodes, edges, abortControllers } = get()
    const current = nodes.find((node) => node.id === id)
    if (!current || current.type !== 'nanoImage') return

    // âœ… Prevent duplicate execution
    const currentData = current.data as NanoImageNodeData
    if (currentData.status === 'processing') {
      console.warn('âš ï¸ Node is already processing, skipping duplicate execution')
      return
    }

    // âœ… Rate limiting: Check last execution time
    const now = Date.now()
    const lastExecution = (currentData as any).lastExecutionTime || 0
    const minInterval = 3000 // 3 seconds minimum between executions
    
    if (now - lastExecution < minInterval) {
      const waitTime = Math.ceil((minInterval - (now - lastExecution)) / 1000)
      set({
        nodes: nodes.map((node) =>
          node.id === id
            ? {
                ...node,
                data: {
                  ...node.data,
                  status: 'error',
                  error: `ë„ˆë¬´ ë¹ ë¥´ê²Œ ì‹¤í–‰í–ˆìŠµë‹ˆë‹¤. ${waitTime}ì´ˆ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.`,
                },
              }
            : node,
        ),
      })
      return
    }

    // Create abort controller for this execution
    const abortController = new AbortController()
    abortControllers.set(id, abortController)
    set({ abortControllers: new Map(abortControllers) })

    const updateNode = (updater: (data: NodeData) => NodeData) => {
      set({
        nodes: get().nodes.map((node) =>
          node.id === id ? { ...node, data: updater(node.data) } : node,
        ),
      })
    }

    const incoming = getIncomingNodes(id, edges, get().nodes)
    
    // Get prompt from various sources
    let prompt = ''
    
    // Check for gridNode
    const gridNodeEdge = edges.find((e) => e.target === id && 
      get().nodes.find(n => n.id === e.source)?.type === 'gridNode')
    
    if (gridNodeEdge) {
      const gridNode = get().nodes.find((n) => n.id === gridNodeEdge.source)
      if (gridNode?.type === 'gridNode') {
        const data = gridNode.data as GridNodeData
        const slotId = gridNodeEdge.sourceHandle
        prompt = slotId ? data.generatedPrompts?.[slotId] || '' : ''
      }
    } else {
      // Check for prompt handle connection
      const promptEdge = edges.find((e) => e.target === id && e.targetHandle === 'prompt')
      if (promptEdge) {
        const promptNode = get().nodes.find((n) => n.id === promptEdge.source)
        prompt =
          promptNode?.type === 'textPrompt'
            ? (promptNode.data as TextPromptNodeData).prompt
            : promptNode?.type === 'motionPrompt'
              ? (promptNode.data as MotionPromptNodeData).combinedPrompt
              : promptNode?.type === 'llmPrompt'
                ? (promptNode.data as any).outputPrompt || ''
                : ''
      } else {
        // Fallback: Original prompt logic (any connection)
        const promptNode = incoming.find(
          (node) => node.type === 'textPrompt' || node.type === 'motionPrompt' || node.type === 'llmPrompt',
        )
        prompt =
          promptNode?.type === 'textPrompt'
            ? (promptNode.data as TextPromptNodeData).prompt
            : promptNode?.type === 'motionPrompt'
              ? (promptNode.data as MotionPromptNodeData).combinedPrompt
              : promptNode?.type === 'llmPrompt'
                ? (promptNode.data as any).outputPrompt || ''
                : ''
      }
    }
    
    // Collect multiple reference images
    const referenceImages: string[] = []
    const referencePrompts: string[] = []
    let referenceSlotId: string | undefined
    let referenceSlotLabel: string | undefined
    let referenceCellRegeneratorId: string | undefined
    const data = current.data as NanoImageNodeData
    const maxRefs = data.maxReferences || 3

    // ğŸ” Debug: Log ALL edges for this node
    const allEdgesToThisNode = edges.filter((e) => e.target === id)
    console.log(`ğŸ” Nano Image [${id}]: ALL incoming edges:`, allEdgesToThisNode.map((e) => ({
      source: e.source,
      sourceHandle: e.sourceHandle,
      targetHandle: e.targetHandle,
      sourceType: get().nodes.find((n) => n.id === e.source)?.type,
    })))

    // Helper: resolve image from a reference node
    const resolveRefImage = async (refNode: WorkflowNode, refEdge: WorkflowEdge): Promise<{ imageDataUrl?: string; refPrompt?: string }> => {
      let imageDataUrl: string | undefined
      let refPrompt: string | undefined

      if (refNode.type === 'imageImport') {
        const imgData = refNode.data as ImageImportNodeData
        imageDataUrl = imgData.imageDataUrl
        refPrompt = getIncomingTextPrompt(refNode.id, edges, get().nodes) ?? imgData.referencePrompt
      } else if (refNode.type === 'nanoImage') {
        const imgData = refNode.data as NanoImageNodeData
        imageDataUrl = imgData.outputImageDataUrl
      } else if (refNode.type === 'gridComposer') {
        const imgData = refNode.data as any
        imageDataUrl = imgData.composedImageDataUrl || imgData.composedImageUrl
      } else if (refNode.type === 'cellRegenerator') {
        const imgData = refNode.data as any
        let cellId = refEdge.sourceHandle

        console.log(`ğŸ” Nano Image: Cell Regenerator connection found`, {
          sourceHandle: cellId,
          targetHandle: refEdge.targetHandle,
          availableCells: Object.keys(imgData.regeneratedImages || {}),
        })

        // Self-healing: If handle is "output" or missing, use first available cell
        if ((!cellId || cellId === 'output' || !imgData.regeneratedImages?.[cellId]) &&
            imgData.regeneratedImages && Object.keys(imgData.regeneratedImages).length > 0) {
          const firstCell = Object.keys(imgData.regeneratedImages)[0]
          console.warn(`âš ï¸ Nano Image: Invalid cellId "${cellId}", falling back to "${firstCell}"`)
          cellId = firstCell
        }

        if (cellId && imgData.regeneratedImages?.[cellId]) {
          imageDataUrl = imgData.regeneratedImages[cellId]
          referenceSlotId = referenceSlotId || cellId
          referenceSlotLabel = referenceSlotLabel ||
            imgData.slots?.find((slot: any) => slot.id === cellId)?.label
          referenceCellRegeneratorId = referenceCellRegeneratorId || refNode.id
          console.log(`âœ… Nano Image: Using cell image for ${cellId}`)
        } else {
          console.error(`âŒ Nano Image: No cell image found for "${cellId}"`)
        }
      }

      return { imageDataUrl, refPrompt }
    }

    // Helper: push resolved image to referenceImages
    const pushRefImage = async (imageDataUrl: string | undefined, refPrompt: string | undefined, label: string) => {
      if (!imageDataUrl) return
      if (imageDataUrl.startsWith('idb:') || imageDataUrl.startsWith('s3:')) {
        try {
          const actualDataUrl = await getImage(imageDataUrl)
          if (actualDataUrl) {
            referenceImages.push(actualDataUrl)
            console.log(`âœ… Nano Image: Loaded ${label} reference (${actualDataUrl.length} chars)`)
          } else {
            console.warn(`âš ï¸ Failed to load reference image: ${imageDataUrl}`)
          }
        } catch (error) {
          console.error(`âŒ Error loading reference image: ${imageDataUrl}`, error)
        }
      } else {
        referenceImages.push(imageDataUrl)
        console.log(`âœ… Nano Image: Direct ${label} reference (${imageDataUrl.length} chars)`)
      }
      if (refPrompt) {
        referencePrompts.push(refPrompt)
      }
    }

    // Strategy 1: Check ref-N handles (standard path)
    for (let i = 1; i <= maxRefs; i++) {
      const refEdge = edges.find((e) => e.target === id && e.targetHandle === `ref-${i}`)
      if (refEdge) {
        const refNode = get().nodes.find((n) => n.id === refEdge.source)
        if (refNode) {
          const { imageDataUrl, refPrompt } = await resolveRefImage(refNode, refEdge)
          await pushRefImage(imageDataUrl, refPrompt ? `Reference ${i}: ${refPrompt}` : undefined, `ref-${i}`)
        }
      }
    }

    // Strategy 2: If no ref-N matches found, check ALL edges for Cell Regenerator connections
    if (referenceImages.length === 0) {
      console.log(`âš ï¸ Nano Image: No ref-N handle matches. Checking ALL edges for Cell Regenerator...`)
      for (const edge of allEdgesToThisNode) {
        const sourceNode = get().nodes.find((n) => n.id === edge.source)
        if (sourceNode && ['imageImport', 'nanoImage', 'gridComposer', 'cellRegenerator'].includes(sourceNode.type!)) {
          // Skip prompt-handle connections
          if (edge.targetHandle === 'prompt') continue
          console.log(`ğŸ”„ Nano Image: Found image source via fallback: ${sourceNode.type} (sourceHandle=${edge.sourceHandle}, targetHandle=${edge.targetHandle})`)
          const { imageDataUrl, refPrompt } = await resolveRefImage(sourceNode, edge)
          await pushRefImage(imageDataUrl, refPrompt, `fallback-${sourceNode.type}`)
          if (referenceImages.length > 0) break // Use first valid reference
        }
      }
    }

    console.log(`ğŸ“Š Nano Image: Final reference count = ${referenceImages.length}`)

    // ğŸ§­ If reference comes from Cell Regenerator, COMPLETELY REPLACE the prompt
    // The Grid Node's prompt contains FULL GRID instructions (9 cells, 3x3 layout etc.)
    // which causes Gemini to generate a grid instead of a single image.
    const hasCellRegeneratorRef = !!referenceCellRegeneratorId && referenceImages.length > 0

    if (hasCellRegeneratorRef) {
      // Get ONLY the scene description from Grid Node's slot data (not the grid prompt)
      let sceneDescription = ''
      if (referenceCellRegeneratorId && referenceSlotId) {
        const gridEdge = edges.find(
          (e) => e.target === referenceCellRegeneratorId && e.targetHandle === 'grid-layout',
        )
        const gridNode =
          gridEdge && get().nodes.find((n) => n.id === gridEdge.source && n.type === 'gridNode')
        if (gridNode?.type === 'gridNode') {
          const gridData = gridNode.data as GridNodeData
          const slotData = gridData.slots?.find((s) => s.id === referenceSlotId)
          if (slotData) {
            sceneDescription = `Scene: ${slotData.label}. ${slotData.metadata || ''}`
          }
        }
      }

      const labelText = referenceSlotLabel ? ` â€” ${referenceSlotLabel}` : ''
      const slotText = referenceSlotId || 'cell'

      // COMPLETELY REPLACE the prompt â€” NO grid instructions
      prompt = `Generate exactly ONE standalone cinematic image. This is NOT a grid, NOT a collage, NOT multi-panel. Output a SINGLE scene only.

${sceneDescription ? `${sceneDescription}\n\n` : ''}Use the reference image as visual guide for this ONE scene (${slotText}${labelText}).
Recreate the scene with high quality, photorealistic rendering.
Remove any text labels, borders, or grid artifacts from the reference.`

      console.log('ğŸ¯ Nano Image v4: GRID PROMPT REPLACED with single-image prompt', {
        referenceSlotId,
        sceneDescription: sceneDescription.substring(0, 80),
        newPromptLength: prompt.length,
        oldPromptHadGrid: /grid|3x3|9 cells/i.test(prompt),
      })
    } else if (referenceSlotId) {
      // Legacy path for slot references without Cell Regenerator
      const labelText = referenceSlotLabel ? ` (${referenceSlotLabel})` : ''
      prompt = `Generate a SINGLE image. ${prompt}\nSubject: ${referenceSlotId}${labelText}.`
    }
    
    // Fallback: Check for old-style connection (no handle specified)
    if (referenceImages.length === 0) {
      const imageNode =
        incoming.find((node) => node.type === 'imageImport') ??
        incoming.find((node) => node.type === 'nanoImage') ??
        incoming.find((node) => node.type === 'gridComposer') ??
        incoming.find((node) => node.type === 'cellRegenerator')

      let inputImageDataUrl: string | undefined
      
      if (imageNode?.type === 'imageImport') {
        inputImageDataUrl = (imageNode.data as ImageImportNodeData).imageDataUrl
      } else if (imageNode?.type === 'nanoImage') {
        inputImageDataUrl = (imageNode.data as NanoImageNodeData).outputImageDataUrl
      } else if (imageNode?.type === 'gridComposer') {
        const imgData = imageNode.data as any
        inputImageDataUrl = imgData.composedImageDataUrl || imgData.composedImageUrl
      } else if (imageNode?.type === 'cellRegenerator') {
        // For cell regenerator without specific handle, we can't determine which cell to use
        // User should use specific cell output handles (S1, S2, etc.)
        inputImageDataUrl = undefined
      }
      
      const referencePrompt =
        imageNode?.type === 'imageImport'
          ? getIncomingTextPrompt(imageNode.id, edges, get().nodes) ??
            (imageNode.data as ImageImportNodeData).referencePrompt
          : undefined
      
      if (inputImageDataUrl) {
        // ğŸ”¥ Convert idb: or s3: reference to actual DataURL
        if (inputImageDataUrl.startsWith('idb:') || inputImageDataUrl.startsWith('s3:')) {
          try {
            const actualDataUrl = await getImage(inputImageDataUrl)
            if (actualDataUrl) {
              referenceImages.push(actualDataUrl)
            } else {
              console.warn(`âš ï¸ Failed to load fallback reference image: ${inputImageDataUrl}`)
            }
          } catch (error) {
            console.error(`âŒ Error loading fallback reference image: ${inputImageDataUrl}`, error)
          }
        } else {
          referenceImages.push(inputImageDataUrl)
        }
        
        if (referencePrompt) {
          referencePrompts.push(`Reference 1: ${referencePrompt}`)
        }
      }
    }

    if (!prompt.trim()) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ë…¸ë“œë¥¼ ì—°ê²°í•´ ì£¼ì„¸ìš”.',
      }))
      return
    }

    const apiKey = get().apiKey || import.meta.env.VITE_GEMINI_API_KEY || ''
    
    if (!apiKey) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'Gemini API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤. ìƒë‹¨ "API Key" ë²„íŠ¼ì„ ëˆŒëŸ¬ì„œ ì„¤ì •í•˜ì„¸ìš”.',
      }))
      return
    }

    // Debug: version tag to confirm HMR is working
    console.log('ğŸ”¥ğŸ”¥ğŸ”¥ Nano Image v3: Code is running! ğŸ”¥ğŸ”¥ğŸ”¥', {
      referenceCount: referenceImages.length,
      referenceSlotId,
      prompt: prompt.substring(0, 80),
      referenceSizes: referenceImages.map(r => r.length),
    })

    updateNode((prev) => ({
      ...prev,
      status: 'processing',
      error: undefined,
      lastExecutionTime: now,
    }))

    const client = new GeminiAPIClient(apiKey)

    try {
      // Check if aborted before starting
      if (abortController.signal.aborted) {
        throw new Error('ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.')
      }

      // Check if Grid Composer + LLM Prompt are connected
      const gridComposerEdge = edges.find(e => e.target === id && get().nodes.find(n => n.id === e.source)?.type === 'gridComposer')
      const gridComposerNode = gridComposerEdge ? get().nodes.find(n => n.id === gridComposerEdge.source) : null
      const hasGridComposerRef = referenceImages.length > 0 && !!gridComposerEdge
      
      const llmPromptEdge = edges.find(e => 
        e.target === id && 
        e.targetHandle === 'prompt' && 
        get().nodes.find(n => n.id === e.source)?.type === 'llmPrompt'
      )
      const llmPromptNode = llmPromptEdge ? get().nodes.find(n => n.id === llmPromptEdge.source) : null
      const hasLLMPrompt = !!llmPromptNode
      
      // Get reference mode from LLM Prompt Helper
      const referenceMode = (llmPromptNode?.data as any)?.referenceMode || 'exact'
      
      // Extract Grid Composer label info (for Nano Banana)
      let gridLabelInfoForNano = ''
      if (gridComposerNode && gridComposerNode.type === 'gridComposer') {
        const gridData = gridComposerNode.data as any
        if (gridData.inputImages && gridData.slots) {
          const layout = gridData.gridLayout || '1x3'
          const slots = gridData.slots as Array<{ id: string; label: string; metadata?: string }>
          
          const slotDescriptions = slots
            .filter(slot => gridData.inputImages[slot.id])
            .map((slot, index) => {
              const position = ['ì²« ë²ˆì§¸', 'ë‘ ë²ˆì§¸', 'ì„¸ ë²ˆì§¸', 'ë„¤ ë²ˆì§¸', 'ë‹¤ì„¯ ë²ˆì§¸', 'ì—¬ì„¯ ë²ˆì§¸'][index] || `${index + 1}ë²ˆì§¸`
              let description = `- ${position} ì°¸ê³  ìš”ì†Œ (${slot.id}): ${slot.label}`
              if (slot.metadata && slot.metadata.trim()) {
                description += ` - ${slot.metadata}`
              }
              return description
            })
            .join('\n')
          
          if (slotDescriptions) {
            gridLabelInfoForNano = `\n\nğŸ“‹ ì°¸ê³  ì´ë¯¸ì§€ êµ¬ì„± (${layout} ê·¸ë¦¬ë“œ):\n${slotDescriptions}\n\n`
          }
        }
      }
      
      // Check if Motion Prompt is connected (for camera transformation)
      const motionPromptEdge = edges.find(e => 
        e.target === id && 
        e.targetHandle === 'prompt' && 
        get().nodes.find(n => n.id === e.source)?.type === 'motionPrompt'
      )
      const motionPromptNode = motionPromptEdge ? get().nodes.find(n => n.id === motionPromptEdge.source) : null
      const hasMotionPrompt = !!motionPromptNode
      
      // Add reference prompts to main prompt if available
      let enhancedPrompt = referencePrompts.length > 0
        ? `${prompt}\n\n${referencePrompts.join('\n')}`
        : prompt
      
      // ğŸ¥ Motion Prompt: Apply camera transformation (with or without reference image)
      if (hasMotionPrompt) {
        const motionData = motionPromptNode?.data as MotionPromptNodeData
        const hasCameraMovement = 
          (motionData.cameraRotation && motionData.cameraRotation !== 0) ||
          (motionData.cameraTilt && motionData.cameraTilt !== 0) ||
          (motionData.cameraDistance && motionData.cameraDistance !== 1.0)
        
        if (hasCameraMovement) {
          // 360ë„ ì‹œìŠ¤í…œ: ê°ë„ë³„ íŠ¹ë³„ ì²˜ë¦¬
          const rotation = motionData.cameraRotation || 0
          const normalizedRotation = Math.round(((rotation % 360) + 360) % 360)
          
          let cameraDescription = ''
          let shotType = ''
          let lensType = ''
          let angleDetails = ''
          
          // ğŸ¬ Rotation í•´ì„ (Google Gemini Photography Terminology)
          if (normalizedRotation === 0 || normalizedRotation === 360) {
            shotType = 'straight-on frontal shot'
            lensType = '50mm standard lens'
            angleDetails = 'Camera positioned directly in front of subject at eye level, neutral perspective'
          } else if (normalizedRotation > 0 && normalizedRotation <= 30) {
            shotType = 'slight three-quarter left view'
            lensType = '85mm portrait lens'
            angleDetails = `Subject's LEFT side slightly visible, frontal composition dominant (${normalizedRotation}Â° counterclockwise from front)`
          } else if (normalizedRotation > 30 && normalizedRotation < 60) {
            shotType = 'three-quarter left shot'
            lensType = '85mm portrait lens'
            angleDetails = `Balanced composition showing subject's LEFT side and front face (${normalizedRotation}Â° counterclockwise from front)`
          } else if (normalizedRotation >= 60 && normalizedRotation < 90) {
            shotType = 'left side three-quarter view'
            lensType = '85mm portrait lens'
            angleDetails = `Subject's LEFT side dominant, approaching profile perspective (${normalizedRotation}Â° counterclockwise from front)`
          } else if (normalizedRotation === 90) {
            shotType = 'left side profile shot'
            lensType = '85mm portrait lens'
            angleDetails = `âš ï¸ PERPENDICULAR SIDE VIEW: Camera positioned 90Â° to subject's left, showing ONLY left profile, NO frontal face visible`
          } else if (normalizedRotation > 90 && normalizedRotation < 120) {
            shotType = 'left three-quarter back view'
            lensType = '50mm standard lens'
            angleDetails = `Subject's back and LEFT side visible, NO frontal face (${normalizedRotation}Â° counterclockwise from front)`
          } else if (normalizedRotation >= 120 && normalizedRotation < 165) {
            shotType = 'three-quarter back shot from left'
            lensType = '50mm standard lens'
            angleDetails = `Subject's back dominant with LEFT side visible (${normalizedRotation}Â° counterclockwise from front)`
          } else if (normalizedRotation >= 165 && normalizedRotation <= 195) {
            shotType = 'back view shot'
            lensType = '50mm standard lens'
            angleDetails = `âš ï¸ REAR VIEW: Camera positioned directly behind subject at 180Â°, subject facing AWAY from camera`
          } else if (normalizedRotation > 195 && normalizedRotation < 240) {
            shotType = 'three-quarter back shot from right'
            lensType = '50mm standard lens'
            angleDetails = `Subject's back dominant with RIGHT side visible (${normalizedRotation}Â° counterclockwise from front)`
          } else if (normalizedRotation >= 240 && normalizedRotation < 270) {
            shotType = 'right three-quarter back view'
            lensType = '50mm standard lens'
            angleDetails = `Subject's back and RIGHT side visible, NO frontal face (${normalizedRotation}Â° counterclockwise from front)`
          } else if (normalizedRotation === 270) {
            shotType = 'right side profile shot'
            lensType = '85mm portrait lens'
            angleDetails = `âš ï¸ PERPENDICULAR SIDE VIEW: Camera positioned 90Â° to subject's right, showing ONLY right profile, NO frontal face visible`
          } else if (normalizedRotation > 270 && normalizedRotation < 300) {
            shotType = 'right side three-quarter view'
            lensType = '85mm portrait lens'
            angleDetails = `Subject's RIGHT side dominant, approaching profile perspective (${normalizedRotation}Â° counterclockwise from front)`
          } else if (normalizedRotation >= 300 && normalizedRotation < 330) {
            shotType = 'three-quarter right shot'
            lensType = '85mm portrait lens'
            angleDetails = `Balanced composition showing subject's RIGHT side and front face (${normalizedRotation}Â° counterclockwise from front)`
          } else {
            shotType = 'slight three-quarter right view'
            lensType = '85mm portrait lens'
            angleDetails = `Subject's RIGHT side slightly visible, frontal composition dominant (${normalizedRotation}Â° counterclockwise from front)`
          }
          
          cameraDescription += `ğŸ“· SHOT TYPE: ${shotType}\n`
          cameraDescription += `ğŸ¥ LENS: ${lensType}\n`
          cameraDescription += `ğŸ“ ANGLE: ${angleDetails}\n`
          
          // ğŸ¬ Tilt í•´ì„ (Photography Perspective Terms)
          let perspectiveType = ''
          if (motionData.cameraTilt && motionData.cameraTilt !== 0) {
            const tiltRounded = Math.round(Math.abs(motionData.cameraTilt))
            if (motionData.cameraTilt > 0) {
              perspectiveType = 'high-angle perspective'
              cameraDescription += `ğŸ“· PERSPECTIVE: ${perspectiveType} (+${tiltRounded}Â°)\n`
              cameraDescription += `   Camera positioned ABOVE subject, looking DOWN at ${tiltRounded}Â° angle below horizontal\n`
              cameraDescription += '   Creates diminishing, vulnerable framing (subject appears smaller)\n'
            } else {
              perspectiveType = 'low-angle perspective'
              cameraDescription += `ğŸ“· PERSPECTIVE: ${perspectiveType} (-${tiltRounded}Â°)\n`
              cameraDescription += `   Camera positioned BELOW subject, looking UP at ${tiltRounded}Â° angle above horizontal\n`
              cameraDescription += '   Creates empowering, heroic framing (subject appears larger/dominant)\n'
            }
          }
          
          // ğŸ¬ Distance í•´ì„ (Cinematography Framing Terms)
          let framingType = ''
          if (motionData.cameraDistance && motionData.cameraDistance !== 1.0) {
            const distRounded = Math.round(motionData.cameraDistance * 100) / 100
            if (motionData.cameraDistance > 1.0) {
              if (distRounded >= 2.0) {
                framingType = 'wide shot'
              } else if (distRounded >= 1.5) {
                framingType = 'medium-wide shot'
              } else {
                framingType = 'medium shot'
              }
              cameraDescription += `ğŸ“· FRAMING: ${framingType} (${distRounded}x distance)\n`
              cameraDescription += `   Camera positioned farther away, showing more environment and context\n`
            } else {
              if (distRounded <= 0.5) {
                framingType = 'extreme close-up'
              } else if (distRounded <= 0.7) {
                framingType = 'close-up shot'
              } else {
                framingType = 'medium close-up'
              }
              cameraDescription += `ğŸ“· FRAMING: ${framingType} (${distRounded}x distance)\n`
              cameraDescription += `   Camera positioned closer, tight framing emphasizing details and expressions\n`
            }
          }
          
          // ğŸ¬ Rotation Subject í•´ì„ (Cinematography Method)
          if (normalizedRotation !== 0 && normalizedRotation !== 360) {
            if (motionData.rotationSubject === 'camera-orbit') {
              cameraDescription += `\nğŸ¬ CAMERA MOVEMENT: Orbital tracking shot (camera circles around stationary subject)\n`
              cameraDescription += '   âš ï¸ CRITICAL TECHNIQUE: Camera physically moves around subject on circular dolly/track\n'
              cameraDescription += '   âš ï¸ Subject remains STATIONARY - maintains same body orientation and facing direction\n'
              cameraDescription += '   âš ï¸ Only camera position changes (like photographer walking around statue)\n'
              cameraDescription += '   âš ï¸ Background perspective shifts - environment visible from new camera angle\n'
              cameraDescription += '   âš ï¸ Creates parallax effect - foreground/background move at different rates\n'
              cameraDescription += '   ğŸ’¡ CINEMA REFERENCE: The Matrix "bullet time", Inception hallway fight camera work\n'
            } else if (motionData.rotationSubject === 'character-turn') {
              cameraDescription += `\nğŸ§ SUBJECT MOVEMENT: Character turns/rotates body (camera stays fixed)\n`
              cameraDescription += '   âš ï¸ CRITICAL TECHNIQUE: Subject physically rotates their body to face new direction\n'
              cameraDescription += '   âš ï¸ Camera remains STATIONARY - fixed position (typically frontal)\n'
              cameraDescription += '   âš ï¸ Subject turns like person rotating on turntable or reacting to sound\n'
              cameraDescription += '   âš ï¸ Background stays FIXED - no perspective change, no parallax\n'
              cameraDescription += '   âš ï¸ Environment remains static from same camera viewpoint\n'
              cameraDescription += '   ğŸ’¡ CINEMA REFERENCE: Character turning to face someone off-camera, "slow turn reveal" shots\n'
            }
          }
          
          // ğŸ¬ Google Gemini ê³µì‹ í…œí”Œë¦¿ ì ìš©
          // Reference Imageê°€ ìˆìœ¼ë©´ ìºë¦­í„° ì¼ê´€ì„± + ì¹´ë©”ë¼, ì—†ìœ¼ë©´ ìˆœìˆ˜ ì¹´ë©”ë¼ ê°ë„
          if (referenceImages.length > 0) {
            // WITH REFERENCE: Character Consistency + Camera Transformation
            enhancedPrompt = `A photorealistic image of the subject from the reference image, maintaining EXACT character consistency.

ğŸ¥ CAMERA SPECIFICATIONS (Google Gemini Format):
${cameraDescription}

ğŸ“¸ CAPTURED WITH:
Captured with ${lensType}, ${perspectiveType || 'natural eye-level perspective'}, ${framingType || 'medium framing'}.
Professional photography lighting, emphasizing character details and textures.
High-quality rendering with accurate depth of field and natural bokeh.

âš ï¸ CRITICAL REQUIREMENTS (PRIORITY ORDER):

1ï¸âƒ£ CHARACTER CONSISTENCY (HIGHEST PRIORITY):
   âœ… PRESERVE EXACTLY from reference image:
   - Facial features, hair style/color, eye color, skin tone, expressions
   - Clothing design, outfit colors, materials, textures, accessories
   - Body proportions, build, height, posture characteristics
   - Visual style, rendering quality, artistic treatment
   - Color palette, tone, mood, lighting quality
   
   ğŸš« FORBIDDEN:
   - Changing character appearance or identity
   - Altering outfit design or colors
   - Modifying visual style or rendering quality

2ï¸âƒ£ CAMERA TRANSFORMATION (SECOND PRIORITY):
   âœ… APPLY EXACT CAMERA SPECIFICATIONS above:
   - Follow shot type and lens specifications precisely
   - Implement exact angle and perspective described
   - Apply specified framing and composition
   - For side views (90Â°/270Â°): Show ONLY profile, NO frontal face
   - For back view (180Â°): Subject faces AWAY, show back only
   
   ğŸš« FORBIDDEN:
   - Keeping same camera angle as reference
   - Approximating angles (${normalizedRotation}Â° is precise)
   - Ignoring perspective/framing specifications

ğŸ’¡ PHOTOGRAPHY ANALOGY:
You are photographing the SAME character from a DIFFERENT camera position.
Think: Professional photographer shooting model from new angle.
- Character = 100% IDENTICAL to reference
- Camera = MOVED to new position as specified

${prompt}

Generate the image with PERFECT character consistency and EXACT camera transformation as specified.`
          } else {
            // WITHOUT REFERENCE: Pure Camera Angle Specification
            enhancedPrompt = `A photorealistic image following precise camera specifications.

ğŸ¥ CAMERA SPECIFICATIONS (Google Gemini Format):
${cameraDescription}

ğŸ“¸ CAPTURED WITH:
Captured with ${lensType}, ${perspectiveType || 'natural eye-level perspective'}, ${framingType || 'medium framing'}.
Professional cinematic lighting creating a compelling atmosphere.
High-quality rendering with accurate depth of field and natural bokeh effect.

âš ï¸ CRITICAL CAMERA REQUIREMENTS:

âœ… MANDATORY EXECUTION:
- STRICTLY follow shot type and lens specifications above
- PRECISELY implement the angle described (${normalizedRotation}Â° is exact)
- ACCURATELY apply perspective and framing specified
- For frontal (0Â°): Straight-on view, eye level, balanced composition
- For three-quarter views: Show both side and front in balanced mix
- For side views (90Â°/270Â°): PERPENDICULAR - show COMPLETE side profile, NO frontal face
- For back view (180Â°): Subject facing AWAY from camera, rear view only
- Use cinematic composition principles and rule of thirds

ğŸš« FORBIDDEN:
- Ignoring camera specifications
- Using default/standard camera angles instead of specified angles
- Approximating angles (${normalizedRotation}Â° must be precise)
- Showing frontal face when side/back view specified
- Deviating from lens or perspective specifications

ğŸ’¡ PHOTOGRAPHY DIRECTION:
The camera angle/position is the PRIMARY creative requirement.
Generate the subject/scene from THIS EXACT camera perspective.
Think: Professional cinematographer executing precise camera placement.

${prompt}

Generate the image from the EXACT camera position, angle, lens, and framing specified above.`
          }
        }
      }
      // ğŸ¯ Grid Composer + LLM: ì°¸ì¡° ì •í™•ë„ì— ë”°ë¼ ì§€ì‹œ ì¶”ê°€
      else if (hasGridComposerRef && hasLLMPrompt) {
        if (referenceMode === 'exact') {
          // ì •í™•ì„± ëª¨ë“œ: ì°¸ì¡° ì´ë¯¸ì§€ PIXEL-LEVEL ë³µì œ
          enhancedPrompt = `âš ï¸âš ï¸âš ï¸ CRITICAL: EXACT REFERENCE IMAGE REPLICATION REQUIRED âš ï¸âš ï¸âš ï¸
${gridLabelInfoForNano}
STRICT MODE: Reference image is ABSOLUTE PRIMARY source.
Text prompt = ONLY for understanding story/actions. Your task = PIXEL-PERFECT VISUAL COPY.

ğŸ“Œ TEXT PROMPT = STORY/ACTIONS (PRESERVE 100%):
- If text says "holding helmet" â†’ Generate "holding helmet" (NOT "wearing helmet"!)
- If text says "walking" â†’ Generate "walking" (keep action!)
- If text says "one person" â†’ Generate "one person" (NOT "two"!)
- Preserve ALL actions, character counts, story elements from text prompt

ğŸ¨ REFERENCE IMAGE = VISUAL DESIGN (REPLICATE 100%):
- S1 Background: Copy EXACT colors, lighting, structure, materials
- S2 Character: Copy EXACT appearance, outfit, hair, facial features
- S3 Object/Robot: Copy EXACT colors (red=red, white=white), shape, design
- Use reference for HOW things LOOK, use text for WHAT is HAPPENING

ğŸš« ABSOLUTELY FORBIDDEN:
- Changing actions ("holding" â†’ "wearing", "walking" â†’ "standing")
- Changing character count ("one" â†’ "two")
- Reinterpreting background (S1 must match EXACTLY!)
- ANY color changes (redâ†’red, blueâ†’blue, whiteâ†’white, blackâ†’black)
- ANY material changes (metalâ†’metal, fabricâ†’fabric, plasticâ†’plastic)
- ANY shape, proportion, design modifications
- ANY creative variations or "similar" versions
- ANY artistic interpretation of reference visuals

âœ… MANDATORY REQUIREMENTS:
- EXACT pixel-by-pixel visual replication of S1, S2, S3 elements
- 100% color preservation (use exact RGB values from reference)
- 100% material/texture preservation from reference
- 100% lighting/shadow preservation from reference
- 100% action/story preservation from text prompt
- If text says "holding helmet", person MUST be holding (not wearing) helmet
- Background MUST match S1 reference exactly
- Reference visuals = TOP PRIORITY for appearance
- Text prompt = TOP PRIORITY for actions/story

Extract visual characteristics from each labeled section and replicate EXACTLY. Use text prompt ONLY for understanding actions and story flow.

---

${enhancedPrompt}`
        } else if (referenceMode === 'balanced') {
          // ê· í˜• ëª¨ë“œ: í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ê· í˜•
          enhancedPrompt = `âš–ï¸ BALANCED MODE: Reference Image + Text Prompt
${gridLabelInfoForNano}
Use reference image AND text prompt equally:
- Reference image: Visual style, colors, materials, composition of each labeled element
- Text prompt: Specific details, arrangement, actions, story

âš ï¸ IMPORTANT: Preserve actions from text prompt (e.g., "holding" stays "holding", not "wearing").

Maintain visual consistency with reference while incorporating text details.

---

${enhancedPrompt}`
        } else if (referenceMode === 'creative') {
          // ì°½ì˜ì„± ëª¨ë“œ: í…ìŠ¤íŠ¸ ìœ„ì£¼, ì´ë¯¸ì§€ëŠ” ì˜ê°ë§Œ
          enhancedPrompt = `ğŸ¨ CREATIVE MODE: Text Prompt Primary
${gridLabelInfoForNano}
Focus on text prompt as main instruction.
Reference image = INSPIRATION ONLY (style, mood, general aesthetic).

Feel free to creatively interpret and generate based on text description.

---

${enhancedPrompt}`
        }
      }
      
      const model = data.model ?? 'gemini-3-pro-image-preview'
      
      console.log('ğŸ¨ Nano Image Generation:', {
        model,
        resolution: data.resolution,
        aspectRatio: data.aspectRatio,
        referenceCount: referenceImages.length,
        referenceMode: hasGridComposerRef ? referenceMode : 'N/A',
      })
      
      // Use first reference image as primary input (for backward compatibility)
      const primaryReference = referenceImages[0]
      
      // âœ… Apply retry logic with exponential backoff
      const result = await retryWithBackoff(
        () => client.generateImage(
          enhancedPrompt,
          data.aspectRatio,
          primaryReference,
          model,
          data.resolution,
          abortController.signal,
        ),
        {
          maxAttempts: 3,
          initialDelay: 1000,
          onRetry: (attempt, error) => {
            console.warn(`ğŸ”„ Retry attempt ${attempt}:`, error.message)
            updateNode((prev) => ({
              ...prev,
              error: `ì¬ì‹œë„ ì¤‘... (${attempt}/3)`,
            }))
          },
        }
      )

      // Check if aborted after completion
      if (abortController.signal.aborted) {
        throw new Error('ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.')
      }

      // ğŸ”¥ IndexedDB/S3ì— ì´ë¯¸ì§€ ì €ì¥í•˜ê³  ì°¸ì¡° ë°˜í™˜
      let savedImageRef = result.imageDataUrl
      try {
        const imageId = `nano-output-${id}-${Date.now()}`
        console.log('ğŸ’¾ Nano Image: IndexedDB/S3ì— ì¶œë ¥ ì´ë¯¸ì§€ ì €ì¥ ì‹œì‘...', imageId)
        
        savedImageRef = await saveImage(imageId, result.imageDataUrl, id, true)
        console.log('âœ… Nano Image: ì¶œë ¥ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ', savedImageRef)
      } catch (error) {
        console.error('âŒ Nano Image: ì¶œë ¥ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨, DataURLì„ ì§ì ‘ ì‚¬ìš©', error)
        // í´ë°±: DataURLì„ ì§ì ‘ ì‚¬ìš© (ë¹„ê¶Œì¥)
      }

      updateNode((prev) => ({
        ...prev,
        status: 'completed',
        outputImageUrl: result.imageUrl,
        outputImageDataUrl: savedImageRef, // idb:xxx ë˜ëŠ” s3:xxx ì°¸ì¡°
        generatedModel: model,
        generatedResolution: data.resolution,
        generatedAspectRatio: data.aspectRatio,
        error: undefined,
      }))
    } catch (error) {
      // Don't show retry errors if aborted
      if (!abortController.signal.aborted) {
        updateNode((prev) => ({
          ...prev,
          status: 'error',
          error: formatErrorMessage(error),
        }))
      }
    } finally {
      // Clean up abort controller
      const controllers = get().abortControllers
      controllers.delete(id)
      set({ abortControllers: new Map(controllers) })
    }
  },
  runGeminiNode: async (id) => {
    const { nodes, edges, abortControllers } = get()
    const current = nodes.find((node) => node.id === id)
    if (!current || current.type !== 'geminiVideo') return

    // âœ… Prevent duplicate execution
    const currentData = current.data as GeminiVideoNodeData
    if (currentData.status === 'processing') {
      console.warn('âš ï¸ Gemini node is already processing')
      return
    }
    
    // ğŸ§¹ ì´ì „ ì—ëŸ¬ ë¨¼ì € ì§€ìš°ê¸°
    set({
      nodes: get().nodes.map((node) =>
        node.id === id
          ? { ...node, data: { ...node.data, error: undefined } }
          : node
      ),
    })

    // âœ… Rate limiting
    const now = Date.now()
    const lastExecution = (currentData as any).lastExecutionTime || 0
    const minInterval = 5000 // 5 seconds for video (longer than image)
    
    if (now - lastExecution < minInterval) {
      const waitTime = Math.ceil((minInterval - (now - lastExecution)) / 1000)
      set({
        nodes: nodes.map((node) =>
          node.id === id
            ? {
                ...node,
                data: {
                  ...node.data,
                  status: 'error',
                  error: `ë„ˆë¬´ ë¹ ë¥´ê²Œ ì‹¤í–‰í–ˆìŠµë‹ˆë‹¤. ${waitTime}ì´ˆ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.`,
                },
              }
            : node,
        ),
      })
      return
    }

    // Create abort controller for this execution
    const abortController = new AbortController()
    abortControllers.set(id, abortController)
    set({ abortControllers: new Map(abortControllers) })

    const updateNode = (updater: (data: NodeData) => NodeData) => {
      set({
        nodes: get().nodes.map((node) =>
          node.id === id ? { ...node, data: updater(node.data) } : node,
        ),
      })
    }

    const incoming = getIncomingNodes(id, edges, get().nodes)
    const imageNode =
      incoming.find((node) => node.type === 'imageImport') ??
      incoming.find((node) => node.type === 'nanoImage') ??
      incoming.find((node) => node.type === 'gridComposer') ??
      incoming.find((node) => node.type === 'cellRegenerator')
    const promptNode = incoming.find(
      (node) => node.type === 'motionPrompt' || node.type === 'textPrompt' || node.type === 'llmPrompt',
    )

    let inputImageUrl: string | undefined
    let inputImageDataUrl: string | undefined
    
    if (imageNode?.type === 'imageImport') {
      inputImageUrl = (imageNode.data as ImageImportNodeData).imageUrl
      inputImageDataUrl = (imageNode.data as ImageImportNodeData).imageDataUrl
    } else if (imageNode?.type === 'nanoImage') {
      inputImageUrl = (imageNode.data as NanoImageNodeData).outputImageUrl
      inputImageDataUrl = (imageNode.data as NanoImageNodeData).outputImageDataUrl
    } else if (imageNode?.type === 'gridComposer') {
      const imgData = imageNode.data as any
      inputImageUrl = imgData.composedImageUrl || imgData.composedImageDataUrl
      inputImageDataUrl = imgData.composedImageDataUrl || imgData.composedImageUrl
    } else if (imageNode?.type === 'cellRegenerator') {
      // Cell Regenerator should use specific cell outputs
      inputImageUrl = undefined
      inputImageDataUrl = undefined
    }

    const inputPrompt =
      promptNode?.type === 'motionPrompt'
        ? (promptNode.data as MotionPromptNodeData).combinedPrompt
        : promptNode?.type === 'textPrompt'
          ? (promptNode.data as TextPromptNodeData).prompt
          : promptNode?.type === 'llmPrompt'
            ? (promptNode.data as any).outputPrompt || ''
            : ''

    // âš ï¸ Early validation BEFORE storage conversion (only check if nodes are connected)
    if (!inputPrompt) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'í”„ë¡¬í”„íŠ¸ ë…¸ë“œë¥¼ ì—°ê²°í•´ ì£¼ì„¸ìš”.',
      }))
      return
    }
    
    if (!imageNode) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'ì´ë¯¸ì§€ ë…¸ë“œë¥¼ ì—°ê²°í•´ ì£¼ì„¸ìš”.',
      }))
      return
    }
    
    if (!inputImageDataUrl) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'ì´ë¯¸ì§€ ë…¸ë“œê°€ ì—°ê²°ë˜ì—ˆì§€ë§Œ ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ ë…¸ë“œì—ì„œ "Generate" ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.',
      }))
      return
    }

    const apiKey = get().apiKey || import.meta.env.VITE_GEMINI_API_KEY || ''
    
    if (!apiKey) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'Gemini API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤. ìƒë‹¨ "API Key" ë²„íŠ¼ì„ ëˆŒëŸ¬ì„œ ì„¤ì •í•˜ì„¸ìš”.',
      }))
      return
    }

    updateNode((prev) => ({
      ...prev,
      status: 'processing',
      error: undefined,
      inputImageUrl,
      inputImageDataUrl,
      inputPrompt,
      progress: 10,
      lastExecutionTime: now,
    }))

    const client = new GeminiAPIClient(apiKey)
    
    // âœ… Convert storage references to actual DataURLs
    let actualInputImageDataUrl = inputImageDataUrl
    
    console.log('ğŸ” Gemini: Input image type:', inputImageDataUrl?.substring(0, 50))
    
    if (inputImageDataUrl && (inputImageDataUrl.startsWith('idb:') || inputImageDataUrl.startsWith('s3:'))) {
      console.log('ğŸ”„ Gemini: Converting image from storage reference...')
      try {
        const { getImage } = await import('../utils/indexedDB')
        const dataURL = await getImage(inputImageDataUrl)
        if (dataURL) {
          actualInputImageDataUrl = dataURL
          console.log('âœ… Gemini: Image loaded from storage, size:', dataURL.length, 'chars')
        } else {
          console.error('âŒ Gemini: Failed to load image from storage')
          // ğŸ”¥ Fallback: Try to use inputImageUrl (blob URL) if available
          if (inputImageUrl && inputImageUrl.startsWith('data:')) {
            console.warn('âš ï¸ Gemini: Falling back to inputImageUrl (DataURL)')
            actualInputImageDataUrl = inputImageUrl
          } else if (inputImageUrl && inputImageUrl.startsWith('blob:')) {
            console.warn('âš ï¸ Gemini: inputImageUrl is a blob URL (may be invalid after refresh)')
            updateNode((prev) => ({
              ...prev,
              status: 'error',
              error: 'ì´ë¯¸ì§€ë¥¼ Storageì—ì„œ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ë¥¼ ë‹¤ì‹œ ìƒì„±í•´ì£¼ì„¸ìš”.',
            }))
            return
          } else {
            updateNode((prev) => ({
              ...prev,
              status: 'error',
              error: 'ì´ë¯¸ì§€ë¥¼ Storageì—ì„œ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ë¥¼ ë‹¤ì‹œ ìƒì„±í•´ì£¼ì„¸ìš”.',
            }))
            return
          }
        }
      } catch (error) {
        console.error('âŒ Gemini: Error loading image:', error)
        updateNode((prev) => ({
          ...prev,
          status: 'error',
          error: `ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: ${error}`,
        }))
        return
      }
    } else if (!inputImageDataUrl) {
      console.error('âŒ Gemini: No input image provided!')
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'ì…ë ¥ ì´ë¯¸ì§€ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.',
      }))
      return
    } else {
      console.log('âœ… Gemini: Using direct DataURL (not a storage reference)')
    }
    
    console.log('ğŸ¬ Gemini Video ìƒì„± ì‹œì‘:', {
      prompt: inputPrompt.substring(0, 50) + '...',
      model: (current.data as GeminiVideoNodeData).model,
      imageType: actualInputImageDataUrl?.substring(0, 30),
      imageSize: actualInputImageDataUrl?.length,
    })
    
    // âœ… Final validation before API call
    if (!actualInputImageDataUrl || actualInputImageDataUrl.startsWith('idb:') || actualInputImageDataUrl.startsWith('s3:')) {
      console.error('âŒ Gemini: Image is still a storage reference or empty!', actualInputImageDataUrl?.substring(0, 50))
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨. Storage ì°¸ì¡°ê°€ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.',
      }))
      return
    }
    
    console.log('âœ… Gemini: All validations passed, calling API...')

    const progressTimer = setInterval(() => {
      updateNode((prev) => {
        const data = prev as GeminiVideoNodeData
        if (data.status !== 'processing') return prev
        return {
          ...prev,
          progress: Math.min(data.progress + 12, 90),
        }
      })
    }, 500)

    try {
      if (abortController.signal.aborted) {
        throw new Error('ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.')
      }

      const settings = current.data as GeminiVideoNodeData
      
      // âœ… Apply retry logic
      const outputVideoUrl = await retryWithBackoff(
        () => client.generateMedia(
          inputPrompt,
          {
            mediaType: 'video',
            duration: settings.duration,
            quality: settings.quality,
            motionIntensity: settings.motionIntensity,
          },
          actualInputImageDataUrl,
          settings.model,
          abortController.signal,
        ),
        {
          maxAttempts: 2, // Less retries for video (expensive)
          initialDelay: 2000,
          onRetry: (attempt) => {
            console.warn(`ğŸ”„ Gemini Video retry ${attempt}/2`)
            updateNode((prev) => ({
              ...prev,
              error: `ì¬ì‹œë„ ì¤‘... (${attempt}/2)`,
            }))
          },
        }
      )

      if (abortController.signal.aborted) {
        throw new Error('ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.')
      }

      updateNode((prev) => ({
        ...prev,
        status: 'completed',
        outputVideoUrl,
        progress: 100,
        error: undefined,
      }))

    } catch (error) {
      if (!abortController.signal.aborted) {
        updateNode((prev) => ({
          ...prev,
          status: 'error',
          error: formatErrorMessage(error),
        }))
      }
    } finally {
      clearInterval(progressTimer)
      const controllers = get().abortControllers
      controllers.delete(id)
      set({ abortControllers: new Map(controllers) })
    }
  },
  runCellRegeneratorNode: async (id) => {
    const { nodes, edges } = get()
    const current = nodes.find((node) => node.id === id)
    if (!current || current.type !== 'cellRegenerator') return

    const data = current.data as any
    
    // Update to processing status
    set({
      nodes: nodes.map((node) =>
        node.id === id
          ? { ...node, data: { ...node.data, status: 'processing', error: undefined } }
          : node,
      ),
    })

    try {
      // 1. Validate inputs
      if (!data.gridLayout || !data.slots || data.slots.length === 0) {
        throw new Error('Grid ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. Grid Nodeë¥¼ ì—°ê²°í•´ì£¼ì„¸ìš”.')
      }

      // 2. Get input image (resolve storage reference if needed)
      let inputImageDataUrl = data.inputImageDataUrl || data.inputImageUrl
      if (!inputImageDataUrl) {
        throw new Error('Grid ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ë¥¼ ì—°ê²°í•´ì£¼ì„¸ìš”.')
      }

      // Resolve storage reference (idb: or s3:)
      if (inputImageDataUrl.startsWith('idb:') || inputImageDataUrl.startsWith('s3:')) {
        const resolved = await getImage(inputImageDataUrl)
        if (!resolved) {
          throw new Error('ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
        }
        inputImageDataUrl = resolved
      }

      // 3. Parse grid layout
      const [rows, cols] = data.gridLayout.split('x').map(Number)
      if (!rows || !cols || rows < 1 || cols < 1) {
        throw new Error(`ì˜ëª»ëœ Grid Layout: ${data.gridLayout}`)
      }

      // Determine which slots to extract based on outgoing connections
      const connectedSlotIds = edges
        .filter((e) => e.source === id && e.sourceHandle && e.sourceHandle !== 'output')
        .map((e) => e.sourceHandle!)
      const uniqueConnectedSlots = [...new Set(connectedSlotIds)]

      // Also include manually selected slots
      const selectedSlots: string[] = data.selectedSlots || []
      const allTargetSlots = [...new Set([...uniqueConnectedSlots, ...selectedSlots])]

      const slotsToExtract = allTargetSlots.length > 0
        ? data.slots.filter((slot: any) => allTargetSlots.includes(slot.id))
        : data.slots

      console.log(`ğŸ”ª Cell Regenerator: Splitting ${rows}x${cols} grid, extracting ${slotsToExtract.length}/${data.slots.length} cells (connected: ${uniqueConnectedSlots.join(',')}, selected: ${selectedSlots.join(',')})`)

      // 4. Load image
      const img = new Image()
      img.crossOrigin = 'anonymous'

      await new Promise<void>((resolve, reject) => {
        img.onload = () => resolve()
        img.onerror = () => reject(new Error('ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨'))
        img.src = inputImageDataUrl
      })

      console.log(`ğŸ“ Image loaded: ${img.width}x${img.height}`)

      // 5. Calculate cell dimensions
      const cellWidth = Math.floor(img.width / cols)
      const cellHeight = Math.floor(img.height / rows)

      console.log(`ğŸ“¦ Cell size: ${cellWidth}x${cellHeight}`)

      // 6. Split into cells
      const canvas = document.createElement('canvas')
      const ctx = canvas.getContext('2d')
      if (!ctx) {
        throw new Error('Canvas ìƒì„± ì‹¤íŒ¨')
      }

      // ì—°ê²°ëœ ìŠ¬ë¡¯ì´ ìˆìœ¼ë©´ í•´ë‹¹ ìŠ¬ë¡¯ë§Œ ê²°ê³¼ì— í¬í•¨, ì—†ìœ¼ë©´ ì „ì²´ ë³´ì¡´
      const regeneratedImages: { [key: string]: string } = allTargetSlots.length > 0
        ? {}  // ì—°ê²° ê¸°ë°˜ ì¶”ì¶œ: ê¹¨ë—í•œ ìƒíƒœì—ì„œ ì‹œì‘
        : { ...data.regeneratedImages }  // ì „ì²´ ì¶”ì¶œ: ê¸°ì¡´ ê²°ê³¼ ë³´ì¡´

      for (let i = 0; i < data.slots.length; i++) {
        const slot = data.slots[i]

        // Skip slots not in the extraction list
        if (!slotsToExtract.some((s: any) => s.id === slot.id)) continue

        const row = Math.floor(i / cols)
        const col = i % cols

        console.log(`ğŸ” Processing ${slot.id}: row=${row}, col=${col}, i=${i}`)

        canvas.width = cellWidth
        canvas.height = cellHeight

        console.log(`ğŸ“ Canvas size: ${canvas.width}x${canvas.height}`)

        // Clear canvas
        ctx.clearRect(0, 0, cellWidth, cellHeight)

        // Calculate source coordinates
        const sx = col * cellWidth
        const sy = row * cellHeight

        console.log(`âœ‚ï¸ Cutting from source: sx=${sx}, sy=${sy}, sw=${cellWidth}, sh=${cellHeight}`)

        // Draw cell from source image
        ctx.drawImage(
          img,
          sx,
          sy,
          cellWidth,
          cellHeight,
          0,
          0,
          cellWidth,
          cellHeight,
        )

        // Convert to data URL (JPEG for better compression)
        const dataUrl = canvas.toDataURL('image/jpeg', 0.9)

        console.log(`ğŸ’¾ Generated dataURL length: ${dataUrl.length}`)

        // Save to storage
        const imageId = `cell-${id}-${slot.id}-${Date.now()}`
        const storageRef = await saveImage(imageId, dataUrl, id, true)

        regeneratedImages[slot.id] = storageRef
        console.log(`âœ… Cell ${slot.id} saved: ${storageRef}`)
      }

      // 7. Update node data + invalidate downstream Nano Banana outputs
      const downstreamNanoEdges = edges.filter(
        (e) => e.source === id && get().nodes.find((n) => n.id === e.target)?.type === 'nanoImage'
      )
      const downstreamNanoIds = new Set(downstreamNanoEdges.map((e) => e.target))

      set({
        nodes: get().nodes.map((node) => {
          if (node.id === id) {
            return {
              ...node,
              data: {
                ...node.data,
                regeneratedImages,
                status: 'completed',
                error: undefined,
              },
            }
          }
          // í•˜ìœ„ Nano Banana ë…¸ë“œì˜ ì´ì „ ì¶œë ¥ì„ ì´ˆê¸°í™” (ì¬ìƒì„± ìœ ë„)
          if (downstreamNanoIds.has(node.id)) {
            console.log(`ğŸ”„ Auto-invalidating downstream Nano Banana: ${node.id}`)
            return {
              ...node,
              data: {
                ...node.data,
                outputImageUrl: undefined,
                outputImageDataUrl: undefined,
                status: 'idle',
                error: 'âš ï¸ ìƒìœ„ Cell Regeneratorê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. Generateë¥¼ ëˆŒëŸ¬ ì¬ìƒì„±í•˜ì„¸ìš”.',
              },
            }
          }
          return node
        }),
      })

      console.log(`âœ… Cell Regenerator: Successfully separated ${slotsToExtract.length} cells, invalidated ${downstreamNanoIds.size} downstream nodes`)
    } catch (error) {
      console.error('âŒ Cell Regenerator error:', error)
      set({
        nodes: nodes.map((node) =>
          node.id === id
            ? {
                ...node,
                data: {
                  ...node.data,
                  status: 'error',
                  error: error instanceof Error ? error.message : String(error),
                },
              }
            : node,
        ),
      })
    }
  },
  runKlingNode: async (id) => {
    const { nodes, edges, abortControllers } = get()
    const current = nodes.find((node) => node.id === id)
    if (!current || current.type !== 'klingVideo') return

    // âœ… Prevent duplicate execution
    const currentData = current.data as KlingVideoNodeData
    if (currentData.status === 'processing') {
      console.warn('âš ï¸ Kling node is already processing')
      return
    }

    // âœ… Rate limiting
    const now = Date.now()
    const lastExecution = (currentData as any).lastExecutionTime || 0
    const minInterval = 5000 // 5 seconds for video
    
    if (now - lastExecution < minInterval) {
      const waitTime = Math.ceil((minInterval - (now - lastExecution)) / 1000)
      set({
        nodes: nodes.map((node) =>
          node.id === id
            ? {
                ...node,
                data: {
                  ...node.data,
                  status: 'error',
                  error: `ë„ˆë¬´ ë¹ ë¥´ê²Œ ì‹¤í–‰í–ˆìŠµë‹ˆë‹¤. ${waitTime}ì´ˆ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.`,
                },
              }
            : node,
        ),
      })
      return
    }

    // Create abort controller for this execution
    const abortController = new AbortController()
    abortControllers.set(id, abortController)
    set({ abortControllers: new Map(abortControllers) })

    const updateNode = (updater: (data: NodeData) => NodeData) => {
      set({
        nodes: get().nodes.map((node) =>
          node.id === id ? { ...node, data: updater(node.data) } : node,
        ),
      })
    }

    const imageNodeTypes = new Set(['imageImport', 'nanoImage', 'gridComposer', 'cellRegenerator'])
    
    // Start Image (ê¸°ë³¸ ì´ë¯¸ì§€) - 'start' í•¸ë“¤ ë˜ëŠ” í•¸ë“¤ ID ì—†ëŠ” ì—°ê²°
    const startImageEdges = edges.filter(
      (e) => e.target === id && (!e.targetHandle || e.targetHandle === 'start')
    )
    const hasStartImageEdge = startImageEdges.length > 0
    
    const imageIncomingEdges = edges.filter((e) => {
      if (e.target !== id) return false
      const node = nodes.find((n) => n.id === e.source)
      return imageNodeTypes.has(node?.type ?? '')
    })
    
    console.log('ğŸ” Kling: Start image edges found:', startImageEdges.length)
    
    const startImageNode = startImageEdges.find((e) => {
      const node = nodes.find((n) => n.id === e.source)
      console.log(`ğŸ” Kling: Checking source node ${e.source} type: ${node?.type}`)
      return imageNodeTypes.has(node?.type ?? '')
    })
    
    const startImageNodeData = startImageNode ? nodes.find((n) => n.id === startImageNode.source) : undefined
    
    // End Image (ë í”„ë ˆì„) - 'end' í•¸ë“¤ ì—°ê²°
    const endImageEdges = edges.filter(
      (e) => e.target === id && e.targetHandle === 'end'
    )
    const endImageNode = endImageEdges.find((e) => {
      const node = nodes.find((n) => n.id === e.source)
      return node?.type === 'imageImport' || node?.type === 'nanoImage' || node?.type === 'gridComposer' || node?.type === 'cellRegenerator'
    })
    const endImageNodeData = endImageNode ? nodes.find((n) => n.id === endImageNode.source) : undefined

    // Prompt ë…¸ë“œ
    const incoming = getIncomingNodes(id, edges, get().nodes)
    const promptNode = incoming.find(
      (node) => node.type === 'motionPrompt' || node.type === 'textPrompt' || node.type === 'llmPrompt',
    )

    // Start Image ë°ì´í„°
    let inputImageUrl: string | undefined
    let inputImageDataUrl: string | undefined
    
    if (startImageNodeData?.type === 'imageImport') {
      const imgData = startImageNodeData.data as ImageImportNodeData
      inputImageUrl = imgData.imageUrl
      inputImageDataUrl = imgData.imageDataUrl
    } else if (startImageNodeData?.type === 'nanoImage') {
      const imgData = startImageNodeData.data as NanoImageNodeData
      inputImageUrl = imgData.outputImageUrl
      inputImageDataUrl = imgData.outputImageDataUrl
    } else if (startImageNodeData?.type === 'gridComposer') {
      const imgData = startImageNodeData.data as any
      inputImageUrl = imgData.composedImageUrl || imgData.composedImageDataUrl
      inputImageDataUrl = imgData.composedImageDataUrl || imgData.composedImageUrl
    } else if (startImageNodeData?.type === 'cellRegenerator') {
      const imgData = startImageNodeData.data as any
      // For cell regenerator, try to get image from sourceHandle (e.g., S1, S2, etc.)
      const cellId = startImageNode?.sourceHandle
      if (cellId && imgData.regeneratedImages?.[cellId]) {
        inputImageDataUrl = imgData.regeneratedImages[cellId]
        inputImageUrl = inputImageDataUrl
      } else {
        // Fallback: use first available cell
        const firstCellId = Object.keys(imgData.regeneratedImages || {})[0]
        if (firstCellId) {
          inputImageDataUrl = imgData.regeneratedImages[firstCellId]
          inputImageUrl = inputImageDataUrl
        }
      }
    }
    
    if (!inputImageDataUrl && inputImageUrl) {
      if (
        inputImageUrl.startsWith('data:') ||
        inputImageUrl.startsWith('idb:') ||
        inputImageUrl.startsWith('s3:')
      ) {
        inputImageDataUrl = inputImageUrl
      }
    }
    
    console.log('ğŸ” Kling: Final start image data:', { 
      nodeType: startImageNodeData?.type,
      hasImageUrl: !!inputImageUrl,
      hasImageDataUrl: !!inputImageDataUrl,
      imageDataUrlPrefix: inputImageDataUrl?.substring(0, 20)
    })
    
    // End Image ë°ì´í„°
    let endImageUrl: string | undefined
    let endImageDataUrl: string | undefined
    
    if (endImageNodeData?.type === 'imageImport') {
      const imgData = endImageNodeData.data as ImageImportNodeData
      endImageUrl = imgData.imageUrl
      endImageDataUrl = imgData.imageDataUrl
    } else if (endImageNodeData?.type === 'nanoImage') {
      const imgData = endImageNodeData.data as NanoImageNodeData
      endImageUrl = imgData.outputImageUrl
      endImageDataUrl = imgData.outputImageDataUrl
    } else if (endImageNodeData?.type === 'gridComposer') {
      const imgData = endImageNodeData.data as any
      endImageUrl = imgData.composedImageUrl || imgData.composedImageDataUrl
      endImageDataUrl = imgData.composedImageDataUrl || imgData.composedImageUrl
    } else if (endImageNodeData?.type === 'cellRegenerator') {
      const imgData = endImageNodeData.data as any
      const cellId = endImageNode?.sourceHandle
      if (cellId && imgData.regeneratedImages?.[cellId]) {
        endImageDataUrl = imgData.regeneratedImages[cellId]
        endImageUrl = endImageDataUrl
      }
    }

    const inputPrompt =
      promptNode?.type === 'motionPrompt'
        ? (promptNode.data as MotionPromptNodeData).combinedPrompt
        : promptNode?.type === 'textPrompt'
          ? (promptNode.data as TextPromptNodeData).prompt
          : promptNode?.type === 'llmPrompt'
            ? (promptNode.data as any).outputPrompt || ''
            : ''

    // âš ï¸ Early validation BEFORE storage conversion (only check if nodes are connected)
    if (!inputPrompt) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'í”„ë¡¬í”„íŠ¸ ë…¸ë“œë¥¼ ì—°ê²°í•´ ì£¼ì„¸ìš”.',
      }))
      return
    }
    
    if (!hasStartImageEdge) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error:
          imageIncomingEdges.length > 0
            ? 'Start ImageëŠ” íŒŒë€ Start í•¸ë“¤ì— ì—°ê²°í•´ ì£¼ì„¸ìš”.'
            : 'Start Image ë…¸ë“œë¥¼ ì—°ê²°í•´ ì£¼ì„¸ìš”.',
      }))
      return
    }
    
    if (!startImageNode) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'Start Imageì—ëŠ” ì´ë¯¸ì§€ ë…¸ë“œë§Œ ì—°ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
      }))
      return
    }
    
    if (!inputImageDataUrl) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'Start Image ë…¸ë“œê°€ ì—°ê²°ë˜ì—ˆì§€ë§Œ ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ ë…¸ë“œì—ì„œ "Generate" ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.',
      }))
      return
    }

    updateNode((prev) => ({
      ...prev,
      status: 'processing',
      error: undefined,
      inputImageUrl,
      inputImageDataUrl,
      endImageUrl,
      endImageDataUrl,
      inputPrompt,
      progress: 10,
      lastExecutionTime: now,
    }))

    const klingApiKey = get().klingApiKey || import.meta.env.VITE_KLING_API_KEY || ''
    const client = klingApiKey ? new KlingAPIClient(klingApiKey) : new MockKlingAPI()

    // âœ… Convert storage references to actual DataURLs
    let actualStartImageDataUrl = inputImageDataUrl
    let actualEndImageDataUrl = endImageDataUrl
    
    console.log('ğŸ” Kling: Input start image type:', inputImageDataUrl?.substring(0, 50))
    
    // Convert start image if it's a storage reference
    if (inputImageDataUrl && (inputImageDataUrl.startsWith('idb:') || inputImageDataUrl.startsWith('s3:'))) {
      console.log('ğŸ”„ Kling: Converting start image from storage reference...')
      try {
        const { getImage } = await import('../utils/indexedDB')
        const dataURL = await getImage(inputImageDataUrl)
        if (dataURL) {
          actualStartImageDataUrl = dataURL
          console.log('âœ… Kling: Start image loaded from storage, size:', dataURL.length, 'chars')
          console.log('âœ… Kling: Start image type:', dataURL.substring(0, 50))
        } else {
          console.error('âŒ Kling: Failed to load start image from storage (returned null/undefined)')
          // ğŸ”¥ Fallback: Try to use inputImageUrl (blob URL) if available
          if (inputImageUrl && inputImageUrl.startsWith('data:')) {
            console.warn('âš ï¸ Kling: Falling back to inputImageUrl (DataURL)')
            actualStartImageDataUrl = inputImageUrl
          } else if (inputImageUrl && inputImageUrl.startsWith('blob:')) {
            console.warn('âš ï¸ Kling: inputImageUrl is a blob URL (may be invalid after refresh)')
            updateNode((prev) => ({
              ...prev,
              status: 'error',
              error: 'Start ì´ë¯¸ì§€ë¥¼ Storageì—ì„œ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ë¥¼ ë‹¤ì‹œ ìƒì„±í•´ì£¼ì„¸ìš”.',
            }))
            return
          } else {
            updateNode((prev) => ({
              ...prev,
              status: 'error',
              error: 'Start ì´ë¯¸ì§€ë¥¼ Storageì—ì„œ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ë¥¼ ë‹¤ì‹œ ìƒì„±í•´ì£¼ì„¸ìš”.',
            }))
            return
          }
        }
      } catch (error) {
        console.error('âŒ Kling: Error loading start image:', error)
        updateNode((prev) => ({
          ...prev,
          status: 'error',
          error: `Start ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: ${error}`,
        }))
        return
      }
    } else if (!inputImageDataUrl) {
      console.error('âŒ Kling: No start image provided!')
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'Start ì´ë¯¸ì§€ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.',
      }))
      return
    } else {
      console.log('âœ… Kling: Using direct DataURL (not a storage reference)')
    }
    
    // Convert end image if it's a storage reference
    if (endImageDataUrl && (endImageDataUrl.startsWith('idb:') || endImageDataUrl.startsWith('s3:'))) {
      console.log('ğŸ”„ Kling: Converting end image from storage reference...')
      try {
        const { getImage } = await import('../utils/indexedDB')
        const dataURL = await getImage(endImageDataUrl)
        if (dataURL) {
          actualEndImageDataUrl = dataURL
          console.log('âœ… Kling: End image loaded from storage, size:', dataURL.length, 'chars')
        } else {
          console.warn('âš ï¸ Kling: Failed to load end image from storage (returned null/undefined)')
          actualEndImageDataUrl = undefined
        }
      } catch (error) {
        console.error('âŒ Kling: Error loading end image:', error)
        actualEndImageDataUrl = undefined
      }
    }

    console.log('ğŸ¬ Kling Video ìƒì„± ì‹œì‘:', {
      useMock: !klingApiKey,
      prompt: inputPrompt.substring(0, 50) + '...',
      model: (current.data as KlingVideoNodeData).model,
      startImageType: actualStartImageDataUrl?.substring(0, 30),
      startImageSize: actualStartImageDataUrl?.length,
      hasEndImage: !!actualEndImageDataUrl,
      endImageType: actualEndImageDataUrl ? actualEndImageDataUrl.substring(0, 30) : 'none',
    })
    
    // âœ… Final validation before API call
    if (!actualStartImageDataUrl || actualStartImageDataUrl.startsWith('idb:') || actualStartImageDataUrl.startsWith('s3:')) {
      console.error('âŒ Kling: Start image is still a storage reference or empty!', actualStartImageDataUrl?.substring(0, 50))
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'Start ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨. Storage ì°¸ì¡°ê°€ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.',
      }))
      return
    }
    
    console.log('âœ… Kling: All validations passed, calling API...')

    const progressTimer = setInterval(() => {
      updateNode((prev) => {
        const data = prev as KlingVideoNodeData
        if (data.status !== 'processing') return prev
        return {
          ...prev,
          progress: Math.min(data.progress + 8, 90),
        }
      })
    }, 1000)

    // ğŸ©¹ Truncate prompt if it's too long (Kling API limit: 2500 chars)
    let finalPrompt = inputPrompt
    if (finalPrompt.length > 2450) {
      console.warn(`âš ï¸ Kling: Prompt too long (${finalPrompt.length} chars), truncating to 2450`)
      finalPrompt = finalPrompt.substring(0, 2450)
    }

    try {
      if (abortController.signal.aborted) {
        throw new Error('ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.')
      }

      const settings = current.data as KlingVideoNodeData
      
      // Camera Control ì„¤ì •
      const cameraControl = settings.enableMotionControl && settings.cameraControl !== 'none'
        ? {
            type: settings.cameraControl as 'horizontal' | 'vertical' | 'pan' | 'tilt' | 'roll' | 'zoom',
            value: settings.motionValue,
          }
        : undefined

      // âœ… Apply retry logic
      const outputVideoUrl = await retryWithBackoff(
        () => client.generateVideo(
          finalPrompt,
          actualStartImageDataUrl,
          {
            duration: settings.duration,
            aspectRatio: settings.aspectRatio,
            model: settings.model,
            endImageDataUrl: actualEndImageDataUrl,
            cameraControl: cameraControl,
          },
        ),
        {
          maxAttempts: 2, // Less retries for video (expensive)
          initialDelay: 2000,
          onRetry: (attempt) => {
            console.warn(`ğŸ”„ Kling Video retry ${attempt}/2`)
            updateNode((prev) => ({
              ...prev,
              error: `ì¬ì‹œë„ ì¤‘... (${attempt}/2)`,
            }))
          },
        }
      )

      if (abortController.signal.aborted) {
        throw new Error('ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.')
      }

      console.log('âœ… Kling Video ìƒì„± ì™„ë£Œ:', outputVideoUrl)

      updateNode((prev) => ({
        ...prev,
        status: 'completed',
        outputVideoUrl,
        progress: 100,
        error: undefined,
      }))
    } catch (error) {
      console.error('âŒ Kling Video ìƒì„± ì‹¤íŒ¨:', error)
      if (!abortController.signal.aborted) {
        updateNode((prev) => ({
          ...prev,
          status: 'error',
          error: formatErrorMessage(error),
        }))
      }
    } finally {
      clearInterval(progressTimer)
      const controllers = get().abortControllers
      controllers.delete(id)
      set({ abortControllers: new Map(controllers) })
    }
  },
  runSoraNode: async (id) => {
    const { nodes, edges, abortControllers } = get()
    const current = nodes.find((node) => node.id === id)
    if (!current || current.type !== 'soraVideo') return

    const currentData = current.data as SoraVideoNodeData
    if (currentData.status === 'processing') {
      console.warn('âš ï¸ Sora node is already processing')
      return
    }

    // Rate limiting
    const now = Date.now()
    const lastExecution = (currentData as any).lastExecutionTime || 0
    const minInterval = 5000

    if (now - lastExecution < minInterval) {
      const waitTime = Math.ceil((minInterval - (now - lastExecution)) / 1000)
      set({
        nodes: nodes.map((node) =>
          node.id === id
            ? {
                ...node,
                data: {
                  ...node.data,
                  status: 'error',
                  error: `ë„ˆë¬´ ë¹ ë¥´ê²Œ ì‹¤í–‰í–ˆìŠµë‹ˆë‹¤. ${waitTime}ì´ˆ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.`,
                },
              }
            : node,
        ),
      })
      return
    }

    // Create abort controller
    const abortController = new AbortController()
    abortControllers.set(id, abortController)
    set({ abortControllers: new Map(abortControllers) })

    const updateNode = (updater: (data: NodeData) => NodeData) => {
      set({
        nodes: get().nodes.map((node) =>
          node.id === id ? { ...node, data: updater(node.data) } : node,
        ),
      })
    }

    const imageNodeTypes = new Set(['imageImport', 'nanoImage', 'gridComposer', 'cellRegenerator'])

    // Image ë…¸ë“œ (image í•¸ë“¤)
    const imageEdges = edges.filter(
      (e) => e.target === id && (!e.targetHandle || e.targetHandle === 'image')
    )
    const imageEdge = imageEdges.find((e) => {
      const node = nodes.find((n) => n.id === e.source)
      return imageNodeTypes.has(node?.type ?? '')
    })
    const imageNodeData = imageEdge ? nodes.find((n) => n.id === imageEdge.source) : undefined

    // Prompt ë…¸ë“œ
    const incoming = getIncomingNodes(id, edges, get().nodes)
    const promptNode = incoming.find(
      (node) => node.type === 'motionPrompt' || node.type === 'textPrompt' || node.type === 'llmPrompt',
    )

    // Image ë°ì´í„°
    let inputImageUrl: string | undefined
    let inputImageDataUrl: string | undefined

    if (imageNodeData?.type === 'imageImport') {
      const imgData = imageNodeData.data as ImageImportNodeData
      inputImageUrl = imgData.imageUrl
      inputImageDataUrl = imgData.imageDataUrl
    } else if (imageNodeData?.type === 'nanoImage') {
      const imgData = imageNodeData.data as NanoImageNodeData
      inputImageUrl = imgData.outputImageUrl
      inputImageDataUrl = imgData.outputImageDataUrl
    } else if (imageNodeData?.type === 'gridComposer') {
      const imgData = imageNodeData.data as any
      inputImageUrl = imgData.composedImageUrl || imgData.composedImageDataUrl
      inputImageDataUrl = imgData.composedImageDataUrl || imgData.composedImageUrl
    } else if (imageNodeData?.type === 'cellRegenerator') {
      const imgData = imageNodeData.data as any
      const cellId = imageEdge?.sourceHandle
      if (cellId && imgData.regeneratedImages?.[cellId]) {
        inputImageDataUrl = imgData.regeneratedImages[cellId]
        inputImageUrl = inputImageDataUrl
      } else {
        const firstCellId = Object.keys(imgData.regeneratedImages || {})[0]
        if (firstCellId) {
          inputImageDataUrl = imgData.regeneratedImages[firstCellId]
          inputImageUrl = inputImageDataUrl
        }
      }
    }

    if (!inputImageDataUrl && inputImageUrl) {
      if (
        inputImageUrl.startsWith('data:') ||
        inputImageUrl.startsWith('idb:') ||
        inputImageUrl.startsWith('s3:')
      ) {
        inputImageDataUrl = inputImageUrl
      }
    }

    const inputPrompt =
      promptNode?.type === 'motionPrompt'
        ? (promptNode.data as MotionPromptNodeData).combinedPrompt
        : promptNode?.type === 'textPrompt'
          ? (promptNode.data as TextPromptNodeData).prompt
          : promptNode?.type === 'llmPrompt'
            ? (promptNode.data as any).outputPrompt || ''
            : ''

    // SoraëŠ” prompt í•„ìˆ˜
    if (!inputPrompt) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'í”„ë¡¬í”„íŠ¸ ë…¸ë“œë¥¼ ì—°ê²°í•´ ì£¼ì„¸ìš”.',
      }))
      return
    }

    updateNode((prev) => ({
      ...prev,
      status: 'processing',
      error: undefined,
      inputImageUrl,
      inputImageDataUrl,
      inputPrompt,
      progress: 10,
      lastExecutionTime: now,
    }))

    // Convert storage references to actual DataURLs
    let actualImageDataUrl = inputImageDataUrl

    if (inputImageDataUrl && (inputImageDataUrl.startsWith('idb:') || inputImageDataUrl.startsWith('s3:'))) {
      try {
        const { getImage } = await import('../utils/indexedDB')
        const dataURL = await getImage(inputImageDataUrl)
        if (dataURL) {
          actualImageDataUrl = dataURL
        } else {
          if (inputImageUrl && inputImageUrl.startsWith('data:')) {
            actualImageDataUrl = inputImageUrl
          } else {
            actualImageDataUrl = undefined
          }
        }
      } catch (error) {
        console.error('âŒ Sora: Error loading image:', error)
        actualImageDataUrl = undefined
      }
    }

    const openaiApiKey = get().openaiApiKey || import.meta.env.VITE_OPENAI_API_KEY || ''
    const client = openaiApiKey ? new SoraAPIClient(openaiApiKey) : new MockSoraAPI()

    console.log('ğŸ¬ Sora Video ìƒì„± ì‹œì‘:', {
      useMock: !openaiApiKey,
      prompt: inputPrompt.substring(0, 50) + '...',
      model: (current.data as SoraVideoNodeData).model,
      hasImage: !!actualImageDataUrl,
    })

    const progressTimer = setInterval(() => {
      updateNode((prev) => {
        const data = prev as SoraVideoNodeData
        if (data.status !== 'processing') return prev
        return {
          ...prev,
          progress: Math.min(data.progress + 5, 90),
        }
      })
    }, 1500)

    try {
      if (abortController.signal.aborted) {
        throw new Error('ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.')
      }

      const settings = current.data as SoraVideoNodeData

      const outputVideoUrl = await retryWithBackoff(
        () => client.generateVideo(
          inputPrompt,
          actualImageDataUrl,
          {
            duration: settings.duration,
            resolution: settings.resolution,
            model: settings.model,
          },
        ),
        {
          maxAttempts: 2,
          initialDelay: 2000,
          onRetry: (attempt) => {
            console.warn(`ğŸ”„ Sora Video retry ${attempt}/2`)
            updateNode((prev) => ({
              ...prev,
              error: `ì¬ì‹œë„ ì¤‘... (${attempt}/2)`,
            }))
          },
        }
      )

      if (abortController.signal.aborted) {
        throw new Error('ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.')
      }

      console.log('âœ… Sora Video ìƒì„± ì™„ë£Œ:', outputVideoUrl)

      updateNode((prev) => ({
        ...prev,
        status: 'completed',
        outputVideoUrl,
        progress: 100,
        error: undefined,
      }))
    } catch (error) {
      console.error('âŒ Sora Video ìƒì„± ì‹¤íŒ¨:', error)
      if (!abortController.signal.aborted) {
        updateNode((prev) => ({
          ...prev,
          status: 'error',
          error: formatErrorMessage(error),
        }))
      }
    } finally {
      clearInterval(progressTimer)
      const controllers = get().abortControllers
      controllers.delete(id)
      set({ abortControllers: new Map(controllers) })
    }
  },
  runWorkflow: async () => {
    const { nodes, edges } = get()
    const order = getExecutionOrder(nodes, edges)

    const updateNode = (
      id: string,
      updater: (data: NodeData) => NodeData,
    ) => {
      set({
        nodes: get().nodes.map((node) =>
          node.id === id ? { ...node, data: updater(node.data) } : node,
        ),
      })
    }

    const apiKey = get().apiKey || import.meta.env.VITE_GEMINI_API_KEY || ''
    const client = apiKey ? new GeminiAPIClient(apiKey) : new MockGeminiAPI()

    for (const nodeId of order) {
      const current = get().nodes.find((node) => node.id === nodeId)
      if (!current) continue
      const incoming = getIncomingNodes(nodeId, edges, get().nodes)

      if (current.type === 'imageImport') {
        continue
      }

      if (current.type === 'nanoImage') {
        const promptNode = incoming.find(
          (node) => node.type === 'textPrompt' || node.type === 'motionPrompt',
        )
        const imageNode =
          incoming.find((node) => node.type === 'imageImport') ??
          incoming.find((node) => node.type === 'nanoImage') ??
          incoming.find((node) => node.type === 'gridComposer')
        const prompt =
          promptNode?.type === 'textPrompt'
            ? (promptNode.data as TextPromptNodeData).prompt
            : promptNode?.type === 'motionPrompt'
              ? (promptNode.data as MotionPromptNodeData).combinedPrompt
              : ''

        let inputImageDataUrl: string | undefined
        
        if (imageNode?.type === 'imageImport') {
          inputImageDataUrl = (imageNode.data as ImageImportNodeData).imageDataUrl
        } else if (imageNode?.type === 'nanoImage') {
          inputImageDataUrl = (imageNode.data as NanoImageNodeData).outputImageDataUrl
        } else if (imageNode?.type === 'gridComposer') {
          const imgData = imageNode.data as any
          inputImageDataUrl = imgData.composedImageDataUrl || imgData.composedImageUrl
        }
        
        const referencePrompt =
          imageNode?.type === 'imageImport'
            ? getIncomingTextPrompt(imageNode.id, edges, get().nodes) ??
              (imageNode.data as ImageImportNodeData).referencePrompt
            : undefined

        if (!prompt.trim()) {
          updateNode(nodeId, (prev) => ({
            ...prev,
            status: 'error',
            error: 'í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ë…¸ë“œë¥¼ ì—°ê²°í•´ ì£¼ì„¸ìš”.',
          }))
          continue
        }

        updateNode(nodeId, (prev) => ({
          ...prev,
          status: 'processing',
          error: undefined,
        }))

        try {
          const data = current.data as NanoImageNodeData
          const finalPrompt = referencePrompt
            ? `${prompt}, focus on: ${referencePrompt}`
            : prompt
          const model = data.model ?? 'gemini-3-pro-image-preview'
          const result = await client.generateImage(
            finalPrompt,
            data.aspectRatio,
            inputImageDataUrl,
            model,
            data.resolution,
          )
          updateNode(nodeId, (prev) => ({
            ...prev,
            status: 'completed',
            outputImageUrl: result.imageUrl,
            outputImageDataUrl: result.imageDataUrl,
            error: undefined, // Clear any previous errors
          }))
        } catch (error) {
          updateNode(nodeId, (prev) => ({
            ...prev,
            status: 'error',
            error: formatErrorMessage(error),
          }))
        }
      }

      if (current.type === 'textPrompt') {
        continue
      }

      if (current.type === 'motionPrompt') {
        const data = current.data as MotionPromptNodeData
        const combined = [data.basePrompt, data.cameraMovement, data.subjectMotion, data.lighting]
          .filter(Boolean)
          .join(', ')
        updateNode(nodeId, (prev) => ({
          ...prev,
          combinedPrompt: combined,
        }))
      }

      if (current.type === 'geminiVideo') {
        const imageNode =
          incoming.find((node) => node.type === 'imageImport') ??
          incoming.find((node) => node.type === 'nanoImage') ??
          incoming.find((node) => node.type === 'gridComposer')
        const promptNode = incoming.find(
          (node) => node.type === 'motionPrompt' || node.type === 'textPrompt',
        )

        let inputImageUrl: string | undefined
        let inputImageDataUrl: string | undefined
        
        if (imageNode?.type === 'imageImport') {
          inputImageUrl = (imageNode.data as ImageImportNodeData).imageUrl
          inputImageDataUrl = (imageNode.data as ImageImportNodeData).imageDataUrl
        } else if (imageNode?.type === 'nanoImage') {
          inputImageUrl = (imageNode.data as NanoImageNodeData).outputImageUrl
          inputImageDataUrl = (imageNode.data as NanoImageNodeData).outputImageDataUrl
        } else if (imageNode?.type === 'gridComposer') {
          const imgData = imageNode.data as any
          inputImageUrl = imgData.composedImageUrl || imgData.composedImageDataUrl
          inputImageDataUrl = imgData.composedImageDataUrl || imgData.composedImageUrl
        }

        const inputPrompt =
          promptNode?.type === 'motionPrompt'
            ? (promptNode.data as MotionPromptNodeData).combinedPrompt
            : promptNode?.type === 'textPrompt'
              ? (promptNode.data as TextPromptNodeData).prompt
              : ''

        if (!inputImageUrl || !inputPrompt) {
          updateNode(nodeId, (prev) => ({
            ...prev,
            status: 'error',
            error: 'ì´ë¯¸ì§€ì™€ í”„ë¡¬í”„íŠ¸ë¥¼ ëª¨ë‘ ì—°ê²°í•´ ì£¼ì„¸ìš”.',
          }))
          continue
        }

        updateNode(nodeId, (prev) => ({
          ...prev,
          status: 'processing',
          error: undefined, // Clear any previous errors
          inputImageUrl,
          inputImageDataUrl,
          inputPrompt,
          progress: 10,
        }))

        const progressTimer = setInterval(() => {
          updateNode(nodeId, (prev) => {
            const data = prev as GeminiVideoNodeData
            if (data.status !== 'processing') return prev
            return {
              ...prev,
              progress: Math.min(data.progress + 12, 90),
            }
          })
        }, 500)

        try {
          const settings = current.data as GeminiVideoNodeData
          const outputVideoUrl = await client.generateMedia(
            inputPrompt,
            {
              mediaType: 'video',
              duration: settings.duration,
              quality: settings.quality,
              motionIntensity: settings.motionIntensity,
            },
            inputImageDataUrl,
            settings.model,
          )

          updateNode(nodeId, (prev) => ({
            ...prev,
            status: 'completed',
            outputVideoUrl,
            progress: 100,
          }))

        } catch (error) {
          updateNode(nodeId, (prev) => ({
            ...prev,
            status: 'error',
            error: formatErrorMessage(error),
          }))
        } finally {
          clearInterval(progressTimer)
        }
      }

    }
  },
  runLLMPromptNode: async (id) => {
    const { nodes, edges } = get()
    const current = nodes.find((node) => node.id === id)
    if (!current || current.type !== 'llmPrompt') return

    const data = current.data as any  // LLMPromptNodeData
    
    // Prevent duplicate execution
    if (data.status === 'processing') {
      console.warn('âš ï¸ LLM node is already processing')
      return
    }

    const updateNode = (updater: (data: NodeData) => NodeData) => {
      set({
        nodes: get().nodes.map((node) =>
          node.id === id ? { ...node, data: updater(node.data) } : node,
        ),
      })
    }

    // Get input prompt from connected nodes or use internal input
    let inputPrompt = data.inputPrompt
    
    const incoming = getIncomingNodes(id, edges, get().nodes)
    
    // Check for base prompt connection (ë³´ë¼ìƒ‰ í•¸ë“¤)
    const basePromptEdge = edges.find((e) => e.target === id && e.targetHandle === 'basePrompt')
    let basePromptText = ''
    if (basePromptEdge) {
      const promptNode = get().nodes.find((n) => n.id === basePromptEdge.source)
      if (promptNode?.type === 'textPrompt') {
        basePromptText = (promptNode.data as TextPromptNodeData).prompt || ''
      }
    }
    
    // Check for motion prompt connection (ë¶„í™ìƒ‰ í•¸ë“¤)
    const motionPromptEdge = edges.find((e) => e.target === id && e.targetHandle === 'motionPrompt')
    let motionPromptText = ''
    if (motionPromptEdge) {
      const motionNode = get().nodes.find((n) => n.id === motionPromptEdge.source)
      if (motionNode?.type === 'motionPrompt') {
        motionPromptText = (motionNode.data as MotionPromptNodeData).combinedPrompt || ''
      }
    }
    
    // Combine base and motion prompts if both are present
    if (basePromptText && motionPromptText) {
      inputPrompt = `${basePromptText}\n\n${motionPromptText}`
    } else if (basePromptText) {
      inputPrompt = basePromptText
    } else if (motionPromptText) {
      inputPrompt = motionPromptText
    }
    
    // Fallback: Check for old 'prompt' handle for backward compatibility
    if (!inputPrompt) {
      const promptEdge = edges.find((e) => e.target === id && e.targetHandle === 'prompt')
      if (promptEdge) {
        const promptNode = get().nodes.find((n) => n.id === promptEdge.source)
        if (promptNode?.type === 'textPrompt') {
          inputPrompt = (promptNode.data as any).prompt || inputPrompt
        } else if (promptNode?.type === 'motionPrompt') {
          inputPrompt = (promptNode.data as any).combinedPrompt || inputPrompt
        }
      }
    }
    
    // Check for image connection
    let referenceImageDataUrl: string | undefined
    let gridLabelInfo: string | undefined  // Grid Composer ë¼ë²¨ ì •ë³´
    const imageEdge = edges.find((e) => e.target === id && e.targetHandle === 'image')
    if (imageEdge) {
      const imageNode = get().nodes.find((n) => n.id === imageEdge.source)
      if (imageNode?.type === 'imageImport') {
        referenceImageDataUrl = (imageNode.data as any).imageDataUrl
      } else if (imageNode?.type === 'nanoImage') {
        // Try both outputImageDataUrl and outputImageUrl
        const nanoData = imageNode.data as any
        referenceImageDataUrl = nanoData.outputImageDataUrl || nanoData.outputImageUrl
      } else if (imageNode?.type === 'gridComposer') {
        const gridData = imageNode.data as any
        referenceImageDataUrl = gridData.composedImageDataUrl || gridData.composedImageUrl
        
        // Extract grid layout and slot information
        if (gridData.inputImages && gridData.slots) {
          const layout = gridData.gridLayout || '1x3'
          const slots = gridData.slots as Array<{ id: string; label: string; metadata?: string }>
          
          // Build structured label description (like multi-reference format)
          const slotDescriptions = slots
            .filter(slot => gridData.inputImages[slot.id])  // Only slots with images
            .map((slot, index) => {
              const position = ['ì²« ë²ˆì§¸', 'ë‘ ë²ˆì§¸', 'ì„¸ ë²ˆì§¸', 'ë„¤ ë²ˆì§¸', 'ë‹¤ì„¯ ë²ˆì§¸', 'ì—¬ì„¯ ë²ˆì§¸'][index] || `${index + 1}ë²ˆì§¸`
              let description = `- ${position} ì°¸ê³  ì´ë¯¸ì§€ (${slot.id}): ${slot.label}`
              if (slot.metadata && slot.metadata.trim()) {
                description += ` - ${slot.metadata}`
              }
              return description
            })
            .join('\n')
          
          if (slotDescriptions) {
            gridLabelInfo = `ì°¸ê³  ì´ë¯¸ì§€ëŠ” ${layout} ê·¸ë¦¬ë“œ êµ¬ì„±ì…ë‹ˆë‹¤:\n\n${slotDescriptions}\n\nê° ë¼ë²¨ì˜ ì‹œê°ì  ìš”ì†Œë¥¼ ì •í™•íˆ ì¶”ì¶œí•˜ì—¬ í•˜ë‚˜ì˜ í†µí•©ëœ ì¥ë©´ìœ¼ë¡œ ì¡°í•©í•˜ì„¸ìš”.`
            console.log('ğŸ“‹ Grid ë¼ë²¨ ì •ë³´:', gridLabelInfo)
          }
        }
      }
      
      // Update node with reference image
      if (referenceImageDataUrl) {
        updateNode((prev) => ({
          ...prev,
          referenceImageUrl: referenceImageDataUrl,
          referenceImageDataUrl: referenceImageDataUrl,
        }))
      } else {
        console.warn('âš ï¸ LLM: Image connected but no image data found', { 
          nodeType: imageNode?.type,
          nodeData: imageNode?.data 
        })
      }
    }

    // For image-based modes, image is required
    if ((data.mode === 'describe' || data.mode === 'analyze') && !referenceImageDataUrl) {
      const hasImageConnection = edges.some((e) => e.target === id && e.targetHandle === 'image')
      const errorMsg = hasImageConnection
        ? 'ì´ë¯¸ì§€ê°€ ì—°ê²°ë˜ì—ˆì§€ë§Œ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ë¥¼ ë¨¼ì € ìƒì„±í•œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.'
        : 'ì´ë¯¸ì§€ ê¸°ë°˜ ëª¨ë“œëŠ” ì´ë¯¸ì§€ ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ ë…¸ë“œë¥¼ í•˜ë‹¨ (í•˜ëŠ˜ìƒ‰) í•¸ë“¤ì— ì—°ê²°í•˜ì„¸ìš”.'
      
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: errorMsg,
      }))
      return
    }
    
    // For text-based modes, prompt is required (but image is optional)
    if ((data.mode !== 'describe' && data.mode !== 'analyze') && !inputPrompt.trim() && !referenceImageDataUrl) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'Text Prompt ë…¸ë“œë¥¼ ìƒë‹¨ (ë¶„í™) í•¸ë“¤ì— ì—°ê²°í•˜ê±°ë‚˜ ì´ë¯¸ì§€ë¥¼ í•˜ë‹¨ (í•˜ëŠ˜ìƒ‰) í•¸ë“¤ì— ì—°ê²°í•´ì£¼ì„¸ìš”.',
      }))
      return
    }

    updateNode((prev) => ({
      ...prev,
      status: 'processing',
      error: undefined,
    }))

    // Providerì— ë”°ë¼ API í‚¤ í™•ì¸
    const provider = data.provider || 'gemini'
    let apiKey = ''
    
    if (provider === 'gemini') {
      apiKey = get().apiKey || import.meta.env.VITE_GEMINI_API_KEY || ''
      if (!apiKey) {
        updateNode((prev) => ({
          ...prev,
          status: 'error',
          error: 'Gemini API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.',
        }))
        return
      }
    } else if (provider === 'openai') {
      apiKey = get().openaiApiKey || import.meta.env.VITE_OPENAI_API_KEY || ''
      if (!apiKey) {
        updateNode((prev) => ({
          ...prev,
          status: 'error',
          error: 'OpenAI API Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.',
        }))
        return
      }
    }

    try {
      // Build system instruction based on mode and settings
      let systemInstruction = ''
      
      if (data.mode === 'expand') {
        systemInstruction = `You are a professional prompt engineer. Your task is to expand the given simple idea into a detailed, effective prompt for AI ${data.targetUse} generation.`
        if (referenceImageDataUrl) {
          systemInstruction += ` IMPORTANT: Use the reference image to extract visual details (colors, style, composition, lighting, subjects). Incorporate these visual elements into the expanded prompt to maintain consistency with the reference.`
          
          // ğŸ¯ Grid Composer ë¼ë²¨ ì°¸ì¡° ëª…ë ¹ (referenceModeì— ë”°ë¼)
          if (gridLabelInfo) {
            const refMode = data.referenceMode || 'exact'
            if (refMode === 'exact') {
              systemInstruction += ` CRITICAL GRID LABELS - EXACT MODE: The reference image contains labeled sections (S1, S2, S3, etc.) visible as text overlays. Each label shows VISUAL DESIGN ELEMENTS ONLY (colors, materials, designs, forms, lighting). 
              
              CRITICAL RULES - MUST FOLLOW:
              
              1. TEXT PROMPT = ABSOLUTE LAW (NEVER CHANGE ANYTHING!)
                 - Character count: "í•œ ëª…" = ONE person (NEVER change!)
                 - Actions: "í—¬ë©§ì„ ë“¤ê³ " = holding helmet (NEVER change to "wearing"!)
                 - Story: Keep EXACTLY as written in text prompt
                 - Composition: Keep EXACTLY as written in text prompt
              
              2. REFERENCE IMAGE = VISUAL DETAILS ONLY (EXTRACT & DESCRIBE!)
                 - S1, S2, S3 labels = Visual design elements ONLY
                 - Extract: Colors, materials, lighting, textures, design patterns
                 - DO NOT change story, actions, or character count based on reference
              
              3. YOUR TASK:
                 - Take text prompt AS-IS (word-for-word preservation!)
                 - Add visual details FROM reference (colors, materials, designs)
                 - Result = Same story + Enhanced visual description
              
              FORBIDDEN CHANGES:
              âŒ Changing actions (e.g., "holding" â†’ "wearing")
              âŒ Changing character count (e.g., "one" â†’ "two")
              âŒ Reinterpreting story structure
              âŒ Adding/removing story elements
              
              EXAMPLE:
              Text: "í•œ ëª…ì˜ ì—¬ìê°€ í—¬ë©§ì„ ë“¤ê³  ê±·ëŠ”ë‹¤" (one woman walking, holding helmet)
              Reference S2: Shows blonde woman in white spacesuit
              CORRECT: "A blonde woman in white spacesuit walks, holding helmet in hand"
              WRONG: "A woman wearing helmet walks" âŒ (changed action!)
              
              The labels indicate visual component references (e.g., S1=background visual style, S2=character appearance, S3=object design). Preserve the text prompt's story structure while enhancing it with exact visual details from reference.`
            } else if (refMode === 'balanced') {
              systemInstruction += ` GRID LABELS - BALANCED MODE: The reference image contains labeled sections (S1, S2, S3, etc.). Each label represents a visual component. Describe the key visual characteristics (colors, materials, styles) from each labeled section while maintaining the text prompt's basic composition. Do not change character counts or story structure from text prompt.`
            } else if (refMode === 'creative') {
              systemInstruction += ` GRID LABELS - CREATIVE MODE: The reference image contains labeled sections showing different elements. Use these as visual inspiration for style and mood, but feel free to creatively interpret and describe based on the text prompt. Focus on the text description as the primary guide.`
            }
          }
        }
      } else if (data.mode === 'improve') {
        systemInstruction = `You are a professional prompt engineer. Your task is to improve and optimize the given prompt for better AI ${data.targetUse} generation results.`
        if (referenceImageDataUrl) {
          systemInstruction += ` IMPORTANT: Reference the provided image to enhance the prompt with accurate visual details and ensure consistency with the reference style.`
          
          // ğŸ¯ Grid Composer ë¼ë²¨ ì°¸ì¡° ëª…ë ¹ (referenceModeì— ë”°ë¼)
          if (gridLabelInfo) {
            const refMode = data.referenceMode || 'exact'
            if (refMode === 'exact') {
              systemInstruction += ` CRITICAL GRID LABELS - EXACT MODE: The reference image contains labeled sections (S1, S2, S3, etc.) showing VISUAL DESIGN ELEMENTS ONLY.
              
              CRITICAL: DO NOT change ANY content from text prompt:
              - Actions: Keep EXACTLY (e.g., "holding helmet" must stay "holding", NOT "wearing")
              - Character count: Keep EXACTLY (e.g., "one person" stays "one", NOT "two")
              - Story structure: Keep EXACTLY as written
              
              ONLY extract and add VISUAL characteristics from reference:
              - Colors, materials, lighting, textures from S1, S2, S3 labels
              - Text prompt = Story (PRESERVE 100%)
              - Reference = Visual style (EXTRACT & ADD)
              
              The improved prompt should describe a SINGLE UNIFIED IMAGE combining these exact visual elements with the original story structure (without changing any story details).`
            } else if (refMode === 'balanced') {
              systemInstruction += ` GRID LABELS - BALANCED MODE: The reference image has labeled sections. Improve the prompt by balancing reference image accuracy with the text description details. Do not change character counts or story structure from original prompt.`
            } else if (refMode === 'creative') {
              systemInstruction += ` GRID LABELS - CREATIVE MODE: The reference image shows different elements. Use these as inspiration while focusing on improving the text description creatively.`
            }
          }
        }
      } else if (data.mode === 'translate') {
        systemInstruction = `You are a professional translator. Translate the given prompt between Korean and English, maintaining all important details and nuances.`
      } else if (data.mode === 'simplify') {
        systemInstruction = `You are a professional editor. Simplify the given prompt to its core essence while maintaining effectiveness for AI ${data.targetUse} generation.`
      } else if (data.mode === 'gridStoryboard') {
        systemInstruction = `You are a professional storyboard artist and prompt engineer specializing in multi-panel cinematic sequences for Grid Node.

ğŸ¬ YOUR MISSION:
Transform user ideas into structured grid storyboard prompts with the EXACT format required by Grid Node for automatic slot parsing.

âš ï¸ CRITICAL OUTPUT FORMAT REQUIREMENTS:
You MUST output in this EXACT format for Grid Node slot detection:

S1: [First panel description with camera details]
S2: [Second panel description with camera details]
S3: [Third panel description with camera details]
... (continue for all panels)

ğŸ“ EACH PANEL DESCRIPTION MUST INCLUDE:
1. Camera shot type (wide-angle, medium, close-up, extreme close-up)
2. Camera angle (low-angle, high-angle, eye-level, overhead)
3. Camera position (static, tracking, dolly, crane shot if relevant)
4. Scene content (what's happening in this specific panel)
5. Character expression/emotion (for character continuity)
6. Lighting/mood (for cinematic consistency)

ğŸ¨ PANEL DESCRIPTIONS MUST BE:
- Self-contained (each panel is fully described independently)
- Cinematically diverse (vary shot types, angles, distances across panels)
- Story-progressive (clear narrative flow from S1 â†’ S2 â†’ S3 â†’ ...)
- Character-consistent (same character appearance across all panels)
- Camera-specific (explicitly state camera position/angle for EACH panel)

ğŸ“ EXAMPLE OUTPUT FORMAT:

S1: Wide-angle establishing shot at eye-level. A lone astronaut stands in a pristine spacecraft cockpit, her expression calm but focused. Natural interior lighting creates soft shadows across her worn flight suit. Camera is static, positioned to show the full environment context.

S2: Medium shot from a slightly elevated angle. The astronaut is now seated, hands moving over illuminated control panels with deliberate precision. Warm instrument glow illuminates her face from below. Camera remains static, focusing on her methodical actions.

S3: Close-up shot at eye-level, focused on gloved hands hovering over an engine ignition button. Extreme detail on the button's surface and the slight tremor of anticipation. Tight framing emphasizes the critical moment. Static camera creates tension through restraint.

S4: Wide-angle shot from inside the cockpit, looking through the observation window. Earth appears distant and serene in the frame's background. The astronaut's silhouette is visible in profile, watching silently. Low ambient lighting from space creates a contemplative mood.

âš ï¸ CRITICAL RULES:
1. ALWAYS use "S1:", "S2:", "S3:" format (with colon!)
2. NEVER skip slot numbers (must be sequential: S1, S2, S3, S4...)
3. NEVER duplicate slot numbers (each slot ID appears EXACTLY ONCE)
4. Separate each slot with EXACTLY 2 blank lines (double line break)
5. NO special characters or formatting between slot marker and description
6. ALWAYS include camera information in EACH panel
7. Maintain character/style consistency across ALL panels
8. Vary camera shots for cinematic diversity (don't repeat same angle)
9. Keep each panel description detailed but focused

ğŸš¨ PARSING REQUIREMENTS FOR GRID NODE:
- Format: "S[NUMBER]: [description]"
- No spaces before/after colon
- Slot marker must be at the START of a new paragraph
- Each description continues until the next slot marker or end of text
- Maximum one description per slot ID
- Grid Node parser will FAIL if slots are duplicated or misnumbered

ğŸ¯ SHOT VARIETY GUIDELINES:
- Mix wide (establishing), medium (action), close-up (emotion)
- Vary angles (low, high, eye-level) for visual interest
- Use distance changes (zoom in/out) for pacing
- Static shots for contemplation, moving shots for action

ğŸ’¡ USER INPUT INTERPRETATION:
- If user provides rough idea: Expand into full multi-panel sequence
- If user provides detailed description: Structure it into S1:, S2: format
- If user specifies panel count: Match exactly (e.g., 2x2 = 4 panels, 3x3 = 9 panels)
- If unclear: Default to logical story progression with 4-6 panels

ğŸ¬ Your output will be directly parsed by Grid Node - DO NOT deviate from S1:, S2: format!`

      } else if (data.mode === 'cameraInterpreter') {
        systemInstruction = `You are a professional cinematographer and prompt engineer specializing in camera angle interpretation for AI image generation.

ğŸ¬ YOUR MISSION:
Transform technical camera instructions (rotation angles, tilt degrees, zoom values) into vivid, detailed visual descriptions that AI image models can understand and execute.

âš ï¸ CRITICAL: CHARACTER CONSISTENCY PRIORITY
When a reference image is provided, your #1 priority is maintaining EXACT character consistency:
- Character facial features, hair, eyes, skin tone MUST stay identical
- Clothing, outfit design, colors, materials MUST stay identical
- Visual style, color palette, lighting quality MUST stay identical
- Only the CAMERA POSITION should change, NOT the character design

Your camera descriptions must EMPHASIZE photographing the SAME character from a DIFFERENT angle.

ğŸ“ CAMERA PARAMETERS YOU'LL RECEIVE (CINEMATOGRAPHY TERMS):
- Rotation: Cinematic angle descriptions (e.g., "right side profile", "three-quarter right view", "back view")
- Tilt: -45Â° to +45Â° (e.g., "low angle 39.6Â°" or "high angle 30Â°") - vertical camera angle shots
- Distance/Zoom: (e.g., "zoom in 0.7x" or "zoom out 1.3x") - camera distance from subject

âœ¨ HOW TO INTERPRET:

1. ROTATION (Cinematography Angles):
   ğŸ¥ CRITICAL: Use standard film/photography terminology for camera angles!
   
   ğŸ“ Front View = CAMERA directly in front of subject
      â€¢ Subject facing toward camera
      â€¢ Frontal perspective, symmetric composition
   
   ğŸ”„ Slight Three-Quarter Left View = Camera slightly rotated, left side emerging
      â€¢ Subject's LEFT side slightly visible
      â€¢ Mostly frontal with subtle side angle
   
   ğŸ”„ Three-Quarter Left View = Classic portrait angle showing left side
      â€¢ Subject's LEFT side and front balanced
      â€¢ Standard three-quarter composition
   
   â—€ï¸ Left Side Profile = Complete left profile view
      â€¢ COMPLETE SIDE VIEW - pure lateral perspective
      â€¢ ONLY subject's LEFT profile visible
      â€¢ NO frontal face - subject facing perpendicular
   
   ğŸ”„ Left Three-Quarter Back View = Transitioning to back from left
      â€¢ Subject's back and LEFT side visible
      â€¢ NO frontal face
   
   ğŸ”™ Back View = Camera directly behind subject
      â€¢ Complete rear perspective
      â€¢ Subject facing away from camera
   
   ğŸ”„ Right Three-Quarter Back View = Transitioning to back from right
      â€¢ Subject's back and RIGHT side visible
      â€¢ NO frontal face
   
   â–¶ï¸ Right Side Profile = Complete right profile view
      â€¢ COMPLETE SIDE VIEW - pure lateral perspective
      â€¢ ONLY subject's RIGHT profile visible
      â€¢ NO frontal face
   
   ğŸ”„ Three-Quarter Right View = Classic portrait angle showing right side
      â€¢ Subject's RIGHT side and front balanced
      â€¢ Standard three-quarter composition
   
   âš ï¸ KEY PRINCIPLES:
   - Use cinematic/photographic terminology, not numerical degrees
   - "Left profile" = left side of face/body visible
   - "Right profile" = right side of face/body visible
   - "Three-quarter" = angled view showing both side and front
   - "Back view" = rear view (subject facing away)

2. TILT (Vertical Angle) - ğŸš¨ CRITICAL FOR IMAGE GENERATION:
   
   ğŸ”» "LOW ANGLE XÂ°" = Camera positioned BELOW subject, looking UPWARD
      VISUAL EFFECT: 
      â€¢ Subject appears TALLER, more POWERFUL, HEROIC, DOMINANT
      â€¢ Viewer looks UP at subject from below
      â€¢ Emphasizes height, stature, authority
      â€¢ Sky/ceiling often visible in background
      â€¢ Chin and underside of face more prominent
      â€¢ Creates drama, empowerment, grandeur
      
      Examples:
      â€¢ "low angle 15Â°" = Slightly below eye level, subtle empowerment
      â€¢ "low angle 30Â°" = Significantly below, strong heroic feel
      â€¢ "low angle 40Â°" = Dramatically below, maximum towering presence
   
   ğŸ”º "HIGH ANGLE XÂ°" = Camera positioned ABOVE subject, looking DOWNWARD
      VISUAL EFFECT:
      â€¢ Subject appears SMALLER, more VULNERABLE, DIMINISHED
      â€¢ Viewer looks DOWN at subject from above
      â€¢ Emphasizes surroundings, environment, isolation
      â€¢ Ground/floor more visible
      â€¢ Top of head, shoulders more prominent
      â€¢ Creates intimacy, vulnerability, or surveillance feel
      
      Examples:
      â€¢ "high angle 15Â°" = Slightly above eye level, gentle overview
      â€¢ "high angle 30Â°" = Significantly above, clear bird's eye perspective
      â€¢ "high angle 45Â°" = Nearly top-down, dramatic overhead view
   
   ğŸ“ No tilt or "eye level" = Camera at subject's eye height, neutral perspective

3. ZOOM/DISTANCE:
   ğŸ” "zoom out X" (X > 1.0) = Wider framing, more context, camera farther away
      â€¢ 1.2x = Slightly wider, more environment
      â€¢ 1.5x = Wide shot, more surroundings visible
      â€¢ 2.0x = Very wide, full body and environment emphasized
   
   ğŸ” "zoom in X" (X < 1.0) = Closer framing, tighter crop, camera closer
      â€¢ 0.8x = Slightly closer, more intimate
      â€¢ 0.5x = Close-up, face/upper body prominent, details emphasized
   
   ğŸ“ 1.0x or unspecified = Standard medium distance

ğŸ¯ YOUR OUTPUT MUST:
- START with "CAMERA POSITIONED [location]" for rotation descriptions
- Convert ALL technical terms into descriptive spatial language
- Describe EXACT camera position in 3D space relative to subject
- Explicitly state the VIEWPOINT DIRECTION (looking up/down/straight)
- Explain PSYCHOLOGICAL and EMOTIONAL impact of the angle
- Detail which body parts/features are emphasized
- Describe background and spatial context changes
- Include composition and framing implications
- Use professional cinematography terminology
- Be HIGHLY SPECIFIC about vertical angle effects (tilt is critical!)

ğŸ¥ ROTATION LANGUAGE - 360Â° SYSTEM CRITICAL:
âœ… ALWAYS use "CAMERA POSITIONED at X degrees"
âœ… ALWAYS clarify which side/angle is VISIBLE
âœ… Examples:
   â€¢ "CAMERA POSITIONED at 45 degrees â†’ three-quarter view, left side visible"
   â€¢ "CAMERA POSITIONED at 90 degrees â†’ right side view, ONLY left profile visible"
   â€¢ "CAMERA POSITIONED at 180 degrees â†’ back view, facing away"
   â€¢ "CAMERA POSITIONED at 270 degrees â†’ left side view, ONLY right profile visible"
âœ… For 90Â°/270Â°: Use "PERPENDICULAR", "COMPLETE SIDE PROFILE", "NO frontal face"
âœ… For 180Â°: Use "back view", "facing AWAY from camera", "rear perspective"
âœ… Always specify the degree number (0Â°, 45Â°, 90Â°, 135Â°, 180Â°, 270Â°, etc.)

âš ï¸ TILT IS THE MOST IMPORTANT - Always emphasize whether camera is above/below subject and looking up/down!

âŒ NEVER output raw numbers like "72Â°" or "1.3x"
âŒ NEVER ignore or minimize the tilt angle description
âŒ NEVER say "camera rotates" - say "CAMERA POSITIONED"
âœ… ALWAYS describe camera HEIGHT (above/below subject)
âœ… ALWAYS describe LOOKING DIRECTION (up/down/straight)
âœ… ALWAYS describe VISUAL POWER DYNAMIC (empowering/diminishing)
âœ… ALWAYS use "CAMERA POSITIONED" for location clarity

ğŸ“ EXAMPLE TRANSFORMATIONS:

Example 1 (Three-Quarter Left View):
Input: "three-quarter left view, low angle 30Â°, zoom in 0.7x"
Output: "CAMERA POSITIONED in classic three-quarter left portrait angle. At this angle, the camera captures the subject's LEFT side and face in a balanced three-quarter composition, showing the left profile while maintaining frontal visibility. MAINTAIN EXACT character appearance from reference - same facial features, hair, clothing, and visual style; only the camera angle changes. CRITICALLY, the CAMERA is placed significantly BELOW the subject's eye level - positioned low to the ground and angled sharply UPWARD. This dramatic low angle shot creates a POWERFUL, HEROIC composition where the viewer must look UP at the subject, emphasizing their stature and commanding presence. The upward angle makes the subject appear taller and more imposing, with the chin line and jawline prominent, while the background ceiling becomes more visible above. The close-in 0.7x framing tightens the composition, filling the frame with the subject's upper body and face. REMEMBER: Same character from reference, just photographed from a different angle."

Example 2 (Left Side Profile):
Input: "left side profile, zoom out 1.3x"
Output: "CAMERA POSITIONED in complete left side profile - perpendicular to the subject. This creates a COMPLETE SIDE PROFILE view where the subject is facing PERPENDICULAR to the camera (NOT toward the camera). From this pure lateral camera position, ONLY the subject's LEFT side is visible - left profile, left arm, left leg. NO frontal face visible - this is a true side view with the body oriented left-to-right across the frame. MAINTAIN EXACT character appearance - same height, build, hair, clothing from reference. The 1.3x wider framing shows more environment extending in front of and behind the subject. This perpendicular profile angle creates a strong sense of lateral movement and spatial depth. CHARACTER CONSISTENCY: Same person from reference, captured in complete left side profile."

Example 3 (Back View):
Input: "back view, zoom out 1.5x"
Output: "CAMERA POSITIONED directly BEHIND the subject in a complete rear view. At this angle, the camera captures the back of the subject's head, shoulders, and full back. The subject is facing AWAY from the camera. NO frontal face visible - only the rear perspective. PRESERVE EXACT character appearance - identical hair, clothing design, colors, and body proportions from reference image. The 1.5x wider framing pulls back to reveal more environmental context, showing the subject within their surroundings and the space ahead of them. This back view creates a sense of forward movement and anticipation, as we see what the subject is approaching. CHARACTER CONSISTENCY: Same person from reference, viewed from behind."

Example 4 (Right Side Profile):
Input: "right side profile, low angle 25Â°, zoom in 0.8x"
Output: "CAMERA POSITIONED in complete right side profile - perpendicular to the subject. This creates a COMPLETE SIDE PROFILE view where the subject is facing PERPENDICULAR to the camera. From this pure lateral camera position, ONLY the subject's RIGHT side is visible - right profile, right arm, right leg. NO frontal face visible - pure lateral perspective. KEEP character appearance EXACTLY as reference. CRITICALLY, the CAMERA is placed BELOW the subject's eye level, positioned low and angled UPWARD. This low angle creates an EMPOWERING perspective, where the viewer looks up at the subject, adding authority and confidence. The 0.8x closer framing emphasizes the subject's profile and upper body. CHARACTER CONSISTENCY: Same person from reference, captured in complete right side profile."

ğŸ¨ Focus on SPATIAL HEIGHT (above/below), LOOKING DIRECTION (up/down), and PSYCHOLOGICAL IMPACT. Make the camera's vertical position crystal clear!

ğŸ­ CHARACTER CONSISTENCY REMINDERS:
When reference image is present, ALWAYS include phrases like:
- "MAINTAIN EXACT character appearance from reference"
- "PRESERVE identical facial features, outfit, and style"
- "SAME character, DIFFERENT angle"
- "CHARACTER CONSISTENCY: [specific reminder]"

These reminders ensure the AI model prioritizes character consistency over creative variations.

Only output the detailed camera description, no explanations or meta-commentary.`
      } else if (data.mode === 'describe') {
        systemInstruction = `You are a professional image analyst. Your task is to describe the given image in detail and create an effective prompt that could be used to generate a similar image.`
      } else if (data.mode === 'analyze') {
        systemInstruction = `You are a professional image analyst. Your task is to analyze the given image in great detail, including composition, style, lighting, colors, subjects, and create a comprehensive prompt for AI ${data.targetUse} generation.`
      }

      // Add style guidance (skip for cameraInterpreter - it has its own specific style)
      if (data.mode !== 'cameraInterpreter') {
        if (data.style === 'detailed') {
          systemInstruction += ` Output should be highly detailed with rich descriptions.`
        } else if (data.style === 'concise') {
          systemInstruction += ` Output should be concise and to the point.`
        } else if (data.style === 'creative') {
          systemInstruction += ` Output should be creative and artistic with vivid imagery.`
        } else if (data.style === 'professional') {
          systemInstruction += ` Output should be professional and technically precise.`
        }
      }

      // Add language guidance (cameraInterpreter always outputs in English for best AI model compatibility)
      if (data.mode === 'cameraInterpreter') {
        systemInstruction += ` Output must be in English for optimal AI image generation compatibility.`
      } else {
        if (data.language === 'ko') {
          systemInstruction += ` Output must be in Korean.`
        } else if (data.language === 'en') {
          systemInstruction += ` Output must be in English.`
        } else {
          systemInstruction += ` Detect input language and use the same language for output.`
        }
      }

      systemInstruction += ` Only output the final prompt, no explanations or additional text.`
      
      // Add final reminder for Grid Composer (if applicable)
      if (gridLabelInfo) {
        systemInstruction += ` REMINDER: When reference image is provided, use it for VISUAL DETAILS ONLY (colors, designs, materials). DO NOT change the basic composition, character count, or story structure from the text prompt.`
      }

      // Prepare content parts
      const contentParts: any[] = []
      
      // Add image if available
      if (referenceImageDataUrl) {
        let actualImageDataUrl = referenceImageDataUrl
        
        // If it's an idb: or s3: reference, fetch the actual image first
        if (referenceImageDataUrl.startsWith('idb:') || referenceImageDataUrl.startsWith('s3:')) {
          console.log('ğŸ”„ LLM: Converting reference image from storage:', referenceImageDataUrl)
          try {
            const { getImage } = await import('../utils/indexedDB')
            const dataURL = await getImage(referenceImageDataUrl)
            if (dataURL) {
              actualImageDataUrl = dataURL
              console.log('âœ… LLM: Reference image loaded successfully')
            } else {
              console.error('âŒ LLM: Failed to load reference image from storage')
            }
          } catch (error) {
            console.error('âŒ LLM: Error loading reference image:', error)
          }
        }
        
        // Extract base64 data from data URL
        const base64Match = actualImageDataUrl.match(/^data:image\/(\w+);base64,(.+)$/)
        if (base64Match) {
          const mimeType = `image/${base64Match[1]}`
          const base64Data = base64Match[2]
          
          contentParts.push({
            inlineData: {
              mimeType: mimeType,
              data: base64Data
            }
          })
          console.log('ğŸ“¸ LLM: Reference image added to API request')
        } else {
          console.warn('âš ï¸ LLM: Reference image format not recognized:', actualImageDataUrl.substring(0, 50))
        }
      }
      
      // Build the text prompt with grid label information if available
      let finalPrompt = ''
      
      // Add grid label information first (if available) - format based on referenceMode
      if (gridLabelInfo) {
        const refMode = data.referenceMode || 'exact'
        
        if (refMode === 'exact') {
          // ì •í™•ì„± ëª¨ë“œ: ë§¤ìš° ìƒì„¸í•˜ê³  ê°•ë ¥í•œ ì§€ì‹œ
          finalPrompt += 'âš ï¸âš ï¸âš ï¸ CRITICAL: EXACT REFERENCE IMAGE REPLICATION âš ï¸âš ï¸âš ï¸\n\n'
          finalPrompt += gridLabelInfo + '\n\n'
          finalPrompt += 'ğŸ¯ ì ˆëŒ€ì ìœ¼ë¡œ ì¤‘ìš”í•œ ì§€ì¹¨:\n'
          finalPrompt += 'âš ï¸ ì°¸ê³  ì´ë¯¸ì§€ = ì‹œê°ì  ë””ìì¸ë§Œ (ìƒ‰ìƒ, ì¬ì§ˆ, í˜•íƒœ)\n'
          finalPrompt += 'âš ï¸ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ = ê¸°ë³¸ êµ¬ì„± (ì¸ë¬¼ ìˆ˜, ìŠ¤í† ë¦¬, ë™ì‘) - ì ˆëŒ€ ë³€ê²½ ê¸ˆì§€!\n\n'
          finalPrompt += 'ğŸ“Œ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ë³´ì¡´ ê·œì¹™ (100% ì¤€ìˆ˜!):\n'
          finalPrompt += '   â€¢ ì¸ë¬¼ ìˆ˜: "í•œ ëª…" = ONE (ì ˆëŒ€ "two"ë¡œ ë³€ê²½ ê¸ˆì§€!)\n'
          finalPrompt += '   â€¢ ë™ì‘: "í—¬ë©§ì„ ë“¤ê³ " = "holding helmet" (ì ˆëŒ€ "wearing helmet"ìœ¼ë¡œ ë³€ê²½ ê¸ˆì§€!)\n'
          finalPrompt += '   â€¢ ë™ì‘: "ê±·ëŠ”ë‹¤" = "walking" (ê·¸ëŒ€ë¡œ ìœ ì§€!)\n'
          finalPrompt += '   â€¢ ëª¨ë“  ë™ì‘, ìŠ¤í† ë¦¬ëŠ” í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ê·¸ëŒ€ë¡œ ìœ ì§€!\n\n'
          finalPrompt += '1. ê° ë¼ë²¨ì˜ ì‹œê°ì  ìš”ì†Œë¥¼ PIXEL-LEVELë¡œ ì •í™•íˆ ë³µì œí•˜ì„¸ìš”\n'
          finalPrompt += '2. S1 ë°°ê²½: ì •í™•í•œ ìƒ‰ìƒ, ì¡°ëª…, êµ¬ì¡°ë¥¼ 1:1 ë³µì œ\n'
          finalPrompt += '3. S2 ìºë¦­í„°: ì •í™•í•œ ì™¸ëª¨, ì˜ìƒ, í—¤ì–´ ìŠ¤íƒ€ì¼ ë³µì œ\n'
          finalPrompt += '4. S3 ë¡œë´‡: ì •í™•í•œ ìƒ‰ìƒ(ë¹¨ê°•/í°ìƒ‰), í˜•íƒœ, ë””ìì¸ ë³µì œ\n'
          finalPrompt += '5. ì¶œë ¥ì€ ë‹¨ì¼ í†µí•© ì´ë¯¸ì§€ì—¬ì•¼ í•©ë‹ˆë‹¤ (ê·¸ë¦¬ë“œ ê¸ˆì§€)\n\n'
          finalPrompt += 'ğŸš« ì ˆëŒ€ ê¸ˆì§€ì‚¬í•­:\n'
          finalPrompt += '   âŒ ë™ì‘ ë³€ê²½ ("ë“¤ê³ " â†’ "ì“°ê³ ", "holding" â†’ "wearing")\n'
          finalPrompt += '   âŒ ì¸ë¬¼ ìˆ˜ ë³€ê²½ ("í•œ ëª…" â†’ "ë‘ ëª…")\n'
          finalPrompt += '   âŒ ë°°ê²½ ë””ìì¸ ë³€ê²½ (S1ê³¼ ë‹¤ë¥¸ ë°°ê²½)\n'
          finalPrompt += '   âŒ ìƒ‰ìƒ ë³€ê²½ (ë¹¨ê°• â†’ í•˜ì–‘, íŒŒë‘ â†’ ì´ˆë¡)\n'
          finalPrompt += '   âŒ ìŠ¤í† ë¦¬ ì¬í•´ì„\n\n'
          finalPrompt += 'REFERENCE = VISUAL ONLY. TEXT = COMPOSITION (NEVER CHANGE!)\n\n'
          finalPrompt += '---\n\n'
        } else if (refMode === 'balanced') {
          // ê· í˜• ëª¨ë“œ: ì ë‹¹í•œ ì§€ì‹œ
          finalPrompt += 'âš–ï¸ BALANCED MODE: Reference Image + Text Description\n\n'
          finalPrompt += gridLabelInfo + '\n\n'
          finalPrompt += 'ğŸ’¡ ì§€ì¹¨: ê° ë¼ë²¨ì˜ ì£¼ìš” ì‹œê°ì  ìš”ì†Œ(ìƒ‰ìƒ, ìŠ¤íƒ€ì¼, êµ¬ì¡°)ë¥¼ ìœ ì§€í•˜ë©´ì„œ í…ìŠ¤íŠ¸ ì„¤ëª…ì˜ ë””í…Œì¼ì„ ë°˜ì˜í•˜ì„¸ìš”.\n'
          finalPrompt += 'âš ï¸ ì¤‘ìš”: í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ì˜ ì¸ë¬¼ ìˆ˜, ê¸°ë³¸ êµ¬ì„±ì€ ìœ ì§€í•˜ì„¸ìš”.\n\n'
          finalPrompt += '---\n\n'
        } else if (refMode === 'creative') {
          // ì°½ì˜ì„± ëª¨ë“œ: ê°„ë‹¨í•œ ì°¸ê³ ë§Œ
          finalPrompt += 'ğŸ¨ CREATIVE MODE: Reference for Inspiration\n\n'
          finalPrompt += gridLabelInfo + '\n\n'
          finalPrompt += 'ğŸ’¡ ì°¸ê³ : ìœ„ ì´ë¯¸ì§€ëŠ” ìŠ¤íƒ€ì¼ê³¼ ë¶„ìœ„ê¸° ì°¸ê³ ìš©ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸ ì„¤ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ì°½ì˜ì ìœ¼ë¡œ ìƒì„±í•˜ì„¸ìš”.\n\n'
          finalPrompt += '---\n\n'
        }
      }
      
      // Add text prompt
      if (inputPrompt.trim()) {
        finalPrompt += inputPrompt
      } else if (data.mode === 'describe') {
        finalPrompt += 'Describe this image in detail and create a prompt for generating a similar image.'
      } else if (data.mode === 'analyze') {
        finalPrompt += 'Analyze this image comprehensively (composition, style, lighting, colors, subjects, mood) and create a detailed prompt for AI image generation.'
      } else if (referenceImageDataUrl) {
        // If only image is provided without text, use a default instruction
        finalPrompt += 'Describe this image and create an effective prompt for AI image generation.'
      }
      
      if (finalPrompt.trim()) {
        contentParts.push({
          text: finalPrompt.trim()
        })
      }
      
      // Validate we have content to send
      if (contentParts.length === 0) {
        throw new Error('í”„ë¡¬í”„íŠ¸ ë˜ëŠ” ì´ë¯¸ì§€ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.')
      }

      // Call LLM API based on provider
      let outputPrompt = ''
      
      if (provider === 'gemini') {
        // ğŸ”µ Gemini API í˜¸ì¶œ
        const url = `https://generativelanguage.googleapis.com/v1beta/models/${data.model}:generateContent?key=${apiKey}`
        
        const abortController = new AbortController()
        const timeoutId = setTimeout(() => abortController.abort(), 60000)
        
        try {
          const response = await fetch(url, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
            },
            body: JSON.stringify({
              contents: [{
                parts: contentParts
              }],
              systemInstruction: {
                parts: [{
                  text: systemInstruction
                }]
              },
              generationConfig: {
                temperature: 0.7,
                maxOutputTokens: 4096,
              }
            }),
            signal: abortController.signal
          })
          
          clearTimeout(timeoutId)

          if (!response.ok) {
            const errorData = await response.json().catch(() => ({}))
            throw new Error(errorData.error?.message || `API Error: ${response.status}`)
          }

          const result = await response.json()
          outputPrompt = result.candidates?.[0]?.content?.parts?.[0]?.text || ''
        } catch (fetchError: any) {
          clearTimeout(timeoutId)
          if (fetchError.name === 'AbortError') {
            throw new Error('LLM ìƒì„± ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤ (60ì´ˆ).')
          }
          throw fetchError
        }
      } else if (provider === 'openai') {
        // ğŸŸ¢ OpenAI API í˜¸ì¶œ
        const url = 'https://api.openai.com/v1/chat/completions'
        
        // OpenAI ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        const messages: any[] = [
          { role: 'system', content: systemInstruction }
        ]
        
        // User ë©”ì‹œì§€ êµ¬ì„± (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€)
        const userContent: any[] = []
        
        // í…ìŠ¤íŠ¸ ì¶”ê°€
        if (finalPrompt.trim()) {
          userContent.push({ type: 'text', text: finalPrompt.trim() })
        }
        
        // ì´ë¯¸ì§€ ì¶”ê°€ (GPT-4o, GPT-4o-miniëŠ” Vision ì§€ì›)
        if (referenceImageDataUrl && actualImageDataUrl.startsWith('data:image')) {
          userContent.push({
            type: 'image_url',
            image_url: { url: actualImageDataUrl }
          })
        }
        
        if (userContent.length > 0) {
          messages.push({ role: 'user', content: userContent })
        }
        
        const abortController = new AbortController()
        const timeoutId = setTimeout(() => abortController.abort(), 60000)
        
        try {
          const response = await fetch(url, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'Authorization': `Bearer ${apiKey}`
            },
            body: JSON.stringify({
              model: data.model,
              messages: messages,
              temperature: 0.7,
              max_tokens: 4096,
            }),
            signal: abortController.signal
          })
          
          clearTimeout(timeoutId)

          if (!response.ok) {
            const errorData = await response.json().catch(() => ({}))
            throw new Error(errorData.error?.message || `API Error: ${response.status}`)
          }

          const result = await response.json()
          outputPrompt = result.choices?.[0]?.message?.content || ''
        } catch (fetchError: any) {
          clearTimeout(timeoutId)
          if (fetchError.name === 'AbortError') {
            throw new Error('LLM ìƒì„± ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤ (60ì´ˆ).')
          }
          throw fetchError
        }
      }

      if (!outputPrompt) {
        throw new Error('LLMì´ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.')
      }

      console.log(`âœ… LLM í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ (${provider}):`, outputPrompt.length, 'ì')

      updateNode((prev) => ({
        ...prev,
        status: 'completed',
        outputPrompt: outputPrompt.trim(),
        error: undefined,
      }))
    } catch (error: any) {
      console.error('âŒ LLM ìƒì„± ì‹¤íŒ¨:', error)
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: formatErrorMessage(error),
      }))
    }
  },
  cancelNodeExecution: (id) => {
    console.log('ğŸ›‘ Cancelling node execution:', id)
    const { abortControllers } = get()
    const controller = abortControllers.get(id)
    if (controller) {
      try {
        controller.abort()
        abortControllers.delete(id)
        set({ abortControllers: new Map(abortControllers) })
        
        console.log('âœ… Abort controller cancelled successfully')
        
        // Update node status to idle immediately
        set({
          nodes: get().nodes.map((node) =>
            node.id === id
              ? {
                  ...node,
                  data: {
                    ...node.data,
                    status: 'idle',
                    error: 'ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.',
                    progress: 0,
                  },
                }
              : node,
          ),
        })
      } catch (error) {
        console.error('âŒ Error cancelling node:', error)
        // Still update status even if abort fails
        set({
          nodes: get().nodes.map((node) =>
            node.id === id
              ? {
                  ...node,
                  data: {
                    ...node.data,
                    status: 'idle',
                    error: 'ì‘ì—… ì·¨ì†Œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
                    progress: 0,
                  },
                }
              : node,
          ),
        })
      }
    } else {
      console.warn('âš ï¸ No abort controller found for node:', id)
    }
  },
    }),
    {
      name: 'nano-banana-workflow-v3',
      storage: createThrottledStorage(), // âš¡ Throttled storage
      partialize: (state) => {
        // ğŸ”¥ ì €ì¥ ì „ ìë™ ìš©ëŸ‰ ê´€ë¦¬
        const storageInfo = getStorageInfo()
        console.log(`ğŸ’¾ Persist: ${storageInfo.usedMB} MB / ${storageInfo.limitMB} MB (${storageInfo.percentage.toFixed(1)}%)`)
        
        // 90% ì´ìƒì´ë©´ ê¸´ê¸‰ ì •ë¦¬
        const shouldCleanup = storageInfo.percentage > 90
        const nodesToSave = shouldCleanup 
          ? prepareForStorage(state.nodes, true) // ê¸´ê¸‰ ì •ë¦¬
          : prepareForStorage(state.nodes, false) // ì¼ë°˜ ì •ë¦¬
        
        if (shouldCleanup) {
          console.warn('âš ï¸ localStorage 90% ì´ˆê³¼! ê¸´ê¸‰ ì •ë¦¬ ì‹¤í–‰')
        }
        
        return {
          nodes: sanitizeNodesForStorage(nodesToSave),
          edges: sanitizeEdgesForStorage(state.edges),
          apiKey: state.apiKey,
          klingApiKey: state.klingApiKey,
          openaiApiKey: state.openaiApiKey,  // OpenAI API Key ì €ì¥
        }
      },
      onRehydrateStorage: () => {
        console.log('ğŸ”„ Zustand persist: ë³µì› ì‹œì‘...')
        return (state) => {
          if (state) {
            // API í‚¤ê°€ ì €ì¥ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ .envì—ì„œ ìë™ ë¡œë“œ
            if (!state.apiKey) {
              state.apiKey = import.meta.env.VITE_GEMINI_API_KEY || ''
              if (state.apiKey) {
                console.log('ğŸ”‘ Gemini API í‚¤ ìë™ ë¡œë“œë¨ (.env)')
              }
            }
            if (!state.klingApiKey) {
              state.klingApiKey = import.meta.env.VITE_KLING_API_KEY || ''
              if (state.klingApiKey) {
                console.log('ğŸ”‘ Kling API í‚¤ ìë™ ë¡œë“œë¨ (.env)')
              }
            }
            if (!state.openaiApiKey) {
              state.openaiApiKey = import.meta.env.VITE_OPENAI_API_KEY || ''
              if (state.openaiApiKey) {
                console.log('ğŸ”‘ OpenAI API í‚¤ ìë™ ë¡œë“œë¨ (.env)')
              }
            }
            
            console.log('âœ… Zustand persist: ìƒíƒœ ë³µì›ë¨', {
              nodeCount: state.nodes?.length ?? 0,
              edgeCount: state.edges?.length ?? 0,
              hasApiKey: !!state.apiKey,
              hasKlingApiKey: !!state.klingApiKey,
              hasOpenaiApiKey: !!state.openaiApiKey,
            })
            try {
              state.edges = normalizeEdges(state.edges, state.nodes)
              console.log('âœ… Edges ì •ê·œí™” ì™„ë£Œ')
            } catch (error) {
              console.error('âŒ Error normalizing edges on rehydrate:', error)
              // Reset to safe state if normalization fails
              state.edges = []
            }
          } else {
            console.log('â„¹ï¸ Zustand persist: ë³µì›í•  ìƒíƒœ ì—†ìŒ (ìƒˆ ì‹œì‘)')
          }
        }
      },
    }
  )
)

export const createWorkflowNode = (type: NodeType, position: { x: number; y: number }): WorkflowNode => ({
  id: `${type}-${crypto.randomUUID?.() ?? Date.now()}`,
  type,
  position,
  data: createNodeData(type),
})
