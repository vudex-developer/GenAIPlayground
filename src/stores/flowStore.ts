import { create } from 'zustand'
import { persist } from 'zustand/middleware'
import {
  addEdge,
  applyEdgeChanges,
  applyNodeChanges,
  type Connection,
  type EdgeChange,
  type NodeChange,
} from 'reactflow'
import { GeminiAPIClient, MockGeminiAPI } from '../services/geminiAPI'
import { KlingAPIClient, MockKlingAPI } from '../services/klingAPI'
import { retryWithBackoff } from '../utils/retry'
import { getStorageInfo, prepareForStorage, getStorageWarning } from '../utils/storage'
import { createBackup } from '../utils/backup'
import { saveImage, getImage } from '../utils/indexedDB'
import type {
  GeminiVideoNodeData,
  GridNodeData,
  ImageImportNodeData,
  KlingVideoNodeData,
  MotionPromptNodeData,
  NanoImageNodeData,
  NodeData,
  NodeType,
  TextPromptNodeData,
  WorkflowEdge,
  WorkflowNode,
} from '../types/nodes'
import { createNodeData } from '../types/nodes'

type HistoryState = {
  nodes: WorkflowNode[]
  edges: WorkflowEdge[]
}

type FlowState = {
  nodes: WorkflowNode[]
  edges: WorkflowEdge[]
  selectedNodeId: string | null
  apiKey: string
  klingApiKey: string
  openaiApiKey: string  // OpenAI API Key
  abortControllers: Map<string, AbortController>
  history: HistoryState[]
  historyIndex: number
  imageModal: { isOpen: boolean; imageUrl: string | null }
  setSelectedNodeId: (id: string | null) => void
  setApiKey: (key: string) => void
  setKlingApiKey: (key: string) => void
  setOpenaiApiKey: (key: string) => void  // OpenAI API Key Setter
  openImageModal: (imageUrl: string) => void
  closeImageModal: () => void
  onNodesChange: (changes: NodeChange[]) => void
  onEdgesChange: (changes: EdgeChange[]) => void
  onConnect: (connection: Connection) => void
  addNode: (node: WorkflowNode) => void
  updateNodeData: (id: string, data: NodeData) => void
  saveWorkflow: () => boolean
  loadWorkflow: () => boolean
  importWorkflow: (nodes: WorkflowNode[], edges: WorkflowEdge[]) => boolean
  exportWorkflow: () => string
  undo: () => void
  redo: () => void
  runWorkflow: () => Promise<void>
  runGeminiNode: (id: string) => Promise<void>
  runNanoImageNode: (id: string) => Promise<void>
  runKlingNode: (id: string) => Promise<void>
  runLLMPromptNode: (id: string) => Promise<void>
  runCellRegeneratorNode: (id: string) => Promise<void>
  cancelNodeExecution: (id: string) => void
}

const getEdgeClass = (edge: WorkflowEdge, nodes: WorkflowNode[]) => {
  const sourceNode = nodes.find((node) => node.id === edge.source)
  switch (sourceNode?.type) {
    case 'textPrompt':
      return 'edge-text-prompt'
    case 'motionPrompt':
      return 'edge-motion-prompt'
    case 'imageImport':
      return 'edge-image-import'
    case 'nanoImage':
      return 'edge-nano-image'
    case 'geminiVideo':
      return 'edge-gemini-video'
    case 'klingVideo':
      return 'edge-kling-video'
    case 'gridNode':
      return 'edge-text-prompt' // Use violet color
    case 'cellRegenerator':
      return 'edge-motion-prompt' // Use purple color
    case 'gridComposer':
      return 'edge-kling-video' // Use emerald color
    case 'llmPrompt':
      return 'edge-motion-prompt' // Use pink color
    default:
      return 'edge-default'
  }
}

const normalizeEdges = (edges: WorkflowEdge[], nodes: WorkflowNode[]) => {
  const nodeIds = new Set(nodes.map(n => n.id))
  
  // Filter out edges that reference deleted nodes
  return edges
    .filter(edge => nodeIds.has(edge.source) && nodeIds.has(edge.target))
    .map((edge) => ({
      ...edge,
      type: 'bezier',
      className: getEdgeClass(edge, nodes),
    }))
}

const sanitizeEdgesForStorage = (edges: WorkflowEdge[]) =>
  edges.map(({ source, target, sourceHandle, targetHandle, id }) => ({
    id,
    source,
    target,
    sourceHandle,
    targetHandle,
  }))

const isConnectionAllowed = (sourceType: NodeType, targetType: NodeType) => {
  if (sourceType === 'imageImport' && targetType === 'motionPrompt') return true
  if (sourceType === 'imageImport' && targetType === 'nanoImage') return true
  if (sourceType === 'nanoImage' && targetType === 'nanoImage') return true
  if (sourceType === 'textPrompt' && targetType === 'imageImport') return true
  if (sourceType === 'textPrompt' && targetType === 'nanoImage') return true
  if (sourceType === 'textPrompt' && targetType === 'motionPrompt') return true
  if (sourceType === 'motionPrompt' && targetType === 'nanoImage') return true
  if (sourceType === 'motionPrompt' && targetType === 'geminiVideo') return true
  if (sourceType === 'nanoImage' && targetType === 'geminiVideo') return true
  if (sourceType === 'imageImport' && targetType === 'geminiVideo') return true
  // Kling connections
  if (sourceType === 'motionPrompt' && targetType === 'klingVideo') return true
  if (sourceType === 'textPrompt' && targetType === 'klingVideo') return true
  if (sourceType === 'nanoImage' && targetType === 'klingVideo') return true
  if (sourceType === 'imageImport' && targetType === 'klingVideo') return true
  // Grid Node connections
  if (sourceType === 'textPrompt' && targetType === 'gridNode') return true
  if (sourceType === 'motionPrompt' && targetType === 'gridNode') return true
  if (sourceType === 'gridNode' && targetType === 'nanoImage') return true
  // Cell Regenerator connections
  if (sourceType === 'gridNode' && targetType === 'cellRegenerator') return true
  if (sourceType === 'nanoImage' && targetType === 'cellRegenerator') return true
  if (sourceType === 'imageImport' && targetType === 'cellRegenerator') return true
  if (sourceType === 'cellRegenerator' && targetType === 'nanoImage') return true
  if (sourceType === 'cellRegenerator' && targetType === 'imageImport') return true
  // Grid Composer connections
  if (sourceType === 'gridNode' && targetType === 'gridComposer') return true
  if (sourceType === 'nanoImage' && targetType === 'gridComposer') return true
  if (sourceType === 'imageImport' && targetType === 'gridComposer') return true
  if (sourceType === 'cellRegenerator' && targetType === 'gridComposer') return true
  if (sourceType === 'gridComposer' && targetType === 'nanoImage') return true
  if (sourceType === 'gridComposer' && targetType === 'imageImport') return true
  if (sourceType === 'gridComposer' && targetType === 'geminiVideo') return true
  if (sourceType === 'gridComposer' && targetType === 'klingVideo') return true
  // LLM Prompt connections
  if (sourceType === 'textPrompt' && targetType === 'llmPrompt') return true
  if (sourceType === 'motionPrompt' && targetType === 'llmPrompt') return true
  if (sourceType === 'imageImport' && targetType === 'llmPrompt') return true
  if (sourceType === 'nanoImage' && targetType === 'llmPrompt') return true
  if (sourceType === 'gridComposer' && targetType === 'llmPrompt') return true
  if (sourceType === 'llmPrompt' && targetType === 'textPrompt') return true
  if (sourceType === 'llmPrompt' && targetType === 'nanoImage') return true
  if (sourceType === 'llmPrompt' && targetType === 'motionPrompt') return true
  if (sourceType === 'llmPrompt' && targetType === 'gridNode') return true
  if (sourceType === 'llmPrompt' && targetType === 'geminiVideo') return true
  if (sourceType === 'llmPrompt' && targetType === 'klingVideo') return true
  return false
}

const getExecutionOrder = (nodes: WorkflowNode[], edges: WorkflowEdge[]) => {
  const incoming = new Map<string, number>()
  const outgoing = new Map<string, string[]>()

  nodes.forEach((node) => {
    incoming.set(node.id, 0)
    outgoing.set(node.id, [])
  })

  edges.forEach((edge) => {
    incoming.set(edge.target, (incoming.get(edge.target) ?? 0) + 1)
    outgoing.get(edge.source)?.push(edge.target)
  })

  const queue = nodes.filter((node) => (incoming.get(node.id) ?? 0) === 0)
  const order: string[] = []

  while (queue.length) {
    const node = queue.shift()
    if (!node) break
    order.push(node.id)
    const neighbors = outgoing.get(node.id) ?? []
    neighbors.forEach((neighbor) => {
      incoming.set(neighbor, (incoming.get(neighbor) ?? 1) - 1)
      if ((incoming.get(neighbor) ?? 0) === 0) {
        const neighborNode = nodes.find((candidate) => candidate.id === neighbor)
        if (neighborNode) queue.push(neighborNode)
      }
    })
  }

  return order
}

const getIncomingNodes = (
  nodeId: string,
  edges: WorkflowEdge[],
  nodes: WorkflowNode[],
) =>
  edges
    .filter((edge) => edge.target === nodeId)
    .map((edge) => nodes.find((node) => node.id === edge.source))
    .filter(Boolean) as WorkflowNode[]

const getIncomingTextPrompt = (
  nodeId: string,
  edges: WorkflowEdge[],
  nodes: WorkflowNode[],
) => {
  const promptEdge = edges.find((edge) => edge.target === nodeId)
  if (!promptEdge) return undefined
  const promptNode = nodes.find((node) => node.id === promptEdge.source)
  if (!promptNode || promptNode.type !== 'textPrompt') return undefined
  return (promptNode.data as TextPromptNodeData).prompt
}

const STORAGE_KEY = 'nano-banana-workflow-v3'

// Helper function to make error messages more user-friendly
const formatErrorMessage = (error: unknown): string => {
  if (!(error instanceof Error)) return 'ÏòàÏÉÅÏπò Î™ªÌïú Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.'
  
  const message = error.message.toLowerCase()
  
  // API quota exceeded
  if (message.includes('quota') && message.includes('exceeded')) {
    return 'API Ìï†ÎãπÎüâÏù¥ Ï¥àÍ≥ºÎêòÏóàÏäµÎãàÎã§. Ïû†Ïãú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî.'
  }
  
  // Other common errors
  if (message.includes('network') || message.includes('fetch')) {
    return 'ÎÑ§Ìä∏ÏõåÌÅ¨ Ïó∞Í≤∞ÏùÑ ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî.'
  }
  
  if (message.includes('api key') || message.includes('unauthorized')) {
    return 'API ÌÇ§Î•º ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî.'
  }
  
  // Return original message if no match
  return error.message
}

const sanitizeNodesForStorage = (nodes: WorkflowNode[], forExport = false): WorkflowNode[] =>
  nodes.map((node) => {
    const data = { ...(node.data as Record<string, unknown>) }
    
    // Ïù¥Ï†ú localStorageÏóêÎèÑ Ïù¥ÎØ∏ÏßÄÎ•º Ï†ÄÏû•Ìï©ÎãàÎã§
    // base64 DataURLÏùÄ Ïú†ÏßÄ (ÏÉàÎ°úÍ≥†Ïπ® ÌõÑÏóêÎèÑ Î≥µÏõêÎê®)
    // Îã®, blob URLÏùÄ Ï†úÍ±∞ (ÌéòÏù¥ÏßÄ Ïû¨Î°úÎìú Ïãú Î¨¥Ìö®ÌôîÎê®)
    
    // Always remove blob URLs (they don't survive page reload)
    if (typeof data.imageUrl === 'string' && data.imageUrl.startsWith('blob:')) {
      delete data.imageUrl
      delete data.width
      delete data.height
    }
    if (typeof data.outputImageUrl === 'string' && data.outputImageUrl.startsWith('blob:')) {
      delete data.outputImageUrl
    }
    if (typeof data.inputImageUrl === 'string' && data.inputImageUrl.startsWith('blob:')) {
      delete data.inputImageUrl
    }
    if (typeof data.composedImageUrl === 'string' && data.composedImageUrl.startsWith('blob:')) {
      delete data.composedImageUrl
    }
    
    // regeneratedImages Í∞ùÏ≤¥ ÎÇ¥Î∂ÄÏùò blob URLÎèÑ Ï†úÍ±∞
    if (data.regeneratedImages && typeof data.regeneratedImages === 'object') {
      const cleanedImages: Record<string, string> = {}
      for (const [key, value] of Object.entries(data.regeneratedImages)) {
        if (typeof value === 'string' && !value.startsWith('blob:')) {
          cleanedImages[key] = value
        }
      }
      data.regeneratedImages = cleanedImages
    }
    
    // inputImages Í∞ùÏ≤¥ ÎÇ¥Î∂ÄÏùò blob URLÎèÑ Ï†úÍ±∞
    if (data.inputImages && typeof data.inputImages === 'object') {
      const cleanedImages: Record<string, string> = {}
      for (const [key, value] of Object.entries(data.inputImages)) {
        if (typeof value === 'string' && !value.startsWith('blob:')) {
          cleanedImages[key] = value
        }
      }
      data.inputImages = cleanedImages
    }
    if (typeof data.outputVideoUrl === 'string' && data.outputVideoUrl.startsWith('blob:')) {
      delete data.outputVideoUrl
    }
    
    // Reset status for all generation nodes
    if (node.type === 'nanoImage') {
      data.status = 'idle'
      delete data.error
      delete data.lastExecutionTime
    }
    if (node.type === 'geminiVideo') {
      data.status = 'idle'
      data.progress = 0
      delete data.error
      delete data.lastExecutionTime
    }
    if (node.type === 'klingVideo') {
      data.status = 'idle'
      data.progress = 0
      delete data.error
      delete data.taskId
      delete data.lastExecutionTime
    }
    return { ...node, data: data as NodeData }
  })

const MAX_HISTORY_SIZE = 20

const saveToHistory = (get: () => FlowState, set: (state: Partial<FlowState>) => void) => {
  const { nodes, edges, history, historyIndex } = get()
  
  // Remove any history after current index (when undoing then making new changes)
  const newHistory = history.slice(0, historyIndex + 1)
  
  // Add current state
  newHistory.push({ nodes: JSON.parse(JSON.stringify(nodes)), edges: JSON.parse(JSON.stringify(edges)) })
  
  // Keep only last MAX_HISTORY_SIZE states
  if (newHistory.length > MAX_HISTORY_SIZE) {
    newHistory.shift()
  }
  
  set({ 
    history: newHistory, 
    historyIndex: newHistory.length - 1 
  })
}

// ‚ö° Throttled localStorage for better performance
const createThrottledStorage = () => {
  let saveTimeout: NodeJS.Timeout | null = null
  const SAVE_DELAY = 1000 // 1Ï¥à throttle

  return {
    getItem: (name: string) => {
      const value = localStorage.getItem(name)
      return value ? JSON.parse(value) : null
    },
    setItem: (name: string, value: any) => {
      // Throttle: 1Ï¥à ÎèôÏïà Ïó¨Îü¨ Î≤à Ìò∏Ï∂úÎêòÎ©¥ ÎßàÏßÄÎßâ Í≤ÉÎßå Ï†ÄÏû•
      if (saveTimeout) {
        clearTimeout(saveTimeout)
      }
      
      saveTimeout = setTimeout(() => {
        try {
          const serialized = JSON.stringify(value)
          localStorage.setItem(name, serialized)
          console.log('üíæ Throttled save completed')
        } catch (error) {
          console.error('‚ùå Save failed:', error)
        }
      }, SAVE_DELAY)
    },
    removeItem: (name: string) => {
      localStorage.removeItem(name)
    },
  }
}

export const useFlowStore = create<FlowState>()(
  persist(
    (set, get) => ({
      nodes: [],
      edges: [],
      selectedNodeId: null,
      // .env ÌååÏùºÏóêÏÑú API ÌÇ§ ÏûêÎèô Î°úÎìú
      apiKey: import.meta.env.VITE_GEMINI_API_KEY || '',
      klingApiKey: import.meta.env.VITE_KLING_API_KEY || '',
      openaiApiKey: import.meta.env.VITE_OPENAI_API_KEY || '',  // OpenAI API Key
      abortControllers: new Map(),
      history: [],
      historyIndex: -1,
      imageModal: { isOpen: false, imageUrl: null },
      setSelectedNodeId: (id) => set({ selectedNodeId: id }),
      setApiKey: (key) => set({ apiKey: key }),
      setKlingApiKey: (key) => set({ klingApiKey: key }),
      setOpenaiApiKey: (key) => set({ openaiApiKey: key }),  // OpenAI API Key Setter
      openImageModal: (imageUrl) => set({ imageModal: { isOpen: true, imageUrl } }),
      closeImageModal: () => set({ imageModal: { isOpen: false, imageUrl: null } }),
  onNodesChange: (changes) => {
    try {
      // üéØ ÏÑ±Îä• ÏµúÏ†ÅÌôî: position Î≥ÄÍ≤ΩÎßå ÏûàÏúºÎ©¥ Î°úÍ∑∏ ÏÉùÎûµ
      const hasNonPositionChange = changes.some(
        change => change.type !== 'position' && change.type !== 'dimensions'
      )
      
      if (hasNonPositionChange) {
        console.log('üîÑ onNodesChange:', changes)
      }
      
      // Clean up abort controllers for removed nodes
      const removedNodeIds = changes
        .filter(change => change.type === 'remove')
        .map(change => (change as any).id)
      
      if (removedNodeIds.length > 0) {
        console.log('üóëÔ∏è ÎÖ∏Îìú ÏÇ≠Ï†ú ÏãúÎèÑ:', removedNodeIds)
        const { abortControllers } = get()
        removedNodeIds.forEach(id => {
          const controller = abortControllers.get(id)
          if (controller) {
            console.log('üßπ Cleaning up abort controller for deleted node:', id)
            controller.abort()
            abortControllers.delete(id)
          }
        })
        if (removedNodeIds.length > 0) {
          set({ abortControllers: new Map(abortControllers) })
        }
      }
      
      const currentNodes = get().nodes
      const currentEdges = get().edges
      
      const newNodes = applyNodeChanges(changes, currentNodes) as WorkflowNode[]
      const newEdges = normalizeEdges(currentEdges, newNodes)
      
      set({ 
        nodes: newNodes,
        edges: newEdges
      })
      
      // ‚ö° ÏÑ±Îä• ÏµúÏ†ÅÌôî: add/removeÎßå history Ï†ÄÏû• (position/selectÎäî Ï†úÏô∏)
      const shouldSaveHistory = changes.some(change => 
        change.type === 'add' || change.type === 'remove'
      )
      if (shouldSaveHistory) {
        saveToHistory(get, set)
      }
    } catch (error) {
      console.error('‚ùå Error in onNodesChange:', error)
      // ÏóêÎü¨ Î∞úÏÉùÌï¥ÎèÑ Ïï±Ïù¥ Î©àÏ∂îÏßÄ ÏïäÎèÑÎ°ù
    }
  },
  onEdgesChange: (changes) => {
    try {
      const currentEdges = get().edges
      const currentNodes = get().nodes
      const newEdges = applyEdgeChanges(changes, currentEdges)
      
      set({
        edges: normalizeEdges(newEdges, currentNodes),
      })
      
      // Save to history for add/remove changes
      const shouldSaveHistory = changes.some(change => 
        change.type === 'add' || change.type === 'remove'
      )
      if (shouldSaveHistory) {
        saveToHistory(get, set)
      }
    } catch (error) {
      console.error('Error in onEdgesChange:', error)
      // Don't crash the app, just log the error
    }
  },
  onConnect: (connection) => {
    const { source, target } = connection
    if (!source || !target) return
    const sourceNode = get().nodes.find((node) => node.id === source)
    const targetNode = get().nodes.find((node) => node.id === target)
    if (!sourceNode || !targetNode || !sourceNode.type || !targetNode.type) return
    if (!isConnectionAllowed(sourceNode.type, targetNode.type)) return

    set({
      edges: normalizeEdges(
        addEdge(
          {
            ...connection,
            type: 'bezier',
          },
          get().edges,
        ),
        get().nodes,
      ),
    })
    saveToHistory(get, set)
  },
  addNode: (node) => {
    set({ nodes: [...get().nodes, node] })
    saveToHistory(get, set)
  },
  updateNodeData: (id, data) => {
    set({
      nodes: get().nodes.map((node) => {
        if (node.id === id) {
          // Support partial updates by spreading existing data
          const newData = typeof data === 'function' ? data(node.data) : data
          return { ...node, data: { ...node.data, ...newData } }
        }
        return node
      }),
    })
    // Note: persist middleware will automatically save to localStorage
  },
  saveWorkflow: () => {
    try {
      // üìä persist ÎØ∏Îì§Ïõ®Ïñ¥Í∞Ä ÏûêÎèôÏúºÎ°ú Ï†ÄÏû•ÌïòÎØÄÎ°ú, Ïó¨Í∏∞ÏÑúÎäî Î∞±ÏóÖÎßå ÏÉùÏÑ±
      console.log('üíæ Î∞±ÏóÖ ÏÉùÏÑ± Ï§ë...')
      
      // Ï†ÄÏû•Í≥µÍ∞Ñ Ï≤¥ÌÅ¨
      const storageInfo = getStorageInfo()
      console.log(`üìä Storage: ${storageInfo.usedMB} MB / ${storageInfo.limitMB} MB (${storageInfo.percentage.toFixed(1)}%)`)
      
      const warning = getStorageWarning(storageInfo)
      if (warning) {
        console.warn(warning)
      }
      
      // persistÍ∞Ä Ï†ÄÏû•Ìïú Îç∞Ïù¥ÌÑ∞ Í∞ÄÏ†∏Ïò§Í∏∞
      const persistedData = localStorage.getItem('nano-banana-workflow-v3')
      if (persistedData) {
        // üîí ÏûêÎèô Î∞±ÏóÖ ÏÉùÏÑ± (5Î∂ÑÎßàÎã§ Ìïú Î≤àÏî©Îßå)
        const lastBackupKey = 'last-backup-time'
        const lastBackup = parseInt(localStorage.getItem(lastBackupKey) || '0')
        const now = Date.now()
        const fiveMinutes = 5 * 60 * 1000
        
        if (now - lastBackup > fiveMinutes) {
          // persist ÌòïÏãù Í∑∏ÎåÄÎ°ú Î∞±ÏóÖ
          createBackup(persistedData)
          localStorage.setItem(lastBackupKey, now.toString())
          console.log('‚úÖ Î∞±ÏóÖ ÏÉùÏÑ± ÏôÑÎ£å')
        } else {
          console.log('‚è≠Ô∏è Î∞±ÏóÖ ÏÉùÏÑ± Í±¥ÎÑàÎúÄ (5Î∂Ñ Ïù¥ÎÇ¥)')
        }
      }
      
      // persistÍ∞Ä ÏûêÎèôÏúºÎ°ú Ï†ÄÏû•ÌïòÎØÄÎ°ú Ìï≠ÏÉÅ ÏÑ±Í≥µ Î∞òÌôò
      return true
    } catch (error) {
      console.error('‚ùå Î∞±ÏóÖ ÏÉùÏÑ± Ïã§Ìå®:', error)
      return false
    }
  },
  loadWorkflow: () => {
    try {
      console.log('üîÑ loadWorkflow Ìò∏Ï∂úÎê®')
      const raw = localStorage.getItem(STORAGE_KEY)
      if (!raw) {
        console.log('‚ÑπÔ∏è localStorageÏóê Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå')
        return false
      }
      
      const parsed = JSON.parse(raw)
      console.log('üì¶ localStorage Îç∞Ïù¥ÌÑ∞ ÌååÏã± ÏÑ±Í≥µ:', parsed)
      
      // persist ÌòïÏãù ÌôïÏù∏: { state: {...}, version: 0 }
      let nodes: WorkflowNode[] = []
      let edges: WorkflowEdge[] = []
      
      if (parsed.state) {
        // persist ÎØ∏Îì§Ïõ®Ïñ¥ ÌòïÏãù
        console.log('‚úÖ persist ÌòïÏãù Í∞êÏßÄ')
        nodes = Array.isArray(parsed.state.nodes) ? parsed.state.nodes : []
        edges = Array.isArray(parsed.state.edges) ? parsed.state.edges : []
      } else if (parsed.nodes) {
        // Íµ¨Î≤ÑÏ†Ñ ÎòêÎäî export ÌòïÏãù
        console.log('‚ÑπÔ∏è Íµ¨Î≤ÑÏ†Ñ ÌòïÏãù Í∞êÏßÄ')
        nodes = Array.isArray(parsed.nodes) ? parsed.nodes : []
        edges = Array.isArray(parsed.edges) ? parsed.edges : []
      }
      
      console.log('üìä Î°úÎìúÎêú Îç∞Ïù¥ÌÑ∞:', { nodeCount: nodes.length, edgeCount: edges.length })
      
      if (nodes.length === 0) {
        console.log('‚ö†Ô∏è ÎÖ∏ÎìúÍ∞Ä ÏóÜÏùå')
        return false
      }
      
      set({
        nodes,
        edges: normalizeEdges(edges, nodes),
        selectedNodeId: null,
      })
      
      console.log('‚úÖ ÏõåÌÅ¨ÌîåÎ°úÏö∞ Î≥µÏõê ÏôÑÎ£å')
      return true
    } catch (error) {
      console.error('‚ùå loadWorkflow Ïã§Ìå®:', error)
      return false
    }
  },
  importWorkflow: (nodes, edges) => {
    try {
      const currentState = get()
      const existingNodes = currentState.nodes
      const existingEdges = currentState.edges
      
      // Create ID mapping to avoid conflicts
      const idMap = new Map<string, string>()
      const existingIds = new Set(existingNodes.map(n => n.id))
      
      // Generate new IDs for imported nodes if they conflict
      const newNodes = nodes.map(node => {
        let newId = node.id
        
        // If ID already exists, generate a new one
        if (existingIds.has(newId)) {
          newId = `${node.type}-${crypto.randomUUID?.() ?? Date.now()}`
        }
        
        idMap.set(node.id, newId)
        
        // Calculate offset: place imported nodes to the right of existing nodes
        const maxX = existingNodes.length > 0 
          ? Math.max(...existingNodes.map(n => n.position.x + 250))
          : 0
        
        return {
          ...node,
          id: newId,
          position: {
            x: node.position.x + maxX + 50,
            y: node.position.y
          },
          selected: false,
        }
      })
      
      // Update edge IDs to match new node IDs
      const newEdges = edges.map(edge => {
        const newSource = idMap.get(edge.source) ?? edge.source
        const newTarget = idMap.get(edge.target) ?? edge.target
        
        return {
          ...edge,
          id: `${newSource}-${edge.sourceHandle ?? 'output'}-${newTarget}-${edge.targetHandle ?? 'input'}`,
          source: newSource,
          target: newTarget,
        }
      })
      
      // Merge with existing nodes and edges
      const mergedNodes = [...existingNodes, ...newNodes]
      const mergedEdges = normalizeEdges([...existingEdges, ...newEdges], mergedNodes)
      
      set({
        nodes: mergedNodes,
        edges: mergedEdges,
        selectedNodeId: null,
      })
      
      // Save to history
      saveToHistory(get, set)
      
      return true
    } catch (error) {
      console.error('Import workflow failed:', error)
      return false
    }
  },
  exportWorkflow: () => {
    const { nodes, edges } = get()
    return JSON.stringify({
      version: '1.0',
      timestamp: new Date().toISOString(),
      nodes: sanitizeNodesForStorage(nodes, true), // Keep base64 for export
      edges: sanitizeEdgesForStorage(edges),
    }, null, 2)
  },
  undo: () => {
    const { history, historyIndex } = get()
    if (historyIndex > 0) {
      const previousState = history[historyIndex - 1]
      set({
        nodes: JSON.parse(JSON.stringify(previousState.nodes)),
        edges: normalizeEdges(JSON.parse(JSON.stringify(previousState.edges)), previousState.nodes),
        historyIndex: historyIndex - 1,
        selectedNodeId: null,
      })
    }
  },
  redo: () => {
    const { history, historyIndex } = get()
    if (historyIndex < history.length - 1) {
      const nextState = history[historyIndex + 1]
      set({
        nodes: JSON.parse(JSON.stringify(nextState.nodes)),
        edges: normalizeEdges(JSON.parse(JSON.stringify(nextState.edges)), nextState.nodes),
        historyIndex: historyIndex + 1,
        selectedNodeId: null,
      })
    }
  },
  runNanoImageNode: async (id) => {
    const { nodes, edges, abortControllers } = get()
    const current = nodes.find((node) => node.id === id)
    if (!current || current.type !== 'nanoImage') return

    // ‚úÖ Prevent duplicate execution
    const currentData = current.data as NanoImageNodeData
    if (currentData.status === 'processing') {
      console.warn('‚ö†Ô∏è Node is already processing, skipping duplicate execution')
      return
    }

    // ‚úÖ Rate limiting: Check last execution time
    const now = Date.now()
    const lastExecution = (currentData as any).lastExecutionTime || 0
    const minInterval = 3000 // 3 seconds minimum between executions
    
    if (now - lastExecution < minInterval) {
      const waitTime = Math.ceil((minInterval - (now - lastExecution)) / 1000)
      set({
        nodes: nodes.map((node) =>
          node.id === id
            ? {
                ...node,
                data: {
                  ...node.data,
                  status: 'error',
                  error: `ÎÑàÎ¨¥ Îπ†Î•¥Í≤å Ïã§ÌñâÌñàÏäµÎãàÎã§. ${waitTime}Ï¥à ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî.`,
                },
              }
            : node,
        ),
      })
      return
    }

    // Create abort controller for this execution
    const abortController = new AbortController()
    abortControllers.set(id, abortController)
    set({ abortControllers: new Map(abortControllers) })

    const updateNode = (updater: (data: NodeData) => NodeData) => {
      set({
        nodes: get().nodes.map((node) =>
          node.id === id ? { ...node, data: updater(node.data) } : node,
        ),
      })
    }

    const incoming = getIncomingNodes(id, edges, get().nodes)
    
    // Get prompt from various sources
    let prompt = ''
    
    // Check for gridNode
    const gridNodeEdge = edges.find((e) => e.target === id && 
      get().nodes.find(n => n.id === e.source)?.type === 'gridNode')
    
    if (gridNodeEdge) {
      const gridNode = get().nodes.find((n) => n.id === gridNodeEdge.source)
      if (gridNode?.type === 'gridNode') {
        const data = gridNode.data as GridNodeData
        const slotId = gridNodeEdge.sourceHandle
        prompt = slotId ? data.generatedPrompts?.[slotId] || '' : ''
      }
    } else {
      // Check for prompt handle connection
      const promptEdge = edges.find((e) => e.target === id && e.targetHandle === 'prompt')
      if (promptEdge) {
        const promptNode = get().nodes.find((n) => n.id === promptEdge.source)
        prompt =
          promptNode?.type === 'textPrompt'
            ? (promptNode.data as TextPromptNodeData).prompt
            : promptNode?.type === 'motionPrompt'
              ? (promptNode.data as MotionPromptNodeData).combinedPrompt
              : promptNode?.type === 'llmPrompt'
                ? (promptNode.data as any).outputPrompt || ''
                : ''
      } else {
        // Fallback: Original prompt logic (any connection)
        const promptNode = incoming.find(
          (node) => node.type === 'textPrompt' || node.type === 'motionPrompt' || node.type === 'llmPrompt',
        )
        prompt =
          promptNode?.type === 'textPrompt'
            ? (promptNode.data as TextPromptNodeData).prompt
            : promptNode?.type === 'motionPrompt'
              ? (promptNode.data as MotionPromptNodeData).combinedPrompt
              : promptNode?.type === 'llmPrompt'
                ? (promptNode.data as any).outputPrompt || ''
                : ''
      }
    }
    
    // Collect multiple reference images
    const referenceImages: string[] = []
    const referencePrompts: string[] = []
    let referenceSlotId: string | undefined
    let referenceSlotLabel: string | undefined
    let referenceCellRegeneratorId: string | undefined
    const data = current.data as NanoImageNodeData
    const maxRefs = data.maxReferences || 3
    
    // Check each ref-N handle
    for (let i = 1; i <= maxRefs; i++) {
      const refEdge = edges.find((e) => e.target === id && e.targetHandle === `ref-${i}`)
      if (refEdge) {
        const refNode = get().nodes.find((n) => n.id === refEdge.source)
        if (refNode) {
          let imageDataUrl: string | undefined
          let refPrompt: string | undefined
          
          if (refNode.type === 'imageImport') {
            const imgData = refNode.data as ImageImportNodeData
            imageDataUrl = imgData.imageDataUrl
            refPrompt = getIncomingTextPrompt(refNode.id, edges, get().nodes) ?? imgData.referencePrompt
          } else if (refNode.type === 'nanoImage') {
            const imgData = refNode.data as NanoImageNodeData
            imageDataUrl = imgData.outputImageDataUrl
          } else if (refNode.type === 'gridComposer') {
            const imgData = refNode.data as any
            imageDataUrl = imgData.composedImageDataUrl || imgData.composedImageUrl
          } else if (refNode.type === 'cellRegenerator') {
            const imgData = refNode.data as any
            // For cell regenerator, try to get image from sourceHandle (e.g., S1, S2, etc.)
            let cellId = refEdge.sourceHandle
            
            console.log(`üîç Nano Image: Checking Cell Regenerator connection`, {
              sourceHandle: cellId,
              availableCells: Object.keys(imgData.regeneratedImages || {})
            })

    // ü©π Self-healing: If handle is "output" or missing but we have cells, use S1
    if ((!cellId || cellId === 'output' || !imgData.regeneratedImages?.[cellId]) && 
        imgData.regeneratedImages && Object.keys(imgData.regeneratedImages).length > 0) {
      const firstCell = Object.keys(imgData.regeneratedImages)[0]
      console.warn(`‚ö†Ô∏è Nano Image: Invalid cellId "${cellId}", falling back to "${firstCell}"`)
      cellId = firstCell
    }

    if (cellId && imgData.regeneratedImages?.[cellId]) {
      const imageData = imgData.regeneratedImages[cellId]
      // üõ°Ô∏è CRITICAL: NEVER use raw data URLs if it's too large, must be idb: reference
      if (imageData.startsWith('data:') && imageData.length > 100000) {
        console.error('‚ùå Nano Image: Data URL is too large for state! Use saveImage first.')
        // In this case, we'll try to use it but warn
      }
      imageDataUrl = imageData
      referenceSlotId = referenceSlotId || cellId
      referenceSlotLabel =
        referenceSlotLabel ||
        imgData.slots?.find((slot: any) => slot.id === cellId)?.label
      referenceCellRegeneratorId = referenceCellRegeneratorId || refNode.id
      console.log(`‚úÖ Nano Image: Found cell image for ${cellId}`)
    } else {
      console.error(`‚ùå Nano Image: Could not find image for cell ${cellId}. Stop fallback to prevent full grid output.`)
      // Stop here - do NOT fall back to other images if this specific handle was requested
      throw new Error(`Ïó∞Í≤∞Îêú ÏÖÄ(${cellId}) Ïù¥ÎØ∏ÏßÄÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. Cell RegeneratorÎ•º Îã§Ïãú Ïã§ÌñâÌï¥Ï£ºÏÑ∏Ïöî.`)
    }
  }
          
          if (imageDataUrl) {
            // üî• Convert idb: or s3: reference to actual DataURL
            if (imageDataUrl.startsWith('idb:') || imageDataUrl.startsWith('s3:')) {
              try {
                const actualDataUrl = await getImage(imageDataUrl)
                if (actualDataUrl) {
                  referenceImages.push(actualDataUrl)
                } else {
                  console.warn(`‚ö†Ô∏è Failed to load reference image: ${imageDataUrl}`)
                }
              } catch (error) {
                console.error(`‚ùå Error loading reference image: ${imageDataUrl}`, error)
              }
            } else {
              referenceImages.push(imageDataUrl)
            }
            
            if (refPrompt) {
              referencePrompts.push(`Reference ${i}: ${refPrompt}`)
            }
          }
        }
      }
    }

    // üß≠ If reference comes from Cell Regenerator, prefer slot-specific prompt
    const normalizedPrompt = (prompt || '').trim()
    const isSlotOnlyPrompt = /^s\d+$/i.test(normalizedPrompt)
    // Even more aggressive grid detection
    const looksLikeGridPrompt =
      /sequence\s+grid|grid\s*\(|rows?,\s*columns?|story\s*board|storyboard|multi-panel|multi\s+panel|contact\s+sheet/i.test(normalizedPrompt)

    if (referenceSlotId) {
      let inferredPrompt = ''
      console.error('üöÄüß≠ Nano Image: Slot detected! Checking for inference...', {
        referenceSlotId,
        normalizedPrompt,
        isSlotOnlyPrompt,
        looksLikeGridPrompt
      })

      if (referenceCellRegeneratorId) {
        const gridEdge = edges.find(
          (e) => e.target === referenceCellRegeneratorId && e.targetHandle === 'grid-layout',
        )
        const gridNode =
          gridEdge && get().nodes.find((n) => n.id === gridEdge.source && n.type === 'gridNode')
        if (gridNode?.type === 'gridNode') {
          const gridData = gridNode.data as GridNodeData
          inferredPrompt = gridData.generatedPrompts?.[referenceSlotId] || ''
          console.error('üöÄüß≠ Nano Image: Found grid node prompt:', inferredPrompt.substring(0, 50) + '...')
        }
      }

      const shouldOverridePrompt =
        !normalizedPrompt || 
        normalizedPrompt.length < 5 || 
        isSlotOnlyPrompt || 
        looksLikeGridPrompt

      if (shouldOverridePrompt) {
        if (!inferredPrompt) {
          const labelText = referenceSlotLabel ? ` (${referenceSlotLabel})` : ''
          inferredPrompt =
            `EXACTLY RECREATE this reference image. Remove text labels only.` +
            ` Maintain composition, lighting, and cinematic style.` +
            ` Subject: ${referenceSlotId}${labelText}.`
        }

        prompt = inferredPrompt
        console.error('‚úÖüöÄ Nano Image: Prompt inferred successfully! NEW PROMPT:', prompt)
      } else {
        console.error('‚ÑπÔ∏èüöÄ Nano Image: Using original user prompt')
      }
    }
    
    // Fallback: Check for old-style connection (no handle specified)
    if (referenceImages.length === 0) {
      const imageNode =
        incoming.find((node) => node.type === 'imageImport') ??
        incoming.find((node) => node.type === 'nanoImage') ??
        incoming.find((node) => node.type === 'gridComposer') ??
        incoming.find((node) => node.type === 'cellRegenerator')

      let inputImageDataUrl: string | undefined
      
      if (imageNode?.type === 'imageImport') {
        inputImageDataUrl = (imageNode.data as ImageImportNodeData).imageDataUrl
      } else if (imageNode?.type === 'nanoImage') {
        inputImageDataUrl = (imageNode.data as NanoImageNodeData).outputImageDataUrl
      } else if (imageNode?.type === 'gridComposer') {
        const imgData = imageNode.data as any
        inputImageDataUrl = imgData.composedImageDataUrl || imgData.composedImageUrl
      } else if (imageNode?.type === 'cellRegenerator') {
        // For cell regenerator without specific handle, we can't determine which cell to use
        // User should use specific cell output handles (S1, S2, etc.)
        inputImageDataUrl = undefined
      }
      
      const referencePrompt =
        imageNode?.type === 'imageImport'
          ? getIncomingTextPrompt(imageNode.id, edges, get().nodes) ??
            (imageNode.data as ImageImportNodeData).referencePrompt
          : undefined
      
      if (inputImageDataUrl) {
        // üî• Convert idb: or s3: reference to actual DataURL
        if (inputImageDataUrl.startsWith('idb:') || inputImageDataUrl.startsWith('s3:')) {
          try {
            const actualDataUrl = await getImage(inputImageDataUrl)
            if (actualDataUrl) {
              referenceImages.push(actualDataUrl)
            } else {
              console.warn(`‚ö†Ô∏è Failed to load fallback reference image: ${inputImageDataUrl}`)
            }
          } catch (error) {
            console.error(`‚ùå Error loading fallback reference image: ${inputImageDataUrl}`, error)
          }
        } else {
          referenceImages.push(inputImageDataUrl)
        }
        
        if (referencePrompt) {
          referencePrompts.push(`Reference 1: ${referencePrompt}`)
        }
      }
    }

    if (!prompt.trim()) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'ÌÖçÏä§Ìä∏ ÌîÑÎ°¨ÌîÑÌä∏ ÎÖ∏ÎìúÎ•º Ïó∞Í≤∞Ìï¥ Ï£ºÏÑ∏Ïöî.',
      }))
      return
    }

    const apiKey = get().apiKey || import.meta.env.VITE_GEMINI_API_KEY || ''
    
    if (!apiKey) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'Gemini API KeyÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§. ÏÉÅÎã® "API Key" Î≤ÑÌäºÏùÑ ÎàåÎü¨ÏÑú ÏÑ§Ï†ïÌïòÏÑ∏Ïöî.',
      }))
      return
    }

    updateNode((prev) => ({
      ...prev,
      status: 'processing',
      error: undefined,
      lastExecutionTime: now,
    }))

    const client = new GeminiAPIClient(apiKey)

    try {
      // Check if aborted before starting
      if (abortController.signal.aborted) {
        throw new Error('ÏûëÏóÖÏù¥ Ï∑®ÏÜåÎêòÏóàÏäµÎãàÎã§.')
      }

      // Check if Grid Composer + LLM Prompt are connected
      const gridComposerEdge = edges.find(e => e.target === id && get().nodes.find(n => n.id === e.source)?.type === 'gridComposer')
      const gridComposerNode = gridComposerEdge ? get().nodes.find(n => n.id === gridComposerEdge.source) : null
      const hasGridComposerRef = referenceImages.length > 0 && !!gridComposerEdge
      
      const llmPromptEdge = edges.find(e => 
        e.target === id && 
        e.targetHandle === 'prompt' && 
        get().nodes.find(n => n.id === e.source)?.type === 'llmPrompt'
      )
      const llmPromptNode = llmPromptEdge ? get().nodes.find(n => n.id === llmPromptEdge.source) : null
      const hasLLMPrompt = !!llmPromptNode
      
      // Get reference mode from LLM Prompt Helper
      const referenceMode = (llmPromptNode?.data as any)?.referenceMode || 'exact'
      
      // Extract Grid Composer label info (for Nano Banana)
      let gridLabelInfoForNano = ''
      if (gridComposerNode && gridComposerNode.type === 'gridComposer') {
        const gridData = gridComposerNode.data as any
        if (gridData.inputImages && gridData.slots) {
          const layout = gridData.gridLayout || '1x3'
          const slots = gridData.slots as Array<{ id: string; label: string; metadata?: string }>
          
          const slotDescriptions = slots
            .filter(slot => gridData.inputImages[slot.id])
            .map((slot, index) => {
              const position = ['Ï≤´ Î≤àÏß∏', 'Îëê Î≤àÏß∏', 'ÏÑ∏ Î≤àÏß∏', 'ÎÑ§ Î≤àÏß∏', 'Îã§ÏÑØ Î≤àÏß∏', 'Ïó¨ÏÑØ Î≤àÏß∏'][index] || `${index + 1}Î≤àÏß∏`
              let description = `- ${position} Ï∞∏Í≥† ÏöîÏÜå (${slot.id}): ${slot.label}`
              if (slot.metadata && slot.metadata.trim()) {
                description += ` - ${slot.metadata}`
              }
              return description
            })
            .join('\n')
          
          if (slotDescriptions) {
            gridLabelInfoForNano = `\n\nüìã Ï∞∏Í≥† Ïù¥ÎØ∏ÏßÄ Íµ¨ÏÑ± (${layout} Í∑∏Î¶¨Îìú):\n${slotDescriptions}\n\n`
          }
        }
      }
      
      // Check if Motion Prompt is connected (for camera transformation)
      const motionPromptEdge = edges.find(e => 
        e.target === id && 
        e.targetHandle === 'prompt' && 
        get().nodes.find(n => n.id === e.source)?.type === 'motionPrompt'
      )
      const motionPromptNode = motionPromptEdge ? get().nodes.find(n => n.id === motionPromptEdge.source) : null
      const hasMotionPrompt = !!motionPromptNode
      
      // Add reference prompts to main prompt if available
      let enhancedPrompt = referencePrompts.length > 0
        ? `${prompt}\n\n${referencePrompts.join('\n')}`
        : prompt
      
      // üé• Motion Prompt: Apply camera transformation (with or without reference image)
      if (hasMotionPrompt) {
        const motionData = motionPromptNode?.data as MotionPromptNodeData
        const hasCameraMovement = 
          (motionData.cameraRotation && motionData.cameraRotation !== 0) ||
          (motionData.cameraTilt && motionData.cameraTilt !== 0) ||
          (motionData.cameraDistance && motionData.cameraDistance !== 1.0)
        
        if (hasCameraMovement) {
          // 360ÎèÑ ÏãúÏä§ÌÖú: Í∞ÅÎèÑÎ≥Ñ ÌäπÎ≥Ñ Ï≤òÎ¶¨
          const rotation = motionData.cameraRotation || 0
          const normalizedRotation = Math.round(((rotation % 360) + 360) % 360)
          
          let cameraDescription = ''
          let shotType = ''
          let lensType = ''
          let angleDetails = ''
          
          // üé¨ Rotation Ìï¥ÏÑù (Google Gemini Photography Terminology)
          if (normalizedRotation === 0 || normalizedRotation === 360) {
            shotType = 'straight-on frontal shot'
            lensType = '50mm standard lens'
            angleDetails = 'Camera positioned directly in front of subject at eye level, neutral perspective'
          } else if (normalizedRotation > 0 && normalizedRotation <= 30) {
            shotType = 'slight three-quarter left view'
            lensType = '85mm portrait lens'
            angleDetails = `Subject's LEFT side slightly visible, frontal composition dominant (${normalizedRotation}¬∞ counterclockwise from front)`
          } else if (normalizedRotation > 30 && normalizedRotation < 60) {
            shotType = 'three-quarter left shot'
            lensType = '85mm portrait lens'
            angleDetails = `Balanced composition showing subject's LEFT side and front face (${normalizedRotation}¬∞ counterclockwise from front)`
          } else if (normalizedRotation >= 60 && normalizedRotation < 90) {
            shotType = 'left side three-quarter view'
            lensType = '85mm portrait lens'
            angleDetails = `Subject's LEFT side dominant, approaching profile perspective (${normalizedRotation}¬∞ counterclockwise from front)`
          } else if (normalizedRotation === 90) {
            shotType = 'left side profile shot'
            lensType = '85mm portrait lens'
            angleDetails = `‚ö†Ô∏è PERPENDICULAR SIDE VIEW: Camera positioned 90¬∞ to subject's left, showing ONLY left profile, NO frontal face visible`
          } else if (normalizedRotation > 90 && normalizedRotation < 120) {
            shotType = 'left three-quarter back view'
            lensType = '50mm standard lens'
            angleDetails = `Subject's back and LEFT side visible, NO frontal face (${normalizedRotation}¬∞ counterclockwise from front)`
          } else if (normalizedRotation >= 120 && normalizedRotation < 165) {
            shotType = 'three-quarter back shot from left'
            lensType = '50mm standard lens'
            angleDetails = `Subject's back dominant with LEFT side visible (${normalizedRotation}¬∞ counterclockwise from front)`
          } else if (normalizedRotation >= 165 && normalizedRotation <= 195) {
            shotType = 'back view shot'
            lensType = '50mm standard lens'
            angleDetails = `‚ö†Ô∏è REAR VIEW: Camera positioned directly behind subject at 180¬∞, subject facing AWAY from camera`
          } else if (normalizedRotation > 195 && normalizedRotation < 240) {
            shotType = 'three-quarter back shot from right'
            lensType = '50mm standard lens'
            angleDetails = `Subject's back dominant with RIGHT side visible (${normalizedRotation}¬∞ counterclockwise from front)`
          } else if (normalizedRotation >= 240 && normalizedRotation < 270) {
            shotType = 'right three-quarter back view'
            lensType = '50mm standard lens'
            angleDetails = `Subject's back and RIGHT side visible, NO frontal face (${normalizedRotation}¬∞ counterclockwise from front)`
          } else if (normalizedRotation === 270) {
            shotType = 'right side profile shot'
            lensType = '85mm portrait lens'
            angleDetails = `‚ö†Ô∏è PERPENDICULAR SIDE VIEW: Camera positioned 90¬∞ to subject's right, showing ONLY right profile, NO frontal face visible`
          } else if (normalizedRotation > 270 && normalizedRotation < 300) {
            shotType = 'right side three-quarter view'
            lensType = '85mm portrait lens'
            angleDetails = `Subject's RIGHT side dominant, approaching profile perspective (${normalizedRotation}¬∞ counterclockwise from front)`
          } else if (normalizedRotation >= 300 && normalizedRotation < 330) {
            shotType = 'three-quarter right shot'
            lensType = '85mm portrait lens'
            angleDetails = `Balanced composition showing subject's RIGHT side and front face (${normalizedRotation}¬∞ counterclockwise from front)`
          } else {
            shotType = 'slight three-quarter right view'
            lensType = '85mm portrait lens'
            angleDetails = `Subject's RIGHT side slightly visible, frontal composition dominant (${normalizedRotation}¬∞ counterclockwise from front)`
          }
          
          cameraDescription += `üì∑ SHOT TYPE: ${shotType}\n`
          cameraDescription += `üé• LENS: ${lensType}\n`
          cameraDescription += `üìê ANGLE: ${angleDetails}\n`
          
          // üé¨ Tilt Ìï¥ÏÑù (Photography Perspective Terms)
          let perspectiveType = ''
          if (motionData.cameraTilt && motionData.cameraTilt !== 0) {
            const tiltRounded = Math.round(Math.abs(motionData.cameraTilt))
            if (motionData.cameraTilt > 0) {
              perspectiveType = 'high-angle perspective'
              cameraDescription += `üì∑ PERSPECTIVE: ${perspectiveType} (+${tiltRounded}¬∞)\n`
              cameraDescription += `   Camera positioned ABOVE subject, looking DOWN at ${tiltRounded}¬∞ angle below horizontal\n`
              cameraDescription += '   Creates diminishing, vulnerable framing (subject appears smaller)\n'
            } else {
              perspectiveType = 'low-angle perspective'
              cameraDescription += `üì∑ PERSPECTIVE: ${perspectiveType} (-${tiltRounded}¬∞)\n`
              cameraDescription += `   Camera positioned BELOW subject, looking UP at ${tiltRounded}¬∞ angle above horizontal\n`
              cameraDescription += '   Creates empowering, heroic framing (subject appears larger/dominant)\n'
            }
          }
          
          // üé¨ Distance Ìï¥ÏÑù (Cinematography Framing Terms)
          let framingType = ''
          if (motionData.cameraDistance && motionData.cameraDistance !== 1.0) {
            const distRounded = Math.round(motionData.cameraDistance * 100) / 100
            if (motionData.cameraDistance > 1.0) {
              if (distRounded >= 2.0) {
                framingType = 'wide shot'
              } else if (distRounded >= 1.5) {
                framingType = 'medium-wide shot'
              } else {
                framingType = 'medium shot'
              }
              cameraDescription += `üì∑ FRAMING: ${framingType} (${distRounded}x distance)\n`
              cameraDescription += `   Camera positioned farther away, showing more environment and context\n`
            } else {
              if (distRounded <= 0.5) {
                framingType = 'extreme close-up'
              } else if (distRounded <= 0.7) {
                framingType = 'close-up shot'
              } else {
                framingType = 'medium close-up'
              }
              cameraDescription += `üì∑ FRAMING: ${framingType} (${distRounded}x distance)\n`
              cameraDescription += `   Camera positioned closer, tight framing emphasizing details and expressions\n`
            }
          }
          
          // üé¨ Rotation Subject Ìï¥ÏÑù (Cinematography Method)
          if (normalizedRotation !== 0 && normalizedRotation !== 360) {
            if (motionData.rotationSubject === 'camera-orbit') {
              cameraDescription += `\nüé¨ CAMERA MOVEMENT: Orbital tracking shot (camera circles around stationary subject)\n`
              cameraDescription += '   ‚ö†Ô∏è CRITICAL TECHNIQUE: Camera physically moves around subject on circular dolly/track\n'
              cameraDescription += '   ‚ö†Ô∏è Subject remains STATIONARY - maintains same body orientation and facing direction\n'
              cameraDescription += '   ‚ö†Ô∏è Only camera position changes (like photographer walking around statue)\n'
              cameraDescription += '   ‚ö†Ô∏è Background perspective shifts - environment visible from new camera angle\n'
              cameraDescription += '   ‚ö†Ô∏è Creates parallax effect - foreground/background move at different rates\n'
              cameraDescription += '   üí° CINEMA REFERENCE: The Matrix "bullet time", Inception hallway fight camera work\n'
            } else if (motionData.rotationSubject === 'character-turn') {
              cameraDescription += `\nüßç SUBJECT MOVEMENT: Character turns/rotates body (camera stays fixed)\n`
              cameraDescription += '   ‚ö†Ô∏è CRITICAL TECHNIQUE: Subject physically rotates their body to face new direction\n'
              cameraDescription += '   ‚ö†Ô∏è Camera remains STATIONARY - fixed position (typically frontal)\n'
              cameraDescription += '   ‚ö†Ô∏è Subject turns like person rotating on turntable or reacting to sound\n'
              cameraDescription += '   ‚ö†Ô∏è Background stays FIXED - no perspective change, no parallax\n'
              cameraDescription += '   ‚ö†Ô∏è Environment remains static from same camera viewpoint\n'
              cameraDescription += '   üí° CINEMA REFERENCE: Character turning to face someone off-camera, "slow turn reveal" shots\n'
            }
          }
          
          // üé¨ Google Gemini Í≥µÏãù ÌÖúÌîåÎ¶ø Ï†ÅÏö©
          // Reference ImageÍ∞Ä ÏûàÏúºÎ©¥ Ï∫êÎ¶≠ÌÑ∞ ÏùºÍ¥ÄÏÑ± + Ïπ¥Î©îÎùº, ÏóÜÏúºÎ©¥ ÏàúÏàò Ïπ¥Î©îÎùº Í∞ÅÎèÑ
          if (referenceImages.length > 0) {
            // WITH REFERENCE: Character Consistency + Camera Transformation
            enhancedPrompt = `A photorealistic image of the subject from the reference image, maintaining EXACT character consistency.

üé• CAMERA SPECIFICATIONS (Google Gemini Format):
${cameraDescription}

üì∏ CAPTURED WITH:
Captured with ${lensType}, ${perspectiveType || 'natural eye-level perspective'}, ${framingType || 'medium framing'}.
Professional photography lighting, emphasizing character details and textures.
High-quality rendering with accurate depth of field and natural bokeh.

‚ö†Ô∏è CRITICAL REQUIREMENTS (PRIORITY ORDER):

1Ô∏è‚É£ CHARACTER CONSISTENCY (HIGHEST PRIORITY):
   ‚úÖ PRESERVE EXACTLY from reference image:
   - Facial features, hair style/color, eye color, skin tone, expressions
   - Clothing design, outfit colors, materials, textures, accessories
   - Body proportions, build, height, posture characteristics
   - Visual style, rendering quality, artistic treatment
   - Color palette, tone, mood, lighting quality
   
   üö´ FORBIDDEN:
   - Changing character appearance or identity
   - Altering outfit design or colors
   - Modifying visual style or rendering quality

2Ô∏è‚É£ CAMERA TRANSFORMATION (SECOND PRIORITY):
   ‚úÖ APPLY EXACT CAMERA SPECIFICATIONS above:
   - Follow shot type and lens specifications precisely
   - Implement exact angle and perspective described
   - Apply specified framing and composition
   - For side views (90¬∞/270¬∞): Show ONLY profile, NO frontal face
   - For back view (180¬∞): Subject faces AWAY, show back only
   
   üö´ FORBIDDEN:
   - Keeping same camera angle as reference
   - Approximating angles (${normalizedRotation}¬∞ is precise)
   - Ignoring perspective/framing specifications

üí° PHOTOGRAPHY ANALOGY:
You are photographing the SAME character from a DIFFERENT camera position.
Think: Professional photographer shooting model from new angle.
- Character = 100% IDENTICAL to reference
- Camera = MOVED to new position as specified

${prompt}

Generate the image with PERFECT character consistency and EXACT camera transformation as specified.`
          } else {
            // WITHOUT REFERENCE: Pure Camera Angle Specification
            enhancedPrompt = `A photorealistic image following precise camera specifications.

üé• CAMERA SPECIFICATIONS (Google Gemini Format):
${cameraDescription}

üì∏ CAPTURED WITH:
Captured with ${lensType}, ${perspectiveType || 'natural eye-level perspective'}, ${framingType || 'medium framing'}.
Professional cinematic lighting creating a compelling atmosphere.
High-quality rendering with accurate depth of field and natural bokeh effect.

‚ö†Ô∏è CRITICAL CAMERA REQUIREMENTS:

‚úÖ MANDATORY EXECUTION:
- STRICTLY follow shot type and lens specifications above
- PRECISELY implement the angle described (${normalizedRotation}¬∞ is exact)
- ACCURATELY apply perspective and framing specified
- For frontal (0¬∞): Straight-on view, eye level, balanced composition
- For three-quarter views: Show both side and front in balanced mix
- For side views (90¬∞/270¬∞): PERPENDICULAR - show COMPLETE side profile, NO frontal face
- For back view (180¬∞): Subject facing AWAY from camera, rear view only
- Use cinematic composition principles and rule of thirds

üö´ FORBIDDEN:
- Ignoring camera specifications
- Using default/standard camera angles instead of specified angles
- Approximating angles (${normalizedRotation}¬∞ must be precise)
- Showing frontal face when side/back view specified
- Deviating from lens or perspective specifications

üí° PHOTOGRAPHY DIRECTION:
The camera angle/position is the PRIMARY creative requirement.
Generate the subject/scene from THIS EXACT camera perspective.
Think: Professional cinematographer executing precise camera placement.

${prompt}

Generate the image from the EXACT camera position, angle, lens, and framing specified above.`
          }
        }
      }
      // üéØ Grid Composer + LLM: Ï∞∏Ï°∞ Ï†ïÌôïÎèÑÏóê Îî∞Îùº ÏßÄÏãú Ï∂îÍ∞Ä
      else if (hasGridComposerRef && hasLLMPrompt) {
        if (referenceMode === 'exact') {
          // Ï†ïÌôïÏÑ± Î™®Îìú: Ï∞∏Ï°∞ Ïù¥ÎØ∏ÏßÄ PIXEL-LEVEL Î≥µÏ†ú
          enhancedPrompt = `‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è CRITICAL: EXACT REFERENCE IMAGE REPLICATION REQUIRED ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è
${gridLabelInfoForNano}
STRICT MODE: Reference image is ABSOLUTE PRIMARY source.
Text prompt = ONLY for understanding story/actions. Your task = PIXEL-PERFECT VISUAL COPY.

üìå TEXT PROMPT = STORY/ACTIONS (PRESERVE 100%):
- If text says "holding helmet" ‚Üí Generate "holding helmet" (NOT "wearing helmet"!)
- If text says "walking" ‚Üí Generate "walking" (keep action!)
- If text says "one person" ‚Üí Generate "one person" (NOT "two"!)
- Preserve ALL actions, character counts, story elements from text prompt

üé® REFERENCE IMAGE = VISUAL DESIGN (REPLICATE 100%):
- S1 Background: Copy EXACT colors, lighting, structure, materials
- S2 Character: Copy EXACT appearance, outfit, hair, facial features
- S3 Object/Robot: Copy EXACT colors (red=red, white=white), shape, design
- Use reference for HOW things LOOK, use text for WHAT is HAPPENING

üö´ ABSOLUTELY FORBIDDEN:
- Changing actions ("holding" ‚Üí "wearing", "walking" ‚Üí "standing")
- Changing character count ("one" ‚Üí "two")
- Reinterpreting background (S1 must match EXACTLY!)
- ANY color changes (red‚Üíred, blue‚Üíblue, white‚Üíwhite, black‚Üíblack)
- ANY material changes (metal‚Üímetal, fabric‚Üífabric, plastic‚Üíplastic)
- ANY shape, proportion, design modifications
- ANY creative variations or "similar" versions
- ANY artistic interpretation of reference visuals

‚úÖ MANDATORY REQUIREMENTS:
- EXACT pixel-by-pixel visual replication of S1, S2, S3 elements
- 100% color preservation (use exact RGB values from reference)
- 100% material/texture preservation from reference
- 100% lighting/shadow preservation from reference
- 100% action/story preservation from text prompt
- If text says "holding helmet", person MUST be holding (not wearing) helmet
- Background MUST match S1 reference exactly
- Reference visuals = TOP PRIORITY for appearance
- Text prompt = TOP PRIORITY for actions/story

Extract visual characteristics from each labeled section and replicate EXACTLY. Use text prompt ONLY for understanding actions and story flow.

---

${enhancedPrompt}`
        } else if (referenceMode === 'balanced') {
          // Í∑†Ìòï Î™®Îìú: ÌÖçÏä§Ìä∏ÏôÄ Ïù¥ÎØ∏ÏßÄ Í∑†Ìòï
          enhancedPrompt = `‚öñÔ∏è BALANCED MODE: Reference Image + Text Prompt
${gridLabelInfoForNano}
Use reference image AND text prompt equally:
- Reference image: Visual style, colors, materials, composition of each labeled element
- Text prompt: Specific details, arrangement, actions, story

‚ö†Ô∏è IMPORTANT: Preserve actions from text prompt (e.g., "holding" stays "holding", not "wearing").

Maintain visual consistency with reference while incorporating text details.

---

${enhancedPrompt}`
        } else if (referenceMode === 'creative') {
          // Ï∞ΩÏùòÏÑ± Î™®Îìú: ÌÖçÏä§Ìä∏ ÏúÑÏ£º, Ïù¥ÎØ∏ÏßÄÎäî ÏòÅÍ∞êÎßå
          enhancedPrompt = `üé® CREATIVE MODE: Text Prompt Primary
${gridLabelInfoForNano}
Focus on text prompt as main instruction.
Reference image = INSPIRATION ONLY (style, mood, general aesthetic).

Feel free to creatively interpret and generate based on text description.

---

${enhancedPrompt}`
        }
      }
      
      const model = data.model ?? 'gemini-3-pro-image-preview'
      
      console.log('üé® Nano Image Generation:', {
        model,
        resolution: data.resolution,
        aspectRatio: data.aspectRatio,
        referenceCount: referenceImages.length,
        referenceMode: hasGridComposerRef ? referenceMode : 'N/A',
      })
      
      // Use first reference image as primary input (for backward compatibility)
      const primaryReference = referenceImages[0]
      
      // ‚úÖ Apply retry logic with exponential backoff
      const result = await retryWithBackoff(
        () => client.generateImage(
          enhancedPrompt,
          data.aspectRatio,
          primaryReference,
          model,
          data.resolution,
          abortController.signal,
        ),
        {
          maxAttempts: 3,
          initialDelay: 1000,
          onRetry: (attempt, error) => {
            console.warn(`üîÑ Retry attempt ${attempt}:`, error.message)
            updateNode((prev) => ({
              ...prev,
              error: `Ïû¨ÏãúÎèÑ Ï§ë... (${attempt}/3)`,
            }))
          },
        }
      )

      // Check if aborted after completion
      if (abortController.signal.aborted) {
        throw new Error('ÏûëÏóÖÏù¥ Ï∑®ÏÜåÎêòÏóàÏäµÎãàÎã§.')
      }

      // üî• IndexedDB/S3Ïóê Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•ÌïòÍ≥† Ï∞∏Ï°∞ Î∞òÌôò
      let savedImageRef = result.imageDataUrl
      try {
        const imageId = `nano-output-${id}-${Date.now()}`
        console.log('üíæ Nano Image: IndexedDB/S3Ïóê Ï∂úÎ†• Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû• ÏãúÏûë...', imageId)
        
        savedImageRef = await saveImage(imageId, result.imageDataUrl, id, true)
        console.log('‚úÖ Nano Image: Ï∂úÎ†• Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû• ÏôÑÎ£å', savedImageRef)
      } catch (error) {
        console.error('‚ùå Nano Image: Ï∂úÎ†• Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû• Ïã§Ìå®, DataURLÏùÑ ÏßÅÏ†ë ÏÇ¨Ïö©', error)
        // Ìè¥Î∞±: DataURLÏùÑ ÏßÅÏ†ë ÏÇ¨Ïö© (ÎπÑÍ∂åÏû•)
      }

      updateNode((prev) => ({
        ...prev,
        status: 'completed',
        outputImageUrl: result.imageUrl,
        outputImageDataUrl: savedImageRef, // idb:xxx ÎòêÎäî s3:xxx Ï∞∏Ï°∞
        generatedModel: model,
        generatedResolution: data.resolution,
        generatedAspectRatio: data.aspectRatio,
        error: undefined,
      }))
    } catch (error) {
      // Don't show retry errors if aborted
      if (!abortController.signal.aborted) {
        updateNode((prev) => ({
          ...prev,
          status: 'error',
          error: formatErrorMessage(error),
        }))
      }
    } finally {
      // Clean up abort controller
      const controllers = get().abortControllers
      controllers.delete(id)
      set({ abortControllers: new Map(controllers) })
    }
  },
  runGeminiNode: async (id) => {
    const { nodes, edges, abortControllers } = get()
    const current = nodes.find((node) => node.id === id)
    if (!current || current.type !== 'geminiVideo') return

    // ‚úÖ Prevent duplicate execution
    const currentData = current.data as GeminiVideoNodeData
    if (currentData.status === 'processing') {
      console.warn('‚ö†Ô∏è Gemini node is already processing')
      return
    }
    
    // üßπ Ïù¥Ï†Ñ ÏóêÎü¨ Î®ºÏ†Ä ÏßÄÏö∞Í∏∞
    set({
      nodes: get().nodes.map((node) =>
        node.id === id
          ? { ...node, data: { ...node.data, error: undefined } }
          : node
      ),
    })

    // ‚úÖ Rate limiting
    const now = Date.now()
    const lastExecution = (currentData as any).lastExecutionTime || 0
    const minInterval = 5000 // 5 seconds for video (longer than image)
    
    if (now - lastExecution < minInterval) {
      const waitTime = Math.ceil((minInterval - (now - lastExecution)) / 1000)
      set({
        nodes: nodes.map((node) =>
          node.id === id
            ? {
                ...node,
                data: {
                  ...node.data,
                  status: 'error',
                  error: `ÎÑàÎ¨¥ Îπ†Î•¥Í≤å Ïã§ÌñâÌñàÏäµÎãàÎã§. ${waitTime}Ï¥à ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî.`,
                },
              }
            : node,
        ),
      })
      return
    }

    // Create abort controller for this execution
    const abortController = new AbortController()
    abortControllers.set(id, abortController)
    set({ abortControllers: new Map(abortControllers) })

    const updateNode = (updater: (data: NodeData) => NodeData) => {
      set({
        nodes: get().nodes.map((node) =>
          node.id === id ? { ...node, data: updater(node.data) } : node,
        ),
      })
    }

    const incoming = getIncomingNodes(id, edges, get().nodes)
    const imageNode =
      incoming.find((node) => node.type === 'imageImport') ??
      incoming.find((node) => node.type === 'nanoImage') ??
      incoming.find((node) => node.type === 'gridComposer') ??
      incoming.find((node) => node.type === 'cellRegenerator')
    const promptNode = incoming.find(
      (node) => node.type === 'motionPrompt' || node.type === 'textPrompt' || node.type === 'llmPrompt',
    )

    let inputImageUrl: string | undefined
    let inputImageDataUrl: string | undefined
    
    if (imageNode?.type === 'imageImport') {
      inputImageUrl = (imageNode.data as ImageImportNodeData).imageUrl
      inputImageDataUrl = (imageNode.data as ImageImportNodeData).imageDataUrl
    } else if (imageNode?.type === 'nanoImage') {
      inputImageUrl = (imageNode.data as NanoImageNodeData).outputImageUrl
      inputImageDataUrl = (imageNode.data as NanoImageNodeData).outputImageDataUrl
    } else if (imageNode?.type === 'gridComposer') {
      const imgData = imageNode.data as any
      inputImageUrl = imgData.composedImageUrl || imgData.composedImageDataUrl
      inputImageDataUrl = imgData.composedImageDataUrl || imgData.composedImageUrl
    } else if (imageNode?.type === 'cellRegenerator') {
      // Cell Regenerator should use specific cell outputs
      inputImageUrl = undefined
      inputImageDataUrl = undefined
    }

    const inputPrompt =
      promptNode?.type === 'motionPrompt'
        ? (promptNode.data as MotionPromptNodeData).combinedPrompt
        : promptNode?.type === 'textPrompt'
          ? (promptNode.data as TextPromptNodeData).prompt
          : promptNode?.type === 'llmPrompt'
            ? (promptNode.data as any).outputPrompt || ''
            : ''

    // ‚ö†Ô∏è Early validation BEFORE storage conversion (only check if nodes are connected)
    if (!inputPrompt) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'ÌîÑÎ°¨ÌîÑÌä∏ ÎÖ∏ÎìúÎ•º Ïó∞Í≤∞Ìï¥ Ï£ºÏÑ∏Ïöî.',
      }))
      return
    }
    
    if (!imageNode) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'Ïù¥ÎØ∏ÏßÄ ÎÖ∏ÎìúÎ•º Ïó∞Í≤∞Ìï¥ Ï£ºÏÑ∏Ïöî.',
      }))
      return
    }
    
    if (!inputImageDataUrl) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'Ïù¥ÎØ∏ÏßÄ ÎÖ∏ÎìúÍ∞Ä Ïó∞Í≤∞ÎêòÏóàÏßÄÎßå Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏÉùÏÑ±ÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. Ïù¥ÎØ∏ÏßÄ ÎÖ∏ÎìúÏóêÏÑú "Generate" Î≤ÑÌäºÏùÑ ÎàåÎü¨Ï£ºÏÑ∏Ïöî.',
      }))
      return
    }

    const apiKey = get().apiKey || import.meta.env.VITE_GEMINI_API_KEY || ''
    
    if (!apiKey) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'Gemini API KeyÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§. ÏÉÅÎã® "API Key" Î≤ÑÌäºÏùÑ ÎàåÎü¨ÏÑú ÏÑ§Ï†ïÌïòÏÑ∏Ïöî.',
      }))
      return
    }

    updateNode((prev) => ({
      ...prev,
      status: 'processing',
      error: undefined,
      inputImageUrl,
      inputImageDataUrl,
      inputPrompt,
      progress: 10,
      lastExecutionTime: now,
    }))

    const client = new GeminiAPIClient(apiKey)
    
    // ‚úÖ Convert storage references to actual DataURLs
    let actualInputImageDataUrl = inputImageDataUrl
    
    console.log('üîç Gemini: Input image type:', inputImageDataUrl?.substring(0, 50))
    
    if (inputImageDataUrl && (inputImageDataUrl.startsWith('idb:') || inputImageDataUrl.startsWith('s3:'))) {
      console.log('üîÑ Gemini: Converting image from storage reference...')
      try {
        const { getImage } = await import('../utils/indexedDB')
        const dataURL = await getImage(inputImageDataUrl)
        if (dataURL) {
          actualInputImageDataUrl = dataURL
          console.log('‚úÖ Gemini: Image loaded from storage, size:', dataURL.length, 'chars')
        } else {
          console.error('‚ùå Gemini: Failed to load image from storage')
          // üî• Fallback: Try to use inputImageUrl (blob URL) if available
          if (inputImageUrl && inputImageUrl.startsWith('data:')) {
            console.warn('‚ö†Ô∏è Gemini: Falling back to inputImageUrl (DataURL)')
            actualInputImageDataUrl = inputImageUrl
          } else if (inputImageUrl && inputImageUrl.startsWith('blob:')) {
            console.warn('‚ö†Ô∏è Gemini: inputImageUrl is a blob URL (may be invalid after refresh)')
            updateNode((prev) => ({
              ...prev,
              status: 'error',
              error: 'Ïù¥ÎØ∏ÏßÄÎ•º StorageÏóêÏÑú Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§. Ïù¥ÎØ∏ÏßÄÎ•º Îã§Ïãú ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.',
            }))
            return
          } else {
            updateNode((prev) => ({
              ...prev,
              status: 'error',
              error: 'Ïù¥ÎØ∏ÏßÄÎ•º StorageÏóêÏÑú Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§. Ïù¥ÎØ∏ÏßÄÎ•º Îã§Ïãú ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.',
            }))
            return
          }
        }
      } catch (error) {
        console.error('‚ùå Gemini: Error loading image:', error)
        updateNode((prev) => ({
          ...prev,
          status: 'error',
          error: `Ïù¥ÎØ∏ÏßÄ Î°úÎìú Ïã§Ìå®: ${error}`,
        }))
        return
      }
    } else if (!inputImageDataUrl) {
      console.error('‚ùå Gemini: No input image provided!')
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄÍ∞Ä Ï†úÍ≥µÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.',
      }))
      return
    } else {
      console.log('‚úÖ Gemini: Using direct DataURL (not a storage reference)')
    }
    
    console.log('üé¨ Gemini Video ÏÉùÏÑ± ÏãúÏûë:', {
      prompt: inputPrompt.substring(0, 50) + '...',
      model: (current.data as GeminiVideoNodeData).model,
      imageType: actualInputImageDataUrl?.substring(0, 30),
      imageSize: actualInputImageDataUrl?.length,
    })
    
    // ‚úÖ Final validation before API call
    if (!actualInputImageDataUrl || actualInputImageDataUrl.startsWith('idb:') || actualInputImageDataUrl.startsWith('s3:')) {
      console.error('‚ùå Gemini: Image is still a storage reference or empty!', actualInputImageDataUrl?.substring(0, 50))
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'Ïù¥ÎØ∏ÏßÄ Î≥ÄÌôò Ïã§Ìå®. Storage Ï∞∏Ï°∞Í∞Ä ÎÇ®ÏïÑÏûàÏäµÎãàÎã§.',
      }))
      return
    }
    
    console.log('‚úÖ Gemini: All validations passed, calling API...')

    const progressTimer = setInterval(() => {
      updateNode((prev) => {
        const data = prev as GeminiVideoNodeData
        if (data.status !== 'processing') return prev
        return {
          ...prev,
          progress: Math.min(data.progress + 12, 90),
        }
      })
    }, 500)

    try {
      if (abortController.signal.aborted) {
        throw new Error('ÏûëÏóÖÏù¥ Ï∑®ÏÜåÎêòÏóàÏäµÎãàÎã§.')
      }

      const settings = current.data as GeminiVideoNodeData
      
      // ‚úÖ Apply retry logic
      const outputVideoUrl = await retryWithBackoff(
        () => client.generateMedia(
          inputPrompt,
          {
            mediaType: 'video',
            duration: settings.duration,
            quality: settings.quality,
            motionIntensity: settings.motionIntensity,
          },
          actualInputImageDataUrl,
          settings.model,
          abortController.signal,
        ),
        {
          maxAttempts: 2, // Less retries for video (expensive)
          initialDelay: 2000,
          onRetry: (attempt) => {
            console.warn(`üîÑ Gemini Video retry ${attempt}/2`)
            updateNode((prev) => ({
              ...prev,
              error: `Ïû¨ÏãúÎèÑ Ï§ë... (${attempt}/2)`,
            }))
          },
        }
      )

      if (abortController.signal.aborted) {
        throw new Error('ÏûëÏóÖÏù¥ Ï∑®ÏÜåÎêòÏóàÏäµÎãàÎã§.')
      }

      updateNode((prev) => ({
        ...prev,
        status: 'completed',
        outputVideoUrl,
        progress: 100,
        error: undefined,
      }))

    } catch (error) {
      if (!abortController.signal.aborted) {
        updateNode((prev) => ({
          ...prev,
          status: 'error',
          error: formatErrorMessage(error),
        }))
      }
    } finally {
      clearInterval(progressTimer)
      const controllers = get().abortControllers
      controllers.delete(id)
      set({ abortControllers: new Map(controllers) })
    }
  },
  runCellRegeneratorNode: async (id) => {
    const { nodes, edges } = get()
    const current = nodes.find((node) => node.id === id)
    if (!current || current.type !== 'cellRegenerator') return

    const data = current.data as any
    
    // Update to processing status
    set({
      nodes: nodes.map((node) =>
        node.id === id
          ? { ...node, data: { ...node.data, status: 'processing', error: undefined } }
          : node,
      ),
    })

    try {
      // 1. Validate inputs
      if (!data.gridLayout || !data.slots || data.slots.length === 0) {
        throw new Error('Grid Ï†ïÎ≥¥Í∞Ä ÏóÜÏäµÎãàÎã§. Grid NodeÎ•º Ïó∞Í≤∞Ìï¥Ï£ºÏÑ∏Ïöî.')
      }

      // 2. Get input image (resolve storage reference if needed)
      let inputImageDataUrl = data.inputImageDataUrl || data.inputImageUrl
      if (!inputImageDataUrl) {
        throw new Error('Grid Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏóÜÏäµÎãàÎã§. Ïù¥ÎØ∏ÏßÄÎ•º Ïó∞Í≤∞Ìï¥Ï£ºÏÑ∏Ïöî.')
      }

      // Resolve storage reference (idb: or s3:)
      if (inputImageDataUrl.startsWith('idb:') || inputImageDataUrl.startsWith('s3:')) {
        const resolved = await getImage(inputImageDataUrl)
        if (!resolved) {
          throw new Error('Ïù¥ÎØ∏ÏßÄÎ•º Î∂àÎü¨Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§.')
        }
        inputImageDataUrl = resolved
      }

      // 3. Parse grid layout
      const [rows, cols] = data.gridLayout.split('x').map(Number)
      if (!rows || !cols || rows < 1 || cols < 1) {
        throw new Error(`ÏûòÎ™ªÎêú Grid Layout: ${data.gridLayout}`)
      }

      console.log(`üî™ Cell Regenerator: Splitting ${rows}x${cols} grid into ${data.slots.length} cells`)

      // 4. Load image
      const img = new Image()
      img.crossOrigin = 'anonymous'
      
      await new Promise<void>((resolve, reject) => {
        img.onload = () => resolve()
        img.onerror = () => reject(new Error('Ïù¥ÎØ∏ÏßÄ Î°úÎìú Ïã§Ìå®'))
        img.src = inputImageDataUrl
      })

      console.log(`üìê Image loaded: ${img.width}x${img.height}`)

      // 5. Calculate cell dimensions
      const cellWidth = Math.floor(img.width / cols)
      const cellHeight = Math.floor(img.height / rows)

      console.log(`üì¶ Cell size: ${cellWidth}x${cellHeight}`)

      // 6. Split into cells
      const canvas = document.createElement('canvas')
      const ctx = canvas.getContext('2d')
      if (!ctx) {
        throw new Error('Canvas ÏÉùÏÑ± Ïã§Ìå®')
      }

      const regeneratedImages: { [key: string]: string } = {}

      for (let i = 0; i < data.slots.length; i++) {
        const slot = data.slots[i]
        const row = Math.floor(i / cols)
        const col = i % cols

        console.log(`üîç Processing ${slot.id}: row=${row}, col=${col}, i=${i}`)

        canvas.width = cellWidth
        canvas.height = cellHeight

        console.log(`üìê Canvas size: ${canvas.width}x${canvas.height}`)

        // Clear canvas
        ctx.clearRect(0, 0, cellWidth, cellHeight)

        // Calculate source coordinates
        const sx = col * cellWidth
        const sy = row * cellHeight

        console.log(`‚úÇÔ∏è Cutting from source: sx=${sx}, sy=${sy}, sw=${cellWidth}, sh=${cellHeight}`)

        // Draw cell from source image
        ctx.drawImage(
          img,
          sx,
          sy,
          cellWidth,
          cellHeight,
          0,
          0,
          cellWidth,
          cellHeight,
        )

        // Convert to data URL (JPEG for better compression)
        const dataUrl = canvas.toDataURL('image/jpeg', 0.9)

        console.log(`üíæ Generated dataURL length: ${dataUrl.length}`)

        // Save to storage
        const imageId = `cell-${id}-${slot.id}-${Date.now()}`
        const storageRef = await saveImage(imageId, dataUrl, id, true)

        regeneratedImages[slot.id] = storageRef
        console.log(`‚úÖ Cell ${slot.id} saved: ${storageRef}`)
      }

      // 7. Update node data
      set({
        nodes: nodes.map((node) =>
          node.id === id
            ? {
                ...node,
                data: {
                  ...node.data,
                  regeneratedImages,
                  status: 'success',
                  error: undefined,
                },
              }
            : node,
        ),
      })

      console.log(`‚úÖ Cell Regenerator: Successfully separated ${data.slots.length} cells`)
    } catch (error) {
      console.error('‚ùå Cell Regenerator error:', error)
      set({
        nodes: nodes.map((node) =>
          node.id === id
            ? {
                ...node,
                data: {
                  ...node.data,
                  status: 'error',
                  error: error instanceof Error ? error.message : String(error),
                },
              }
            : node,
        ),
      })
    }
  },
  runKlingNode: async (id) => {
    const { nodes, edges, abortControllers } = get()
    const current = nodes.find((node) => node.id === id)
    if (!current || current.type !== 'klingVideo') return

    // ‚úÖ Prevent duplicate execution
    const currentData = current.data as KlingVideoNodeData
    if (currentData.status === 'processing') {
      console.warn('‚ö†Ô∏è Kling node is already processing')
      return
    }

    // ‚úÖ Rate limiting
    const now = Date.now()
    const lastExecution = (currentData as any).lastExecutionTime || 0
    const minInterval = 5000 // 5 seconds for video
    
    if (now - lastExecution < minInterval) {
      const waitTime = Math.ceil((minInterval - (now - lastExecution)) / 1000)
      set({
        nodes: nodes.map((node) =>
          node.id === id
            ? {
                ...node,
                data: {
                  ...node.data,
                  status: 'error',
                  error: `ÎÑàÎ¨¥ Îπ†Î•¥Í≤å Ïã§ÌñâÌñàÏäµÎãàÎã§. ${waitTime}Ï¥à ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî.`,
                },
              }
            : node,
        ),
      })
      return
    }

    // Create abort controller for this execution
    const abortController = new AbortController()
    abortControllers.set(id, abortController)
    set({ abortControllers: new Map(abortControllers) })

    const updateNode = (updater: (data: NodeData) => NodeData) => {
      set({
        nodes: get().nodes.map((node) =>
          node.id === id ? { ...node, data: updater(node.data) } : node,
        ),
      })
    }

    const imageNodeTypes = new Set(['imageImport', 'nanoImage', 'gridComposer', 'cellRegenerator'])
    
    // Start Image (Í∏∞Î≥∏ Ïù¥ÎØ∏ÏßÄ) - 'start' Ìï∏Îì§ ÎòêÎäî Ìï∏Îì§ ID ÏóÜÎäî Ïó∞Í≤∞
    const startImageEdges = edges.filter(
      (e) => e.target === id && (!e.targetHandle || e.targetHandle === 'start')
    )
    const hasStartImageEdge = startImageEdges.length > 0
    
    const imageIncomingEdges = edges.filter((e) => {
      if (e.target !== id) return false
      const node = nodes.find((n) => n.id === e.source)
      return imageNodeTypes.has(node?.type ?? '')
    })
    
    console.log('üîç Kling: Start image edges found:', startImageEdges.length)
    
    const startImageNode = startImageEdges.find((e) => {
      const node = nodes.find((n) => n.id === e.source)
      console.log(`üîç Kling: Checking source node ${e.source} type: ${node?.type}`)
      return imageNodeTypes.has(node?.type ?? '')
    })
    
    const startImageNodeData = startImageNode ? nodes.find((n) => n.id === startImageNode.source) : undefined
    
    // End Image (ÎÅù ÌîÑÎ†àÏûÑ) - 'end' Ìï∏Îì§ Ïó∞Í≤∞
    const endImageEdges = edges.filter(
      (e) => e.target === id && e.targetHandle === 'end'
    )
    const endImageNode = endImageEdges.find((e) => {
      const node = nodes.find((n) => n.id === e.source)
      return node?.type === 'imageImport' || node?.type === 'nanoImage' || node?.type === 'gridComposer' || node?.type === 'cellRegenerator'
    })
    const endImageNodeData = endImageNode ? nodes.find((n) => n.id === endImageNode.source) : undefined

    // Prompt ÎÖ∏Îìú
    const incoming = getIncomingNodes(id, edges, get().nodes)
    const promptNode = incoming.find(
      (node) => node.type === 'motionPrompt' || node.type === 'textPrompt' || node.type === 'llmPrompt',
    )

    // Start Image Îç∞Ïù¥ÌÑ∞
    let inputImageUrl: string | undefined
    let inputImageDataUrl: string | undefined
    
    if (startImageNodeData?.type === 'imageImport') {
      const imgData = startImageNodeData.data as ImageImportNodeData
      inputImageUrl = imgData.imageUrl
      inputImageDataUrl = imgData.imageDataUrl
    } else if (startImageNodeData?.type === 'nanoImage') {
      const imgData = startImageNodeData.data as NanoImageNodeData
      inputImageUrl = imgData.outputImageUrl
      inputImageDataUrl = imgData.outputImageDataUrl
    } else if (startImageNodeData?.type === 'gridComposer') {
      const imgData = startImageNodeData.data as any
      inputImageUrl = imgData.composedImageUrl || imgData.composedImageDataUrl
      inputImageDataUrl = imgData.composedImageDataUrl || imgData.composedImageUrl
    } else if (startImageNodeData?.type === 'cellRegenerator') {
      const imgData = startImageNodeData.data as any
      // For cell regenerator, try to get image from sourceHandle (e.g., S1, S2, etc.)
      const cellId = startImageNode?.sourceHandle
      if (cellId && imgData.regeneratedImages?.[cellId]) {
        inputImageDataUrl = imgData.regeneratedImages[cellId]
        inputImageUrl = inputImageDataUrl
      } else {
        // Fallback: use first available cell
        const firstCellId = Object.keys(imgData.regeneratedImages || {})[0]
        if (firstCellId) {
          inputImageDataUrl = imgData.regeneratedImages[firstCellId]
          inputImageUrl = inputImageDataUrl
        }
      }
    }
    
    if (!inputImageDataUrl && inputImageUrl) {
      if (
        inputImageUrl.startsWith('data:') ||
        inputImageUrl.startsWith('idb:') ||
        inputImageUrl.startsWith('s3:')
      ) {
        inputImageDataUrl = inputImageUrl
      }
    }
    
    console.log('üîç Kling: Final start image data:', { 
      nodeType: startImageNodeData?.type,
      hasImageUrl: !!inputImageUrl,
      hasImageDataUrl: !!inputImageDataUrl,
      imageDataUrlPrefix: inputImageDataUrl?.substring(0, 20)
    })
    
    // End Image Îç∞Ïù¥ÌÑ∞
    let endImageUrl: string | undefined
    let endImageDataUrl: string | undefined
    
    if (endImageNodeData?.type === 'imageImport') {
      const imgData = endImageNodeData.data as ImageImportNodeData
      endImageUrl = imgData.imageUrl
      endImageDataUrl = imgData.imageDataUrl
    } else if (endImageNodeData?.type === 'nanoImage') {
      const imgData = endImageNodeData.data as NanoImageNodeData
      endImageUrl = imgData.outputImageUrl
      endImageDataUrl = imgData.outputImageDataUrl
    } else if (endImageNodeData?.type === 'gridComposer') {
      const imgData = endImageNodeData.data as any
      endImageUrl = imgData.composedImageUrl || imgData.composedImageDataUrl
      endImageDataUrl = imgData.composedImageDataUrl || imgData.composedImageUrl
    } else if (endImageNodeData?.type === 'cellRegenerator') {
      const imgData = endImageNodeData.data as any
      const cellId = endImageNode?.sourceHandle
      if (cellId && imgData.regeneratedImages?.[cellId]) {
        endImageDataUrl = imgData.regeneratedImages[cellId]
        endImageUrl = endImageDataUrl
      }
    }

    const inputPrompt =
      promptNode?.type === 'motionPrompt'
        ? (promptNode.data as MotionPromptNodeData).combinedPrompt
        : promptNode?.type === 'textPrompt'
          ? (promptNode.data as TextPromptNodeData).prompt
          : promptNode?.type === 'llmPrompt'
            ? (promptNode.data as any).outputPrompt || ''
            : ''

    // ‚ö†Ô∏è Early validation BEFORE storage conversion (only check if nodes are connected)
    if (!inputPrompt) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'ÌîÑÎ°¨ÌîÑÌä∏ ÎÖ∏ÎìúÎ•º Ïó∞Í≤∞Ìï¥ Ï£ºÏÑ∏Ïöî.',
      }))
      return
    }
    
    if (!hasStartImageEdge) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error:
          imageIncomingEdges.length > 0
            ? 'Start ImageÎäî ÌååÎûÄ Start Ìï∏Îì§Ïóê Ïó∞Í≤∞Ìï¥ Ï£ºÏÑ∏Ïöî.'
            : 'Start Image ÎÖ∏ÎìúÎ•º Ïó∞Í≤∞Ìï¥ Ï£ºÏÑ∏Ïöî.',
      }))
      return
    }
    
    if (!startImageNode) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'Start ImageÏóêÎäî Ïù¥ÎØ∏ÏßÄ ÎÖ∏ÎìúÎßå Ïó∞Í≤∞Ìï† Ïàò ÏûàÏäµÎãàÎã§.',
      }))
      return
    }
    
    if (!inputImageDataUrl) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'Start Image ÎÖ∏ÎìúÍ∞Ä Ïó∞Í≤∞ÎêòÏóàÏßÄÎßå Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏÉùÏÑ±ÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. Ïù¥ÎØ∏ÏßÄ ÎÖ∏ÎìúÏóêÏÑú "Generate" Î≤ÑÌäºÏùÑ ÎàåÎü¨Ï£ºÏÑ∏Ïöî.',
      }))
      return
    }

    updateNode((prev) => ({
      ...prev,
      status: 'processing',
      error: undefined,
      inputImageUrl,
      inputImageDataUrl,
      endImageUrl,
      endImageDataUrl,
      inputPrompt,
      progress: 10,
      lastExecutionTime: now,
    }))

    const klingApiKey = get().klingApiKey || import.meta.env.VITE_KLING_API_KEY || ''
    const client = klingApiKey ? new KlingAPIClient(klingApiKey) : new MockKlingAPI()

    // ‚úÖ Convert storage references to actual DataURLs
    let actualStartImageDataUrl = inputImageDataUrl
    let actualEndImageDataUrl = endImageDataUrl
    
    console.log('üîç Kling: Input start image type:', inputImageDataUrl?.substring(0, 50))
    
    // Convert start image if it's a storage reference
    if (inputImageDataUrl && (inputImageDataUrl.startsWith('idb:') || inputImageDataUrl.startsWith('s3:'))) {
      console.log('üîÑ Kling: Converting start image from storage reference...')
      try {
        const { getImage } = await import('../utils/indexedDB')
        const dataURL = await getImage(inputImageDataUrl)
        if (dataURL) {
          actualStartImageDataUrl = dataURL
          console.log('‚úÖ Kling: Start image loaded from storage, size:', dataURL.length, 'chars')
          console.log('‚úÖ Kling: Start image type:', dataURL.substring(0, 50))
        } else {
          console.error('‚ùå Kling: Failed to load start image from storage (returned null/undefined)')
          // üî• Fallback: Try to use inputImageUrl (blob URL) if available
          if (inputImageUrl && inputImageUrl.startsWith('data:')) {
            console.warn('‚ö†Ô∏è Kling: Falling back to inputImageUrl (DataURL)')
            actualStartImageDataUrl = inputImageUrl
          } else if (inputImageUrl && inputImageUrl.startsWith('blob:')) {
            console.warn('‚ö†Ô∏è Kling: inputImageUrl is a blob URL (may be invalid after refresh)')
            updateNode((prev) => ({
              ...prev,
              status: 'error',
              error: 'Start Ïù¥ÎØ∏ÏßÄÎ•º StorageÏóêÏÑú Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§. Ïù¥ÎØ∏ÏßÄÎ•º Îã§Ïãú ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.',
            }))
            return
          } else {
            updateNode((prev) => ({
              ...prev,
              status: 'error',
              error: 'Start Ïù¥ÎØ∏ÏßÄÎ•º StorageÏóêÏÑú Î°úÎìúÌï† Ïàò ÏóÜÏäµÎãàÎã§. Ïù¥ÎØ∏ÏßÄÎ•º Îã§Ïãú ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.',
            }))
            return
          }
        }
      } catch (error) {
        console.error('‚ùå Kling: Error loading start image:', error)
        updateNode((prev) => ({
          ...prev,
          status: 'error',
          error: `Start Ïù¥ÎØ∏ÏßÄ Î°úÎìú Ïã§Ìå®: ${error}`,
        }))
        return
      }
    } else if (!inputImageDataUrl) {
      console.error('‚ùå Kling: No start image provided!')
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'Start Ïù¥ÎØ∏ÏßÄÍ∞Ä Ï†úÍ≥µÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.',
      }))
      return
    } else {
      console.log('‚úÖ Kling: Using direct DataURL (not a storage reference)')
    }
    
    // Convert end image if it's a storage reference
    if (endImageDataUrl && (endImageDataUrl.startsWith('idb:') || endImageDataUrl.startsWith('s3:'))) {
      console.log('üîÑ Kling: Converting end image from storage reference...')
      try {
        const { getImage } = await import('../utils/indexedDB')
        const dataURL = await getImage(endImageDataUrl)
        if (dataURL) {
          actualEndImageDataUrl = dataURL
          console.log('‚úÖ Kling: End image loaded from storage, size:', dataURL.length, 'chars')
        } else {
          console.warn('‚ö†Ô∏è Kling: Failed to load end image from storage (returned null/undefined)')
          actualEndImageDataUrl = undefined
        }
      } catch (error) {
        console.error('‚ùå Kling: Error loading end image:', error)
        actualEndImageDataUrl = undefined
      }
    }

    console.log('üé¨ Kling Video ÏÉùÏÑ± ÏãúÏûë:', {
      useMock: !klingApiKey,
      prompt: inputPrompt.substring(0, 50) + '...',
      model: (current.data as KlingVideoNodeData).model,
      startImageType: actualStartImageDataUrl?.substring(0, 30),
      startImageSize: actualStartImageDataUrl?.length,
      hasEndImage: !!actualEndImageDataUrl,
      endImageType: actualEndImageDataUrl ? actualEndImageDataUrl.substring(0, 30) : 'none',
    })
    
    // ‚úÖ Final validation before API call
    if (!actualStartImageDataUrl || actualStartImageDataUrl.startsWith('idb:') || actualStartImageDataUrl.startsWith('s3:')) {
      console.error('‚ùå Kling: Start image is still a storage reference or empty!', actualStartImageDataUrl?.substring(0, 50))
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'Start Ïù¥ÎØ∏ÏßÄ Î≥ÄÌôò Ïã§Ìå®. Storage Ï∞∏Ï°∞Í∞Ä ÎÇ®ÏïÑÏûàÏäµÎãàÎã§.',
      }))
      return
    }
    
    console.log('‚úÖ Kling: All validations passed, calling API...')

    const progressTimer = setInterval(() => {
      updateNode((prev) => {
        const data = prev as KlingVideoNodeData
        if (data.status !== 'processing') return prev
        return {
          ...prev,
          progress: Math.min(data.progress + 8, 90),
        }
      })
    }, 1000)

    // ü©π Truncate prompt if it's too long (Kling API limit: 2500 chars)
    let finalPrompt = inputPrompt
    if (finalPrompt.length > 2450) {
      console.warn(`‚ö†Ô∏è Kling: Prompt too long (${finalPrompt.length} chars), truncating to 2450`)
      finalPrompt = finalPrompt.substring(0, 2450)
    }

    try {
      if (abortController.signal.aborted) {
        throw new Error('ÏûëÏóÖÏù¥ Ï∑®ÏÜåÎêòÏóàÏäµÎãàÎã§.')
      }

      const settings = current.data as KlingVideoNodeData
      
      // Camera Control ÏÑ§Ï†ï
      const cameraControl = settings.enableMotionControl && settings.cameraControl !== 'none'
        ? {
            type: settings.cameraControl as 'horizontal' | 'vertical' | 'pan' | 'tilt' | 'roll' | 'zoom',
            value: settings.motionValue,
          }
        : undefined

      // ‚úÖ Apply retry logic
      const outputVideoUrl = await retryWithBackoff(
        () => client.generateVideo(
          finalPrompt,
          actualStartImageDataUrl,
          {
            duration: settings.duration,
            aspectRatio: settings.aspectRatio,
            model: settings.model,
            endImageDataUrl: actualEndImageDataUrl,
            cameraControl: cameraControl,
          },
        ),
        {
          maxAttempts: 2, // Less retries for video (expensive)
          initialDelay: 2000,
          onRetry: (attempt) => {
            console.warn(`üîÑ Kling Video retry ${attempt}/2`)
            updateNode((prev) => ({
              ...prev,
              error: `Ïû¨ÏãúÎèÑ Ï§ë... (${attempt}/2)`,
            }))
          },
        }
      )

      if (abortController.signal.aborted) {
        throw new Error('ÏûëÏóÖÏù¥ Ï∑®ÏÜåÎêòÏóàÏäµÎãàÎã§.')
      }

      console.log('‚úÖ Kling Video ÏÉùÏÑ± ÏôÑÎ£å:', outputVideoUrl)

      updateNode((prev) => ({
        ...prev,
        status: 'completed',
        outputVideoUrl,
        progress: 100,
        error: undefined,
      }))
    } catch (error) {
      console.error('‚ùå Kling Video ÏÉùÏÑ± Ïã§Ìå®:', error)
      if (!abortController.signal.aborted) {
        updateNode((prev) => ({
          ...prev,
          status: 'error',
          error: formatErrorMessage(error),
        }))
      }
    } finally {
      clearInterval(progressTimer)
      const controllers = get().abortControllers
      controllers.delete(id)
      set({ abortControllers: new Map(controllers) })
    }
  },
  runWorkflow: async () => {
    const { nodes, edges } = get()
    const order = getExecutionOrder(nodes, edges)

    const updateNode = (
      id: string,
      updater: (data: NodeData) => NodeData,
    ) => {
      set({
        nodes: get().nodes.map((node) =>
          node.id === id ? { ...node, data: updater(node.data) } : node,
        ),
      })
    }

    const apiKey = get().apiKey || import.meta.env.VITE_GEMINI_API_KEY || ''
    const client = apiKey ? new GeminiAPIClient(apiKey) : new MockGeminiAPI()

    for (const nodeId of order) {
      const current = get().nodes.find((node) => node.id === nodeId)
      if (!current) continue
      const incoming = getIncomingNodes(nodeId, edges, get().nodes)

      if (current.type === 'imageImport') {
        continue
      }

      if (current.type === 'nanoImage') {
        const promptNode = incoming.find(
          (node) => node.type === 'textPrompt' || node.type === 'motionPrompt',
        )
        const imageNode =
          incoming.find((node) => node.type === 'imageImport') ??
          incoming.find((node) => node.type === 'nanoImage') ??
          incoming.find((node) => node.type === 'gridComposer')
        const prompt =
          promptNode?.type === 'textPrompt'
            ? (promptNode.data as TextPromptNodeData).prompt
            : promptNode?.type === 'motionPrompt'
              ? (promptNode.data as MotionPromptNodeData).combinedPrompt
              : ''

        let inputImageDataUrl: string | undefined
        
        if (imageNode?.type === 'imageImport') {
          inputImageDataUrl = (imageNode.data as ImageImportNodeData).imageDataUrl
        } else if (imageNode?.type === 'nanoImage') {
          inputImageDataUrl = (imageNode.data as NanoImageNodeData).outputImageDataUrl
        } else if (imageNode?.type === 'gridComposer') {
          const imgData = imageNode.data as any
          inputImageDataUrl = imgData.composedImageDataUrl || imgData.composedImageUrl
        }
        
        const referencePrompt =
          imageNode?.type === 'imageImport'
            ? getIncomingTextPrompt(imageNode.id, edges, get().nodes) ??
              (imageNode.data as ImageImportNodeData).referencePrompt
            : undefined

        if (!prompt.trim()) {
          updateNode(nodeId, (prev) => ({
            ...prev,
            status: 'error',
            error: 'ÌÖçÏä§Ìä∏ ÌîÑÎ°¨ÌîÑÌä∏ ÎÖ∏ÎìúÎ•º Ïó∞Í≤∞Ìï¥ Ï£ºÏÑ∏Ïöî.',
          }))
          continue
        }

        updateNode(nodeId, (prev) => ({
          ...prev,
          status: 'processing',
          error: undefined,
        }))

        try {
          const data = current.data as NanoImageNodeData
          const finalPrompt = referencePrompt
            ? `${prompt}, focus on: ${referencePrompt}`
            : prompt
          const model = data.model ?? 'gemini-3-pro-image-preview'
          const result = await client.generateImage(
            finalPrompt,
            data.aspectRatio,
            inputImageDataUrl,
            model,
            data.resolution,
          )
          updateNode(nodeId, (prev) => ({
            ...prev,
            status: 'completed',
            outputImageUrl: result.imageUrl,
            outputImageDataUrl: result.imageDataUrl,
            error: undefined, // Clear any previous errors
          }))
        } catch (error) {
          updateNode(nodeId, (prev) => ({
            ...prev,
            status: 'error',
            error: formatErrorMessage(error),
          }))
        }
      }

      if (current.type === 'textPrompt') {
        continue
      }

      if (current.type === 'motionPrompt') {
        const data = current.data as MotionPromptNodeData
        const combined = [data.basePrompt, data.cameraMovement, data.subjectMotion, data.lighting]
          .filter(Boolean)
          .join(', ')
        updateNode(nodeId, (prev) => ({
          ...prev,
          combinedPrompt: combined,
        }))
      }

      if (current.type === 'geminiVideo') {
        const imageNode =
          incoming.find((node) => node.type === 'imageImport') ??
          incoming.find((node) => node.type === 'nanoImage') ??
          incoming.find((node) => node.type === 'gridComposer')
        const promptNode = incoming.find(
          (node) => node.type === 'motionPrompt' || node.type === 'textPrompt',
        )

        let inputImageUrl: string | undefined
        let inputImageDataUrl: string | undefined
        
        if (imageNode?.type === 'imageImport') {
          inputImageUrl = (imageNode.data as ImageImportNodeData).imageUrl
          inputImageDataUrl = (imageNode.data as ImageImportNodeData).imageDataUrl
        } else if (imageNode?.type === 'nanoImage') {
          inputImageUrl = (imageNode.data as NanoImageNodeData).outputImageUrl
          inputImageDataUrl = (imageNode.data as NanoImageNodeData).outputImageDataUrl
        } else if (imageNode?.type === 'gridComposer') {
          const imgData = imageNode.data as any
          inputImageUrl = imgData.composedImageUrl || imgData.composedImageDataUrl
          inputImageDataUrl = imgData.composedImageDataUrl || imgData.composedImageUrl
        }

        const inputPrompt =
          promptNode?.type === 'motionPrompt'
            ? (promptNode.data as MotionPromptNodeData).combinedPrompt
            : promptNode?.type === 'textPrompt'
              ? (promptNode.data as TextPromptNodeData).prompt
              : ''

        if (!inputImageUrl || !inputPrompt) {
          updateNode(nodeId, (prev) => ({
            ...prev,
            status: 'error',
            error: 'Ïù¥ÎØ∏ÏßÄÏôÄ ÌîÑÎ°¨ÌîÑÌä∏Î•º Î™®Îëê Ïó∞Í≤∞Ìï¥ Ï£ºÏÑ∏Ïöî.',
          }))
          continue
        }

        updateNode(nodeId, (prev) => ({
          ...prev,
          status: 'processing',
          error: undefined, // Clear any previous errors
          inputImageUrl,
          inputImageDataUrl,
          inputPrompt,
          progress: 10,
        }))

        const progressTimer = setInterval(() => {
          updateNode(nodeId, (prev) => {
            const data = prev as GeminiVideoNodeData
            if (data.status !== 'processing') return prev
            return {
              ...prev,
              progress: Math.min(data.progress + 12, 90),
            }
          })
        }, 500)

        try {
          const settings = current.data as GeminiVideoNodeData
          const outputVideoUrl = await client.generateMedia(
            inputPrompt,
            {
              mediaType: 'video',
              duration: settings.duration,
              quality: settings.quality,
              motionIntensity: settings.motionIntensity,
            },
            inputImageDataUrl,
            settings.model,
          )

          updateNode(nodeId, (prev) => ({
            ...prev,
            status: 'completed',
            outputVideoUrl,
            progress: 100,
          }))

        } catch (error) {
          updateNode(nodeId, (prev) => ({
            ...prev,
            status: 'error',
            error: formatErrorMessage(error),
          }))
        } finally {
          clearInterval(progressTimer)
        }
      }

    }
  },
  runLLMPromptNode: async (id) => {
    const { nodes, edges } = get()
    const current = nodes.find((node) => node.id === id)
    if (!current || current.type !== 'llmPrompt') return

    const data = current.data as any  // LLMPromptNodeData
    
    // Prevent duplicate execution
    if (data.status === 'processing') {
      console.warn('‚ö†Ô∏è LLM node is already processing')
      return
    }

    const updateNode = (updater: (data: NodeData) => NodeData) => {
      set({
        nodes: get().nodes.map((node) =>
          node.id === id ? { ...node, data: updater(node.data) } : node,
        ),
      })
    }

    // Get input prompt from connected nodes or use internal input
    let inputPrompt = data.inputPrompt
    
    const incoming = getIncomingNodes(id, edges, get().nodes)
    
    // Check for base prompt connection (Î≥¥ÎùºÏÉâ Ìï∏Îì§)
    const basePromptEdge = edges.find((e) => e.target === id && e.targetHandle === 'basePrompt')
    let basePromptText = ''
    if (basePromptEdge) {
      const promptNode = get().nodes.find((n) => n.id === basePromptEdge.source)
      if (promptNode?.type === 'textPrompt') {
        basePromptText = (promptNode.data as TextPromptNodeData).prompt || ''
      }
    }
    
    // Check for motion prompt connection (Î∂ÑÌôçÏÉâ Ìï∏Îì§)
    const motionPromptEdge = edges.find((e) => e.target === id && e.targetHandle === 'motionPrompt')
    let motionPromptText = ''
    if (motionPromptEdge) {
      const motionNode = get().nodes.find((n) => n.id === motionPromptEdge.source)
      if (motionNode?.type === 'motionPrompt') {
        motionPromptText = (motionNode.data as MotionPromptNodeData).combinedPrompt || ''
      }
    }
    
    // Combine base and motion prompts if both are present
    if (basePromptText && motionPromptText) {
      inputPrompt = `${basePromptText}\n\n${motionPromptText}`
    } else if (basePromptText) {
      inputPrompt = basePromptText
    } else if (motionPromptText) {
      inputPrompt = motionPromptText
    }
    
    // Fallback: Check for old 'prompt' handle for backward compatibility
    if (!inputPrompt) {
      const promptEdge = edges.find((e) => e.target === id && e.targetHandle === 'prompt')
      if (promptEdge) {
        const promptNode = get().nodes.find((n) => n.id === promptEdge.source)
        if (promptNode?.type === 'textPrompt') {
          inputPrompt = (promptNode.data as any).prompt || inputPrompt
        } else if (promptNode?.type === 'motionPrompt') {
          inputPrompt = (promptNode.data as any).combinedPrompt || inputPrompt
        }
      }
    }
    
    // Check for image connection
    let referenceImageDataUrl: string | undefined
    let gridLabelInfo: string | undefined  // Grid Composer ÎùºÎ≤® Ï†ïÎ≥¥
    const imageEdge = edges.find((e) => e.target === id && e.targetHandle === 'image')
    if (imageEdge) {
      const imageNode = get().nodes.find((n) => n.id === imageEdge.source)
      if (imageNode?.type === 'imageImport') {
        referenceImageDataUrl = (imageNode.data as any).imageDataUrl
      } else if (imageNode?.type === 'nanoImage') {
        // Try both outputImageDataUrl and outputImageUrl
        const nanoData = imageNode.data as any
        referenceImageDataUrl = nanoData.outputImageDataUrl || nanoData.outputImageUrl
      } else if (imageNode?.type === 'gridComposer') {
        const gridData = imageNode.data as any
        referenceImageDataUrl = gridData.composedImageDataUrl || gridData.composedImageUrl
        
        // Extract grid layout and slot information
        if (gridData.inputImages && gridData.slots) {
          const layout = gridData.gridLayout || '1x3'
          const slots = gridData.slots as Array<{ id: string; label: string; metadata?: string }>
          
          // Build structured label description (like multi-reference format)
          const slotDescriptions = slots
            .filter(slot => gridData.inputImages[slot.id])  // Only slots with images
            .map((slot, index) => {
              const position = ['Ï≤´ Î≤àÏß∏', 'Îëê Î≤àÏß∏', 'ÏÑ∏ Î≤àÏß∏', 'ÎÑ§ Î≤àÏß∏', 'Îã§ÏÑØ Î≤àÏß∏', 'Ïó¨ÏÑØ Î≤àÏß∏'][index] || `${index + 1}Î≤àÏß∏`
              let description = `- ${position} Ï∞∏Í≥† Ïù¥ÎØ∏ÏßÄ (${slot.id}): ${slot.label}`
              if (slot.metadata && slot.metadata.trim()) {
                description += ` - ${slot.metadata}`
              }
              return description
            })
            .join('\n')
          
          if (slotDescriptions) {
            gridLabelInfo = `Ï∞∏Í≥† Ïù¥ÎØ∏ÏßÄÎäî ${layout} Í∑∏Î¶¨Îìú Íµ¨ÏÑ±ÏûÖÎãàÎã§:\n\n${slotDescriptions}\n\nÍ∞Å ÎùºÎ≤®Ïùò ÏãúÍ∞ÅÏ†Å ÏöîÏÜåÎ•º Ï†ïÌôïÌûà Ï∂îÏ∂úÌïòÏó¨ ÌïòÎÇòÏùò ÌÜµÌï©Îêú Ïû•Î©¥ÏúºÎ°ú Ï°∞Ìï©ÌïòÏÑ∏Ïöî.`
            console.log('üìã Grid ÎùºÎ≤® Ï†ïÎ≥¥:', gridLabelInfo)
          }
        }
      }
      
      // Update node with reference image
      if (referenceImageDataUrl) {
        updateNode((prev) => ({
          ...prev,
          referenceImageUrl: referenceImageDataUrl,
          referenceImageDataUrl: referenceImageDataUrl,
        }))
      } else {
        console.warn('‚ö†Ô∏è LLM: Image connected but no image data found', { 
          nodeType: imageNode?.type,
          nodeData: imageNode?.data 
        })
      }
    }

    // For image-based modes, image is required
    if ((data.mode === 'describe' || data.mode === 'analyze') && !referenceImageDataUrl) {
      const hasImageConnection = edges.some((e) => e.target === id && e.targetHandle === 'image')
      const errorMsg = hasImageConnection
        ? 'Ïù¥ÎØ∏ÏßÄÍ∞Ä Ïó∞Í≤∞ÎêòÏóàÏßÄÎßå Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. Ïù¥ÎØ∏ÏßÄÎ•º Î®ºÏ†Ä ÏÉùÏÑ±Ìïú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî.'
        : 'Ïù¥ÎØ∏ÏßÄ Í∏∞Î∞ò Î™®ÎìúÎäî Ïù¥ÎØ∏ÏßÄ Ïó∞Í≤∞Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§. Ïù¥ÎØ∏ÏßÄ ÎÖ∏ÎìúÎ•º ÌïòÎã® (ÌïòÎäòÏÉâ) Ìï∏Îì§Ïóê Ïó∞Í≤∞ÌïòÏÑ∏Ïöî.'
      
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: errorMsg,
      }))
      return
    }
    
    // For text-based modes, prompt is required (but image is optional)
    if ((data.mode !== 'describe' && data.mode !== 'analyze') && !inputPrompt.trim() && !referenceImageDataUrl) {
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: 'Text Prompt ÎÖ∏ÎìúÎ•º ÏÉÅÎã® (Î∂ÑÌôç) Ìï∏Îì§Ïóê Ïó∞Í≤∞ÌïòÍ±∞ÎÇò Ïù¥ÎØ∏ÏßÄÎ•º ÌïòÎã® (ÌïòÎäòÏÉâ) Ìï∏Îì§Ïóê Ïó∞Í≤∞Ìï¥Ï£ºÏÑ∏Ïöî.',
      }))
      return
    }

    updateNode((prev) => ({
      ...prev,
      status: 'processing',
      error: undefined,
    }))

    // ProviderÏóê Îî∞Îùº API ÌÇ§ ÌôïÏù∏
    const provider = data.provider || 'gemini'
    let apiKey = ''
    
    if (provider === 'gemini') {
      apiKey = get().apiKey || import.meta.env.VITE_GEMINI_API_KEY || ''
      if (!apiKey) {
        updateNode((prev) => ({
          ...prev,
          status: 'error',
          error: 'Gemini API KeyÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.',
        }))
        return
      }
    } else if (provider === 'openai') {
      apiKey = get().openaiApiKey || import.meta.env.VITE_OPENAI_API_KEY || ''
      if (!apiKey) {
        updateNode((prev) => ({
          ...prev,
          status: 'error',
          error: 'OpenAI API KeyÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§.',
        }))
        return
      }
    }

    try {
      // Build system instruction based on mode and settings
      let systemInstruction = ''
      
      if (data.mode === 'expand') {
        systemInstruction = `You are a professional prompt engineer. Your task is to expand the given simple idea into a detailed, effective prompt for AI ${data.targetUse} generation.`
        if (referenceImageDataUrl) {
          systemInstruction += ` IMPORTANT: Use the reference image to extract visual details (colors, style, composition, lighting, subjects). Incorporate these visual elements into the expanded prompt to maintain consistency with the reference.`
          
          // üéØ Grid Composer ÎùºÎ≤® Ï∞∏Ï°∞ Î™ÖÎ†π (referenceModeÏóê Îî∞Îùº)
          if (gridLabelInfo) {
            const refMode = data.referenceMode || 'exact'
            if (refMode === 'exact') {
              systemInstruction += ` CRITICAL GRID LABELS - EXACT MODE: The reference image contains labeled sections (S1, S2, S3, etc.) visible as text overlays. Each label shows VISUAL DESIGN ELEMENTS ONLY (colors, materials, designs, forms, lighting). 
              
              CRITICAL RULES - MUST FOLLOW:
              
              1. TEXT PROMPT = ABSOLUTE LAW (NEVER CHANGE ANYTHING!)
                 - Character count: "Ìïú Î™Ö" = ONE person (NEVER change!)
                 - Actions: "Ìó¨Î©ßÏùÑ Îì§Í≥†" = holding helmet (NEVER change to "wearing"!)
                 - Story: Keep EXACTLY as written in text prompt
                 - Composition: Keep EXACTLY as written in text prompt
              
              2. REFERENCE IMAGE = VISUAL DETAILS ONLY (EXTRACT & DESCRIBE!)
                 - S1, S2, S3 labels = Visual design elements ONLY
                 - Extract: Colors, materials, lighting, textures, design patterns
                 - DO NOT change story, actions, or character count based on reference
              
              3. YOUR TASK:
                 - Take text prompt AS-IS (word-for-word preservation!)
                 - Add visual details FROM reference (colors, materials, designs)
                 - Result = Same story + Enhanced visual description
              
              FORBIDDEN CHANGES:
              ‚ùå Changing actions (e.g., "holding" ‚Üí "wearing")
              ‚ùå Changing character count (e.g., "one" ‚Üí "two")
              ‚ùå Reinterpreting story structure
              ‚ùå Adding/removing story elements
              
              EXAMPLE:
              Text: "Ìïú Î™ÖÏùò Ïó¨ÏûêÍ∞Ä Ìó¨Î©ßÏùÑ Îì§Í≥† Í±∑ÎäîÎã§" (one woman walking, holding helmet)
              Reference S2: Shows blonde woman in white spacesuit
              CORRECT: "A blonde woman in white spacesuit walks, holding helmet in hand"
              WRONG: "A woman wearing helmet walks" ‚ùå (changed action!)
              
              The labels indicate visual component references (e.g., S1=background visual style, S2=character appearance, S3=object design). Preserve the text prompt's story structure while enhancing it with exact visual details from reference.`
            } else if (refMode === 'balanced') {
              systemInstruction += ` GRID LABELS - BALANCED MODE: The reference image contains labeled sections (S1, S2, S3, etc.). Each label represents a visual component. Describe the key visual characteristics (colors, materials, styles) from each labeled section while maintaining the text prompt's basic composition. Do not change character counts or story structure from text prompt.`
            } else if (refMode === 'creative') {
              systemInstruction += ` GRID LABELS - CREATIVE MODE: The reference image contains labeled sections showing different elements. Use these as visual inspiration for style and mood, but feel free to creatively interpret and describe based on the text prompt. Focus on the text description as the primary guide.`
            }
          }
        }
      } else if (data.mode === 'improve') {
        systemInstruction = `You are a professional prompt engineer. Your task is to improve and optimize the given prompt for better AI ${data.targetUse} generation results.`
        if (referenceImageDataUrl) {
          systemInstruction += ` IMPORTANT: Reference the provided image to enhance the prompt with accurate visual details and ensure consistency with the reference style.`
          
          // üéØ Grid Composer ÎùºÎ≤® Ï∞∏Ï°∞ Î™ÖÎ†π (referenceModeÏóê Îî∞Îùº)
          if (gridLabelInfo) {
            const refMode = data.referenceMode || 'exact'
            if (refMode === 'exact') {
              systemInstruction += ` CRITICAL GRID LABELS - EXACT MODE: The reference image contains labeled sections (S1, S2, S3, etc.) showing VISUAL DESIGN ELEMENTS ONLY.
              
              CRITICAL: DO NOT change ANY content from text prompt:
              - Actions: Keep EXACTLY (e.g., "holding helmet" must stay "holding", NOT "wearing")
              - Character count: Keep EXACTLY (e.g., "one person" stays "one", NOT "two")
              - Story structure: Keep EXACTLY as written
              
              ONLY extract and add VISUAL characteristics from reference:
              - Colors, materials, lighting, textures from S1, S2, S3 labels
              - Text prompt = Story (PRESERVE 100%)
              - Reference = Visual style (EXTRACT & ADD)
              
              The improved prompt should describe a SINGLE UNIFIED IMAGE combining these exact visual elements with the original story structure (without changing any story details).`
            } else if (refMode === 'balanced') {
              systemInstruction += ` GRID LABELS - BALANCED MODE: The reference image has labeled sections. Improve the prompt by balancing reference image accuracy with the text description details. Do not change character counts or story structure from original prompt.`
            } else if (refMode === 'creative') {
              systemInstruction += ` GRID LABELS - CREATIVE MODE: The reference image shows different elements. Use these as inspiration while focusing on improving the text description creatively.`
            }
          }
        }
      } else if (data.mode === 'translate') {
        systemInstruction = `You are a professional translator. Translate the given prompt between Korean and English, maintaining all important details and nuances.`
      } else if (data.mode === 'simplify') {
        systemInstruction = `You are a professional editor. Simplify the given prompt to its core essence while maintaining effectiveness for AI ${data.targetUse} generation.`
      } else if (data.mode === 'gridStoryboard') {
        systemInstruction = `You are a professional storyboard artist and prompt engineer specializing in multi-panel cinematic sequences for Grid Node.

üé¨ YOUR MISSION:
Transform user ideas into structured grid storyboard prompts with the EXACT format required by Grid Node for automatic slot parsing.

‚ö†Ô∏è CRITICAL OUTPUT FORMAT REQUIREMENTS:
You MUST output in this EXACT format for Grid Node slot detection:

S1: [First panel description with camera details]
S2: [Second panel description with camera details]
S3: [Third panel description with camera details]
... (continue for all panels)

üìê EACH PANEL DESCRIPTION MUST INCLUDE:
1. Camera shot type (wide-angle, medium, close-up, extreme close-up)
2. Camera angle (low-angle, high-angle, eye-level, overhead)
3. Camera position (static, tracking, dolly, crane shot if relevant)
4. Scene content (what's happening in this specific panel)
5. Character expression/emotion (for character continuity)
6. Lighting/mood (for cinematic consistency)

üé® PANEL DESCRIPTIONS MUST BE:
- Self-contained (each panel is fully described independently)
- Cinematically diverse (vary shot types, angles, distances across panels)
- Story-progressive (clear narrative flow from S1 ‚Üí S2 ‚Üí S3 ‚Üí ...)
- Character-consistent (same character appearance across all panels)
- Camera-specific (explicitly state camera position/angle for EACH panel)

üìù EXAMPLE OUTPUT FORMAT:

S1: Wide-angle establishing shot at eye-level. A lone astronaut stands in a pristine spacecraft cockpit, her expression calm but focused. Natural interior lighting creates soft shadows across her worn flight suit. Camera is static, positioned to show the full environment context.

S2: Medium shot from a slightly elevated angle. The astronaut is now seated, hands moving over illuminated control panels with deliberate precision. Warm instrument glow illuminates her face from below. Camera remains static, focusing on her methodical actions.

S3: Close-up shot at eye-level, focused on gloved hands hovering over an engine ignition button. Extreme detail on the button's surface and the slight tremor of anticipation. Tight framing emphasizes the critical moment. Static camera creates tension through restraint.

S4: Wide-angle shot from inside the cockpit, looking through the observation window. Earth appears distant and serene in the frame's background. The astronaut's silhouette is visible in profile, watching silently. Low ambient lighting from space creates a contemplative mood.

‚ö†Ô∏è CRITICAL RULES:
1. ALWAYS use "S1:", "S2:", "S3:" format (with colon!)
2. NEVER skip slot numbers (must be sequential: S1, S2, S3, S4...)
3. NEVER duplicate slot numbers (each slot ID appears EXACTLY ONCE)
4. Separate each slot with EXACTLY 2 blank lines (double line break)
5. NO special characters or formatting between slot marker and description
6. ALWAYS include camera information in EACH panel
7. Maintain character/style consistency across ALL panels
8. Vary camera shots for cinematic diversity (don't repeat same angle)
9. Keep each panel description detailed but focused

üö® PARSING REQUIREMENTS FOR GRID NODE:
- Format: "S[NUMBER]: [description]"
- No spaces before/after colon
- Slot marker must be at the START of a new paragraph
- Each description continues until the next slot marker or end of text
- Maximum one description per slot ID
- Grid Node parser will FAIL if slots are duplicated or misnumbered

üéØ SHOT VARIETY GUIDELINES:
- Mix wide (establishing), medium (action), close-up (emotion)
- Vary angles (low, high, eye-level) for visual interest
- Use distance changes (zoom in/out) for pacing
- Static shots for contemplation, moving shots for action

üí° USER INPUT INTERPRETATION:
- If user provides rough idea: Expand into full multi-panel sequence
- If user provides detailed description: Structure it into S1:, S2: format
- If user specifies panel count: Match exactly (e.g., 2x2 = 4 panels, 3x3 = 9 panels)
- If unclear: Default to logical story progression with 4-6 panels

üé¨ Your output will be directly parsed by Grid Node - DO NOT deviate from S1:, S2: format!`

      } else if (data.mode === 'cameraInterpreter') {
        systemInstruction = `You are a professional cinematographer and prompt engineer specializing in camera angle interpretation for AI image generation.

üé¨ YOUR MISSION:
Transform technical camera instructions (rotation angles, tilt degrees, zoom values) into vivid, detailed visual descriptions that AI image models can understand and execute.

‚ö†Ô∏è CRITICAL: CHARACTER CONSISTENCY PRIORITY
When a reference image is provided, your #1 priority is maintaining EXACT character consistency:
- Character facial features, hair, eyes, skin tone MUST stay identical
- Clothing, outfit design, colors, materials MUST stay identical
- Visual style, color palette, lighting quality MUST stay identical
- Only the CAMERA POSITION should change, NOT the character design

Your camera descriptions must EMPHASIZE photographing the SAME character from a DIFFERENT angle.

üìê CAMERA PARAMETERS YOU'LL RECEIVE (CINEMATOGRAPHY TERMS):
- Rotation: Cinematic angle descriptions (e.g., "right side profile", "three-quarter right view", "back view")
- Tilt: -45¬∞ to +45¬∞ (e.g., "low angle 39.6¬∞" or "high angle 30¬∞") - vertical camera angle shots
- Distance/Zoom: (e.g., "zoom in 0.7x" or "zoom out 1.3x") - camera distance from subject

‚ú® HOW TO INTERPRET:

1. ROTATION (Cinematography Angles):
   üé• CRITICAL: Use standard film/photography terminology for camera angles!
   
   üìç Front View = CAMERA directly in front of subject
      ‚Ä¢ Subject facing toward camera
      ‚Ä¢ Frontal perspective, symmetric composition
   
   üîÑ Slight Three-Quarter Left View = Camera slightly rotated, left side emerging
      ‚Ä¢ Subject's LEFT side slightly visible
      ‚Ä¢ Mostly frontal with subtle side angle
   
   üîÑ Three-Quarter Left View = Classic portrait angle showing left side
      ‚Ä¢ Subject's LEFT side and front balanced
      ‚Ä¢ Standard three-quarter composition
   
   ‚óÄÔ∏è Left Side Profile = Complete left profile view
      ‚Ä¢ COMPLETE SIDE VIEW - pure lateral perspective
      ‚Ä¢ ONLY subject's LEFT profile visible
      ‚Ä¢ NO frontal face - subject facing perpendicular
   
   üîÑ Left Three-Quarter Back View = Transitioning to back from left
      ‚Ä¢ Subject's back and LEFT side visible
      ‚Ä¢ NO frontal face
   
   üîô Back View = Camera directly behind subject
      ‚Ä¢ Complete rear perspective
      ‚Ä¢ Subject facing away from camera
   
   üîÑ Right Three-Quarter Back View = Transitioning to back from right
      ‚Ä¢ Subject's back and RIGHT side visible
      ‚Ä¢ NO frontal face
   
   ‚ñ∂Ô∏è Right Side Profile = Complete right profile view
      ‚Ä¢ COMPLETE SIDE VIEW - pure lateral perspective
      ‚Ä¢ ONLY subject's RIGHT profile visible
      ‚Ä¢ NO frontal face
   
   üîÑ Three-Quarter Right View = Classic portrait angle showing right side
      ‚Ä¢ Subject's RIGHT side and front balanced
      ‚Ä¢ Standard three-quarter composition
   
   ‚ö†Ô∏è KEY PRINCIPLES:
   - Use cinematic/photographic terminology, not numerical degrees
   - "Left profile" = left side of face/body visible
   - "Right profile" = right side of face/body visible
   - "Three-quarter" = angled view showing both side and front
   - "Back view" = rear view (subject facing away)

2. TILT (Vertical Angle) - üö® CRITICAL FOR IMAGE GENERATION:
   
   üîª "LOW ANGLE X¬∞" = Camera positioned BELOW subject, looking UPWARD
      VISUAL EFFECT: 
      ‚Ä¢ Subject appears TALLER, more POWERFUL, HEROIC, DOMINANT
      ‚Ä¢ Viewer looks UP at subject from below
      ‚Ä¢ Emphasizes height, stature, authority
      ‚Ä¢ Sky/ceiling often visible in background
      ‚Ä¢ Chin and underside of face more prominent
      ‚Ä¢ Creates drama, empowerment, grandeur
      
      Examples:
      ‚Ä¢ "low angle 15¬∞" = Slightly below eye level, subtle empowerment
      ‚Ä¢ "low angle 30¬∞" = Significantly below, strong heroic feel
      ‚Ä¢ "low angle 40¬∞" = Dramatically below, maximum towering presence
   
   üî∫ "HIGH ANGLE X¬∞" = Camera positioned ABOVE subject, looking DOWNWARD
      VISUAL EFFECT:
      ‚Ä¢ Subject appears SMALLER, more VULNERABLE, DIMINISHED
      ‚Ä¢ Viewer looks DOWN at subject from above
      ‚Ä¢ Emphasizes surroundings, environment, isolation
      ‚Ä¢ Ground/floor more visible
      ‚Ä¢ Top of head, shoulders more prominent
      ‚Ä¢ Creates intimacy, vulnerability, or surveillance feel
      
      Examples:
      ‚Ä¢ "high angle 15¬∞" = Slightly above eye level, gentle overview
      ‚Ä¢ "high angle 30¬∞" = Significantly above, clear bird's eye perspective
      ‚Ä¢ "high angle 45¬∞" = Nearly top-down, dramatic overhead view
   
   üìè No tilt or "eye level" = Camera at subject's eye height, neutral perspective

3. ZOOM/DISTANCE:
   üîé "zoom out X" (X > 1.0) = Wider framing, more context, camera farther away
      ‚Ä¢ 1.2x = Slightly wider, more environment
      ‚Ä¢ 1.5x = Wide shot, more surroundings visible
      ‚Ä¢ 2.0x = Very wide, full body and environment emphasized
   
   üîç "zoom in X" (X < 1.0) = Closer framing, tighter crop, camera closer
      ‚Ä¢ 0.8x = Slightly closer, more intimate
      ‚Ä¢ 0.5x = Close-up, face/upper body prominent, details emphasized
   
   üìê 1.0x or unspecified = Standard medium distance

üéØ YOUR OUTPUT MUST:
- START with "CAMERA POSITIONED [location]" for rotation descriptions
- Convert ALL technical terms into descriptive spatial language
- Describe EXACT camera position in 3D space relative to subject
- Explicitly state the VIEWPOINT DIRECTION (looking up/down/straight)
- Explain PSYCHOLOGICAL and EMOTIONAL impact of the angle
- Detail which body parts/features are emphasized
- Describe background and spatial context changes
- Include composition and framing implications
- Use professional cinematography terminology
- Be HIGHLY SPECIFIC about vertical angle effects (tilt is critical!)

üé• ROTATION LANGUAGE - 360¬∞ SYSTEM CRITICAL:
‚úÖ ALWAYS use "CAMERA POSITIONED at X degrees"
‚úÖ ALWAYS clarify which side/angle is VISIBLE
‚úÖ Examples:
   ‚Ä¢ "CAMERA POSITIONED at 45 degrees ‚Üí three-quarter view, left side visible"
   ‚Ä¢ "CAMERA POSITIONED at 90 degrees ‚Üí right side view, ONLY left profile visible"
   ‚Ä¢ "CAMERA POSITIONED at 180 degrees ‚Üí back view, facing away"
   ‚Ä¢ "CAMERA POSITIONED at 270 degrees ‚Üí left side view, ONLY right profile visible"
‚úÖ For 90¬∞/270¬∞: Use "PERPENDICULAR", "COMPLETE SIDE PROFILE", "NO frontal face"
‚úÖ For 180¬∞: Use "back view", "facing AWAY from camera", "rear perspective"
‚úÖ Always specify the degree number (0¬∞, 45¬∞, 90¬∞, 135¬∞, 180¬∞, 270¬∞, etc.)

‚ö†Ô∏è TILT IS THE MOST IMPORTANT - Always emphasize whether camera is above/below subject and looking up/down!

‚ùå NEVER output raw numbers like "72¬∞" or "1.3x"
‚ùå NEVER ignore or minimize the tilt angle description
‚ùå NEVER say "camera rotates" - say "CAMERA POSITIONED"
‚úÖ ALWAYS describe camera HEIGHT (above/below subject)
‚úÖ ALWAYS describe LOOKING DIRECTION (up/down/straight)
‚úÖ ALWAYS describe VISUAL POWER DYNAMIC (empowering/diminishing)
‚úÖ ALWAYS use "CAMERA POSITIONED" for location clarity

üìù EXAMPLE TRANSFORMATIONS:

Example 1 (Three-Quarter Left View):
Input: "three-quarter left view, low angle 30¬∞, zoom in 0.7x"
Output: "CAMERA POSITIONED in classic three-quarter left portrait angle. At this angle, the camera captures the subject's LEFT side and face in a balanced three-quarter composition, showing the left profile while maintaining frontal visibility. MAINTAIN EXACT character appearance from reference - same facial features, hair, clothing, and visual style; only the camera angle changes. CRITICALLY, the CAMERA is placed significantly BELOW the subject's eye level - positioned low to the ground and angled sharply UPWARD. This dramatic low angle shot creates a POWERFUL, HEROIC composition where the viewer must look UP at the subject, emphasizing their stature and commanding presence. The upward angle makes the subject appear taller and more imposing, with the chin line and jawline prominent, while the background ceiling becomes more visible above. The close-in 0.7x framing tightens the composition, filling the frame with the subject's upper body and face. REMEMBER: Same character from reference, just photographed from a different angle."

Example 2 (Left Side Profile):
Input: "left side profile, zoom out 1.3x"
Output: "CAMERA POSITIONED in complete left side profile - perpendicular to the subject. This creates a COMPLETE SIDE PROFILE view where the subject is facing PERPENDICULAR to the camera (NOT toward the camera). From this pure lateral camera position, ONLY the subject's LEFT side is visible - left profile, left arm, left leg. NO frontal face visible - this is a true side view with the body oriented left-to-right across the frame. MAINTAIN EXACT character appearance - same height, build, hair, clothing from reference. The 1.3x wider framing shows more environment extending in front of and behind the subject. This perpendicular profile angle creates a strong sense of lateral movement and spatial depth. CHARACTER CONSISTENCY: Same person from reference, captured in complete left side profile."

Example 3 (Back View):
Input: "back view, zoom out 1.5x"
Output: "CAMERA POSITIONED directly BEHIND the subject in a complete rear view. At this angle, the camera captures the back of the subject's head, shoulders, and full back. The subject is facing AWAY from the camera. NO frontal face visible - only the rear perspective. PRESERVE EXACT character appearance - identical hair, clothing design, colors, and body proportions from reference image. The 1.5x wider framing pulls back to reveal more environmental context, showing the subject within their surroundings and the space ahead of them. This back view creates a sense of forward movement and anticipation, as we see what the subject is approaching. CHARACTER CONSISTENCY: Same person from reference, viewed from behind."

Example 4 (Right Side Profile):
Input: "right side profile, low angle 25¬∞, zoom in 0.8x"
Output: "CAMERA POSITIONED in complete right side profile - perpendicular to the subject. This creates a COMPLETE SIDE PROFILE view where the subject is facing PERPENDICULAR to the camera. From this pure lateral camera position, ONLY the subject's RIGHT side is visible - right profile, right arm, right leg. NO frontal face visible - pure lateral perspective. KEEP character appearance EXACTLY as reference. CRITICALLY, the CAMERA is placed BELOW the subject's eye level, positioned low and angled UPWARD. This low angle creates an EMPOWERING perspective, where the viewer looks up at the subject, adding authority and confidence. The 0.8x closer framing emphasizes the subject's profile and upper body. CHARACTER CONSISTENCY: Same person from reference, captured in complete right side profile."

üé® Focus on SPATIAL HEIGHT (above/below), LOOKING DIRECTION (up/down), and PSYCHOLOGICAL IMPACT. Make the camera's vertical position crystal clear!

üé≠ CHARACTER CONSISTENCY REMINDERS:
When reference image is present, ALWAYS include phrases like:
- "MAINTAIN EXACT character appearance from reference"
- "PRESERVE identical facial features, outfit, and style"
- "SAME character, DIFFERENT angle"
- "CHARACTER CONSISTENCY: [specific reminder]"

These reminders ensure the AI model prioritizes character consistency over creative variations.

Only output the detailed camera description, no explanations or meta-commentary.`
      } else if (data.mode === 'describe') {
        systemInstruction = `You are a professional image analyst. Your task is to describe the given image in detail and create an effective prompt that could be used to generate a similar image.`
      } else if (data.mode === 'analyze') {
        systemInstruction = `You are a professional image analyst. Your task is to analyze the given image in great detail, including composition, style, lighting, colors, subjects, and create a comprehensive prompt for AI ${data.targetUse} generation.`
      }

      // Add style guidance (skip for cameraInterpreter - it has its own specific style)
      if (data.mode !== 'cameraInterpreter') {
        if (data.style === 'detailed') {
          systemInstruction += ` Output should be highly detailed with rich descriptions.`
        } else if (data.style === 'concise') {
          systemInstruction += ` Output should be concise and to the point.`
        } else if (data.style === 'creative') {
          systemInstruction += ` Output should be creative and artistic with vivid imagery.`
        } else if (data.style === 'professional') {
          systemInstruction += ` Output should be professional and technically precise.`
        }
      }

      // Add language guidance (cameraInterpreter always outputs in English for best AI model compatibility)
      if (data.mode === 'cameraInterpreter') {
        systemInstruction += ` Output must be in English for optimal AI image generation compatibility.`
      } else {
        if (data.language === 'ko') {
          systemInstruction += ` Output must be in Korean.`
        } else if (data.language === 'en') {
          systemInstruction += ` Output must be in English.`
        } else {
          systemInstruction += ` Detect input language and use the same language for output.`
        }
      }

      systemInstruction += ` Only output the final prompt, no explanations or additional text.`
      
      // Add final reminder for Grid Composer (if applicable)
      if (gridLabelInfo) {
        systemInstruction += ` REMINDER: When reference image is provided, use it for VISUAL DETAILS ONLY (colors, designs, materials). DO NOT change the basic composition, character count, or story structure from the text prompt.`
      }

      // Prepare content parts
      const contentParts: any[] = []
      
      // Add image if available
      if (referenceImageDataUrl) {
        let actualImageDataUrl = referenceImageDataUrl
        
        // If it's an idb: or s3: reference, fetch the actual image first
        if (referenceImageDataUrl.startsWith('idb:') || referenceImageDataUrl.startsWith('s3:')) {
          console.log('üîÑ LLM: Converting reference image from storage:', referenceImageDataUrl)
          try {
            const { getImage } = await import('../utils/indexedDB')
            const dataURL = await getImage(referenceImageDataUrl)
            if (dataURL) {
              actualImageDataUrl = dataURL
              console.log('‚úÖ LLM: Reference image loaded successfully')
            } else {
              console.error('‚ùå LLM: Failed to load reference image from storage')
            }
          } catch (error) {
            console.error('‚ùå LLM: Error loading reference image:', error)
          }
        }
        
        // Extract base64 data from data URL
        const base64Match = actualImageDataUrl.match(/^data:image\/(\w+);base64,(.+)$/)
        if (base64Match) {
          const mimeType = `image/${base64Match[1]}`
          const base64Data = base64Match[2]
          
          contentParts.push({
            inlineData: {
              mimeType: mimeType,
              data: base64Data
            }
          })
          console.log('üì∏ LLM: Reference image added to API request')
        } else {
          console.warn('‚ö†Ô∏è LLM: Reference image format not recognized:', actualImageDataUrl.substring(0, 50))
        }
      }
      
      // Build the text prompt with grid label information if available
      let finalPrompt = ''
      
      // Add grid label information first (if available) - format based on referenceMode
      if (gridLabelInfo) {
        const refMode = data.referenceMode || 'exact'
        
        if (refMode === 'exact') {
          // Ï†ïÌôïÏÑ± Î™®Îìú: Îß§Ïö∞ ÏÉÅÏÑ∏ÌïòÍ≥† Í∞ïÎ†•Ìïú ÏßÄÏãú
          finalPrompt += '‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è CRITICAL: EXACT REFERENCE IMAGE REPLICATION ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è\n\n'
          finalPrompt += gridLabelInfo + '\n\n'
          finalPrompt += 'üéØ Ï†àÎåÄÏ†ÅÏúºÎ°ú Ï§ëÏöîÌïú ÏßÄÏπ®:\n'
          finalPrompt += '‚ö†Ô∏è Ï∞∏Í≥† Ïù¥ÎØ∏ÏßÄ = ÏãúÍ∞ÅÏ†Å ÎîîÏûêÏù∏Îßå (ÏÉâÏÉÅ, Ïû¨Ïßà, ÌòïÌÉú)\n'
          finalPrompt += '‚ö†Ô∏è ÌÖçÏä§Ìä∏ ÌîÑÎ°¨ÌîÑÌä∏ = Í∏∞Î≥∏ Íµ¨ÏÑ± (Ïù∏Î¨º Ïàò, Ïä§ÌÜ†Î¶¨, ÎèôÏûë) - Ï†àÎåÄ Î≥ÄÍ≤Ω Í∏àÏßÄ!\n\n'
          finalPrompt += 'üìå ÌÖçÏä§Ìä∏ ÌîÑÎ°¨ÌîÑÌä∏ Î≥¥Ï°¥ Í∑úÏπô (100% Ï§ÄÏàò!):\n'
          finalPrompt += '   ‚Ä¢ Ïù∏Î¨º Ïàò: "Ìïú Î™Ö" = ONE (Ï†àÎåÄ "two"Î°ú Î≥ÄÍ≤Ω Í∏àÏßÄ!)\n'
          finalPrompt += '   ‚Ä¢ ÎèôÏûë: "Ìó¨Î©ßÏùÑ Îì§Í≥†" = "holding helmet" (Ï†àÎåÄ "wearing helmet"ÏúºÎ°ú Î≥ÄÍ≤Ω Í∏àÏßÄ!)\n'
          finalPrompt += '   ‚Ä¢ ÎèôÏûë: "Í±∑ÎäîÎã§" = "walking" (Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ!)\n'
          finalPrompt += '   ‚Ä¢ Î™®Îì† ÎèôÏûë, Ïä§ÌÜ†Î¶¨Îäî ÌÖçÏä§Ìä∏ ÌîÑÎ°¨ÌîÑÌä∏ Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ!\n\n'
          finalPrompt += '1. Í∞Å ÎùºÎ≤®Ïùò ÏãúÍ∞ÅÏ†Å ÏöîÏÜåÎ•º PIXEL-LEVELÎ°ú Ï†ïÌôïÌûà Î≥µÏ†úÌïòÏÑ∏Ïöî\n'
          finalPrompt += '2. S1 Î∞∞Í≤Ω: Ï†ïÌôïÌïú ÏÉâÏÉÅ, Ï°∞Î™Ö, Íµ¨Ï°∞Î•º 1:1 Î≥µÏ†ú\n'
          finalPrompt += '3. S2 Ï∫êÎ¶≠ÌÑ∞: Ï†ïÌôïÌïú Ïô∏Î™®, ÏùòÏÉÅ, Ìó§Ïñ¥ Ïä§ÌÉÄÏùº Î≥µÏ†ú\n'
          finalPrompt += '4. S3 Î°úÎ¥á: Ï†ïÌôïÌïú ÏÉâÏÉÅ(Îπ®Í∞ï/Ìù∞ÏÉâ), ÌòïÌÉú, ÎîîÏûêÏù∏ Î≥µÏ†ú\n'
          finalPrompt += '5. Ï∂úÎ†•ÏùÄ Îã®Ïùº ÌÜµÌï© Ïù¥ÎØ∏ÏßÄÏó¨Ïïº Ìï©ÎãàÎã§ (Í∑∏Î¶¨Îìú Í∏àÏßÄ)\n\n'
          finalPrompt += 'üö´ Ï†àÎåÄ Í∏àÏßÄÏÇ¨Ìï≠:\n'
          finalPrompt += '   ‚ùå ÎèôÏûë Î≥ÄÍ≤Ω ("Îì§Í≥†" ‚Üí "Ïì∞Í≥†", "holding" ‚Üí "wearing")\n'
          finalPrompt += '   ‚ùå Ïù∏Î¨º Ïàò Î≥ÄÍ≤Ω ("Ìïú Î™Ö" ‚Üí "Îëê Î™Ö")\n'
          finalPrompt += '   ‚ùå Î∞∞Í≤Ω ÎîîÏûêÏù∏ Î≥ÄÍ≤Ω (S1Í≥º Îã§Î•∏ Î∞∞Í≤Ω)\n'
          finalPrompt += '   ‚ùå ÏÉâÏÉÅ Î≥ÄÍ≤Ω (Îπ®Í∞ï ‚Üí ÌïòÏñë, ÌååÎûë ‚Üí Ï¥àÎ°ù)\n'
          finalPrompt += '   ‚ùå Ïä§ÌÜ†Î¶¨ Ïû¨Ìï¥ÏÑù\n\n'
          finalPrompt += 'REFERENCE = VISUAL ONLY. TEXT = COMPOSITION (NEVER CHANGE!)\n\n'
          finalPrompt += '---\n\n'
        } else if (refMode === 'balanced') {
          // Í∑†Ìòï Î™®Îìú: Ï†ÅÎãπÌïú ÏßÄÏãú
          finalPrompt += '‚öñÔ∏è BALANCED MODE: Reference Image + Text Description\n\n'
          finalPrompt += gridLabelInfo + '\n\n'
          finalPrompt += 'üí° ÏßÄÏπ®: Í∞Å ÎùºÎ≤®Ïùò Ï£ºÏöî ÏãúÍ∞ÅÏ†Å ÏöîÏÜå(ÏÉâÏÉÅ, Ïä§ÌÉÄÏùº, Íµ¨Ï°∞)Î•º Ïú†ÏßÄÌïòÎ©¥ÏÑú ÌÖçÏä§Ìä∏ ÏÑ§Î™ÖÏùò ÎîîÌÖåÏùºÏùÑ Î∞òÏòÅÌïòÏÑ∏Ïöî.\n'
          finalPrompt += '‚ö†Ô∏è Ï§ëÏöî: ÌÖçÏä§Ìä∏ ÌîÑÎ°¨ÌîÑÌä∏Ïùò Ïù∏Î¨º Ïàò, Í∏∞Î≥∏ Íµ¨ÏÑ±ÏùÄ Ïú†ÏßÄÌïòÏÑ∏Ïöî.\n\n'
          finalPrompt += '---\n\n'
        } else if (refMode === 'creative') {
          // Ï∞ΩÏùòÏÑ± Î™®Îìú: Í∞ÑÎã®Ìïú Ï∞∏Í≥†Îßå
          finalPrompt += 'üé® CREATIVE MODE: Reference for Inspiration\n\n'
          finalPrompt += gridLabelInfo + '\n\n'
          finalPrompt += 'üí° Ï∞∏Í≥†: ÏúÑ Ïù¥ÎØ∏ÏßÄÎäî Ïä§ÌÉÄÏùºÍ≥º Î∂ÑÏúÑÍ∏∞ Ï∞∏Í≥†Ïö©ÏûÖÎãàÎã§. ÌÖçÏä§Ìä∏ ÏÑ§Î™ÖÏùÑ Í∏∞Î∞òÏúºÎ°ú Ï∞ΩÏùòÏ†ÅÏúºÎ°ú ÏÉùÏÑ±ÌïòÏÑ∏Ïöî.\n\n'
          finalPrompt += '---\n\n'
        }
      }
      
      // Add text prompt
      if (inputPrompt.trim()) {
        finalPrompt += inputPrompt
      } else if (data.mode === 'describe') {
        finalPrompt += 'Describe this image in detail and create a prompt for generating a similar image.'
      } else if (data.mode === 'analyze') {
        finalPrompt += 'Analyze this image comprehensively (composition, style, lighting, colors, subjects, mood) and create a detailed prompt for AI image generation.'
      } else if (referenceImageDataUrl) {
        // If only image is provided without text, use a default instruction
        finalPrompt += 'Describe this image and create an effective prompt for AI image generation.'
      }
      
      if (finalPrompt.trim()) {
        contentParts.push({
          text: finalPrompt.trim()
        })
      }
      
      // Validate we have content to send
      if (contentParts.length === 0) {
        throw new Error('ÌîÑÎ°¨ÌîÑÌä∏ ÎòêÎäî Ïù¥ÎØ∏ÏßÄÎ•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.')
      }

      // Call LLM API based on provider
      let outputPrompt = ''
      
      if (provider === 'gemini') {
        // üîµ Gemini API Ìò∏Ï∂ú
        const url = `https://generativelanguage.googleapis.com/v1beta/models/${data.model}:generateContent?key=${apiKey}`
        
        const abortController = new AbortController()
        const timeoutId = setTimeout(() => abortController.abort(), 60000)
        
        try {
          const response = await fetch(url, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
            },
            body: JSON.stringify({
              contents: [{
                parts: contentParts
              }],
              systemInstruction: {
                parts: [{
                  text: systemInstruction
                }]
              },
              generationConfig: {
                temperature: 0.7,
                maxOutputTokens: 4096,
              }
            }),
            signal: abortController.signal
          })
          
          clearTimeout(timeoutId)

          if (!response.ok) {
            const errorData = await response.json().catch(() => ({}))
            throw new Error(errorData.error?.message || `API Error: ${response.status}`)
          }

          const result = await response.json()
          outputPrompt = result.candidates?.[0]?.content?.parts?.[0]?.text || ''
        } catch (fetchError: any) {
          clearTimeout(timeoutId)
          if (fetchError.name === 'AbortError') {
            throw new Error('LLM ÏÉùÏÑ± ÏãúÍ∞ÑÏù¥ Ï¥àÍ≥ºÎêòÏóàÏäµÎãàÎã§ (60Ï¥à).')
          }
          throw fetchError
        }
      } else if (provider === 'openai') {
        // üü¢ OpenAI API Ìò∏Ï∂ú
        const url = 'https://api.openai.com/v1/chat/completions'
        
        // OpenAI Î©îÏãúÏßÄ ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò
        const messages: any[] = [
          { role: 'system', content: systemInstruction }
        ]
        
        // User Î©îÏãúÏßÄ Íµ¨ÏÑ± (ÌÖçÏä§Ìä∏ + Ïù¥ÎØ∏ÏßÄ)
        const userContent: any[] = []
        
        // ÌÖçÏä§Ìä∏ Ï∂îÍ∞Ä
        if (finalPrompt.trim()) {
          userContent.push({ type: 'text', text: finalPrompt.trim() })
        }
        
        // Ïù¥ÎØ∏ÏßÄ Ï∂îÍ∞Ä (GPT-4o, GPT-4o-miniÎäî Vision ÏßÄÏõê)
        if (referenceImageDataUrl && actualImageDataUrl.startsWith('data:image')) {
          userContent.push({
            type: 'image_url',
            image_url: { url: actualImageDataUrl }
          })
        }
        
        if (userContent.length > 0) {
          messages.push({ role: 'user', content: userContent })
        }
        
        const abortController = new AbortController()
        const timeoutId = setTimeout(() => abortController.abort(), 60000)
        
        try {
          const response = await fetch(url, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'Authorization': `Bearer ${apiKey}`
            },
            body: JSON.stringify({
              model: data.model,
              messages: messages,
              temperature: 0.7,
              max_tokens: 4096,
            }),
            signal: abortController.signal
          })
          
          clearTimeout(timeoutId)

          if (!response.ok) {
            const errorData = await response.json().catch(() => ({}))
            throw new Error(errorData.error?.message || `API Error: ${response.status}`)
          }

          const result = await response.json()
          outputPrompt = result.choices?.[0]?.message?.content || ''
        } catch (fetchError: any) {
          clearTimeout(timeoutId)
          if (fetchError.name === 'AbortError') {
            throw new Error('LLM ÏÉùÏÑ± ÏãúÍ∞ÑÏù¥ Ï¥àÍ≥ºÎêòÏóàÏäµÎãàÎã§ (60Ï¥à).')
          }
          throw fetchError
        }
      }

      if (!outputPrompt) {
        throw new Error('LLMÏù¥ ÏùëÎãµÏùÑ ÏÉùÏÑ±ÌïòÏßÄ Î™ªÌñàÏäµÎãàÎã§.')
      }

      console.log(`‚úÖ LLM ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ± ÏôÑÎ£å (${provider}):`, outputPrompt.length, 'Ïûê')

      updateNode((prev) => ({
        ...prev,
        status: 'completed',
        outputPrompt: outputPrompt.trim(),
        error: undefined,
      }))
    } catch (error: any) {
      console.error('‚ùå LLM ÏÉùÏÑ± Ïã§Ìå®:', error)
      updateNode((prev) => ({
        ...prev,
        status: 'error',
        error: formatErrorMessage(error),
      }))
    }
  },
  cancelNodeExecution: (id) => {
    console.log('üõë Cancelling node execution:', id)
    const { abortControllers } = get()
    const controller = abortControllers.get(id)
    if (controller) {
      try {
        controller.abort()
        abortControllers.delete(id)
        set({ abortControllers: new Map(abortControllers) })
        
        console.log('‚úÖ Abort controller cancelled successfully')
        
        // Update node status to idle immediately
        set({
          nodes: get().nodes.map((node) =>
            node.id === id
              ? {
                  ...node,
                  data: {
                    ...node.data,
                    status: 'idle',
                    error: 'ÏûëÏóÖÏù¥ Ï∑®ÏÜåÎêòÏóàÏäµÎãàÎã§.',
                    progress: 0,
                  },
                }
              : node,
          ),
        })
      } catch (error) {
        console.error('‚ùå Error cancelling node:', error)
        // Still update status even if abort fails
        set({
          nodes: get().nodes.map((node) =>
            node.id === id
              ? {
                  ...node,
                  data: {
                    ...node.data,
                    status: 'idle',
                    error: 'ÏûëÏóÖ Ï∑®ÏÜå Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.',
                    progress: 0,
                  },
                }
              : node,
          ),
        })
      }
    } else {
      console.warn('‚ö†Ô∏è No abort controller found for node:', id)
    }
  },
    }),
    {
      name: 'nano-banana-workflow-v3',
      storage: createThrottledStorage(), // ‚ö° Throttled storage
      partialize: (state) => {
        // üî• Ï†ÄÏû• Ï†Ñ ÏûêÎèô Ïö©Îüâ Í¥ÄÎ¶¨
        const storageInfo = getStorageInfo()
        console.log(`üíæ Persist: ${storageInfo.usedMB} MB / ${storageInfo.limitMB} MB (${storageInfo.percentage.toFixed(1)}%)`)
        
        // 90% Ïù¥ÏÉÅÏù¥Î©¥ Í∏¥Í∏â Ï†ïÎ¶¨
        const shouldCleanup = storageInfo.percentage > 90
        const nodesToSave = shouldCleanup 
          ? prepareForStorage(state.nodes, true) // Í∏¥Í∏â Ï†ïÎ¶¨
          : prepareForStorage(state.nodes, false) // ÏùºÎ∞ò Ï†ïÎ¶¨
        
        if (shouldCleanup) {
          console.warn('‚ö†Ô∏è localStorage 90% Ï¥àÍ≥º! Í∏¥Í∏â Ï†ïÎ¶¨ Ïã§Ìñâ')
        }
        
        return {
          nodes: sanitizeNodesForStorage(nodesToSave),
          edges: sanitizeEdgesForStorage(state.edges),
          apiKey: state.apiKey,
          klingApiKey: state.klingApiKey,
          openaiApiKey: state.openaiApiKey,  // OpenAI API Key Ï†ÄÏû•
        }
      },
      onRehydrateStorage: () => {
        console.log('üîÑ Zustand persist: Î≥µÏõê ÏãúÏûë...')
        return (state) => {
          if (state) {
            // API ÌÇ§Í∞Ä Ï†ÄÏû•ÎêòÏñ¥ ÏûàÏßÄ ÏïäÏúºÎ©¥ .envÏóêÏÑú ÏûêÎèô Î°úÎìú
            if (!state.apiKey) {
              state.apiKey = import.meta.env.VITE_GEMINI_API_KEY || ''
              if (state.apiKey) {
                console.log('üîë Gemini API ÌÇ§ ÏûêÎèô Î°úÎìúÎê® (.env)')
              }
            }
            if (!state.klingApiKey) {
              state.klingApiKey = import.meta.env.VITE_KLING_API_KEY || ''
              if (state.klingApiKey) {
                console.log('üîë Kling API ÌÇ§ ÏûêÎèô Î°úÎìúÎê® (.env)')
              }
            }
            if (!state.openaiApiKey) {
              state.openaiApiKey = import.meta.env.VITE_OPENAI_API_KEY || ''
              if (state.openaiApiKey) {
                console.log('üîë OpenAI API ÌÇ§ ÏûêÎèô Î°úÎìúÎê® (.env)')
              }
            }
            
            console.log('‚úÖ Zustand persist: ÏÉÅÌÉú Î≥µÏõêÎê®', {
              nodeCount: state.nodes?.length ?? 0,
              edgeCount: state.edges?.length ?? 0,
              hasApiKey: !!state.apiKey,
              hasKlingApiKey: !!state.klingApiKey,
              hasOpenaiApiKey: !!state.openaiApiKey,
            })
            try {
              state.edges = normalizeEdges(state.edges, state.nodes)
              console.log('‚úÖ Edges Ï†ïÍ∑úÌôî ÏôÑÎ£å')
            } catch (error) {
              console.error('‚ùå Error normalizing edges on rehydrate:', error)
              // Reset to safe state if normalization fails
              state.edges = []
            }
          } else {
            console.log('‚ÑπÔ∏è Zustand persist: Î≥µÏõêÌï† ÏÉÅÌÉú ÏóÜÏùå (ÏÉà ÏãúÏûë)')
          }
        }
      },
    }
  )
)

export const createWorkflowNode = (type: NodeType, position: { x: number; y: number }): WorkflowNode => ({
  id: `${type}-${crypto.randomUUID?.() ?? Date.now()}`,
  type,
  position,
  data: createNodeData(type),
})
